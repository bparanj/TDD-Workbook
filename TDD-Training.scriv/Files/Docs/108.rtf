{\rtf1\ansi\ansicpg1252\cocoartf1138\cocoasubrtf320
{\fonttbl\f0\fnil\fcharset0 Verdana;}
{\colortbl;\red255\green255\blue255;}
\pard\tx720\fi720\pardirnatural

\f0\fs28 \cf0 Options for verifying that the SUT has behaved correctly, including exercising the SUT and comparing the actual outcome with the expected outcome.\
\
There is a lot more to writing good tests than just calling the built-in assertion methods. We also need to learn key techniques for making tests easy to understand and for avoiding and removing test code duplication.\
\
A key challenge in coding the assertions is getting access to the information we want to compare with the expected results. This is where observation points come into play; they provide a window into the state or behavior of the SUT so that we can pass it to the assertion methods. \
\

\b Verify State or Behavior
\b0 \
\
Ultimately, test automation is about verifying the behavior of the SUT. Some aspects of the SUT\'92s behavior can be verified directly; the value returned by a function is a good example. Other aspects of the behavior are more easily verified indirectly by looking at the state of some object. We can verify the actual behavior of the SUT in our tests in two ways:\
\
1. We can verify the states of various objects affected by the SUT by extracting each state using an observation point and using assertions to compare it to the expected state.\
2. We can verify the behavior of the SUT directly by using observation points inserted between the SUT and its DoC to monitor its interactions (in the form of the method calls it makes) and comparing those method calls with what we expected.\
\
State Verification is done using assertions and is the simpler of the two approaches. Behavior Verification is more complicated and builds on the assertion techniques we use for verifying state.\
\

\b State Verification
\b0 \
\
The normal way to verify the expected outcome has occurred is called state verification. First we exercise the SUT; then we examine the post-exercise state of the SUT using assertions. We may also examine anything returned by the SUT as a result of the method call we made to exercise it. What is most notable is what we do not do: we do not instrument the SUT in any way to detect how it interacts with other components of the system. That is, we inspect only direct outputs and we use only direct method calls as our observation points.\
\
Fig 10.1 State Verification. In state verification, we assert that the SUT and any objects it returns are in the expected state after we have exercised the SUT. We pay no attention to the man behind the curtain.\
\
State verification can be done in two slightly different ways. Procedural state verification involves writing a sequence of assertions that pick apart the end state of the SUT and verify that it is as expected. Expected object is a way of describing the expected state in such a way that it can be compared with a single assertion method call. This approach minimizes test code duplication and increases test clarity. With both strategies we can use either \'93built-in\'94 assertions or custom assertions.\
\

\b Using Built-in Assertions
\b0 \
\
There are different assertion methods:\
\
Stated Outcome Assertions such as assert(boolean_expression)\
\
Simple Equality Assertions such as assert(expected, actual)\
\
Fuzzy Equality Assertions such as assert(expected, actual, tolerance) which are used for comparing floats.\
\
Each test should make it very clear that \'93When the system is in state S1 and I do X, the result should be R and system should be in state S2\'94. We put the system into state S1 in our fixture setup logic. \'93I do X\'94 corresponds to the exercise SUT phase of the test. \'93The result is R\'94 and \'93the system is in state S2\'94 are implemented using assertions. Thus we want to write our assertions in such a way that they succinctly describe R and S2. When the test fails we want the failure message to tell us enough to enable us to identify the problem. Therefore, we should always include an assertion message. It makes integration build failures much easier to reproduce and fix. It also makes troubleshooting broken tests easier by telling us what 
\i should have 
\i0 happened; the actual outcome tells us what did happen.\
\
We can make the assertion output much more specific by using an argument describing message constructed by incorporating useful bits of data into the message. A good start is to include each of the values in the expression passed as the assertion method\'92s arguments.\
\

\b Verifying Behavior
\b0 \
\
Verifying behavior is more complicated than verifying state because behavior is dynamic. We have to catch the SUT in the act as it generates indirect outputs to the objects it depends on. Two basic styles of behavior verification are : Procedural Behavior Verification and Expected Behavior. Both require a mechanism to access the outgoing method calls of the SUT (its indirect outputs). \
\
Fig 10.2 Behavior Verification. In behavior verification, we focus our assertions on the indirect outputs (outgoing interfaces) of the SUT. This typically involves replacing the DoC with something that facilitates observing and verifying the outgoing calls.\
\

\b Procedural Behavior Verification
\b0 \
\
In procedural behavior verification, we capture the behavior of the SUT as it executes and save that data for later retrieval. The test then compares each output of the SUT one by one with the corresponding expected output. Thus in procedural behavior verification, the test executes a procedure (a set of steps) to verify the behavior.\
\
The key challenge in procedural behavior verification is capturing the behavior as it occurs and saving it until the test is ready to use this information. This task is accomplished by configuring the SUT to use a fake object instead of the depended-on class. After the SUT has been exercised, the test retrieves the recording of the behavior and verifies it using assertions.\
\

\b Expected Behavior Specification
\b0 \
\
Expected behavior is often used in conjunction with layer-crossing tests to verify the indirect outputs of an object or component. We configure a mock object with the method calls we expect the SUT to make to it and install this object before exercising the SUT.\
\

\b Reducing Test Code Duplication
\b0 \
\
In result verification logic, test code duplication usually shows up as a set of repeated assertions. Techniques to reduce the number of assertions : Expected Objects, Custom Assertions, Verification Methods\
\

\b Avoiding Conditional Test Logic
\b0 \
\
Conditional test logic is bad because the same test may execute differently in different circumstances. The only way to verify our test method is to manually edit the SUT so that it produces the error we want to be detected. \
\

\b Eliminating if Statements
\b0 \
\
Use Guard Assertion\
\

\b Eliminating Loops
\b0 \
\
Loops in the test method creates three problems:\
It introduces untestable test code because the looping code, which is part of the test, cannot be tested with fully automated tests.\
It leads to obscure tests because all that looping code obscures the real intent.\
Complexity of writing the loops can discourage the developer from writing the self-checking test.\
\
A better solution is to delegate this logic to a test utility method with an intent-revealing name which can be both tested and reused.\
\

\b Techniques for Writing Easy to Understand Tests
\b0 \
\
Working Backward, Outside-In\
\
A useful little trick for writing very intent revealing code is to work backward. Start with an end in mind. To do so, we write the last line of the test first. For a function, its whole reason for existence is to return a value; for a procedure, it is to produce one or more side effects by modifying something. For a test, the purpose is to verify that the expected outcome has occurred (by making assertions).\
\
Working backward means we write these assertions first. We assert on the values of suitably named local variables to ensure that the assertion is intent-revealing. The reset of writing the test simply consists of filling in whatever is needed to execute those assertions: We declare variables to hold the assertion arguments and initialize them with the appropriate content. Because at least one argument should have been retrieved from the SUT, we must, of course, invoke the SUT. To do so, we may need some variables to use as SUT arguments. Declaring and initializing a variable after it has been used forces us to understand the variable better when we introduce it. This scheme also results in better variable names and avoid meaningless names.\
\
Working outside-in (top-down) means staying at a consistent level of abstraction. The test method should focus on what we need to have in place to induce the relevant behavior in the SUT. The mechanics of how we reach that place should be delegated to a lower layer of test software. In practice, we code this behavior as calls to test utility methods, which allows us to stay focused on the requirements of the SUT as we write each test method. We don\'92t need to worry about how we will create that object or verify that outcome; we merely need to describe what that object or outcome 
\i should be. 
\i0 The utility method we just used but haven\'92t yet defined acts as a placeholder for the unfinished test automation logic. We should always give this method an intent revealing name and stub it out with a call to the fail assertion to remind ourselves that we still need to write the method\'92s body. We can move on to writing the other tests we need for this SUT while they are still fresh in our minds. Later, we can switch to our toolsmith hat and implement the test utility methods.\
\

\b Using TDD to Write Test Utility Methods
\b0 \
\
Once we are finished writing the test method that used the test utility method, we can start the process of writing the test utility method itself. Along the way, we can take advantage of TDD by writing test utility tests. It doesn\'92t take very long to write these unit tests that verify the behavior of our test utility methods and we will have much more confidence in them.\
\
We start with a simple case (say, asserting the equality of two identical collections that hold the same item) and work up to the most complicated case that the test methods 
\i actually require 
\i0 (say, two collections that contain the same two items but in different order). TDD helps us to find the minimal implementation of the test utility method, which may be much simpler than a complete generic solution. There is no point in writing generic logic that handles cases that aren\'92t actually needed but it may be worthwhile to include a guard assertion or two inside the custom assertion to fail tests in cases it doesn\'92t support.\
\
}