<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="106">
            <Title>Test Automation Strategy</Title>
            <Text>What’s Strategic?

Be aware of the strategic decisions necessary and to make them just in time rather than much too late.

What makes a decision strategic? A decision is strategic if it is hard to change. That is, a strategic decision affects a large number of tests, especially such that many or all the tests would need to be converted to a different approach at the same time. Put another way, any decision that could cost a large amount of effort to change is strategic.

Common strategic decisions include the following considerations:
Which kinds of tests to automate?
Which tools to use to automate them?
How to manage the test fixture?
How to ensure that the system is easily tested and how the tests interact with the SUT?

Each of these decisions can have far-reaching consequences, so they are best made consciously, at the right time, and based on the best available information.

Which Kinds of Tests Should We Automate?

We can divide tests into following two categories:

Per-functionality tests (aka functional tests) verify the behavior of the SUT in response to a particular stimulus.
Cross-functional tests verify various aspects of the system’s behavior that cut across specific functionality.

Introducing xUnit

xUnit design is intended to meet the following goals:
Make it easy for developers to write tests without needing to learn a new programming language.
Make it easy to test individual classes and objects without needing to have the rest of the application available. We just have to design for testability to take advantage of this capability.
Make it easy to run one test or many tests with a single action.
Minimize the cost of running the tests so programmers aren’t discouraged from running the existing tests.

Mechanics of Making Software Testable

Control Points and Observation Points

A test interacts with the software through one or more interfaces or interaction points. From the test’s point of view, these interfaces can act as either control points or observation points.

Figure 6-6 Control Points and Observation Points. The test interacts with the SUT through interaction points. Direct interaction points are synchronous method calls made by the test; indirect interaction points require some form of back door manipulation. Control points have arrows pointing toward the SUT; observation points have arrows pointing away from the SUT.

A control point is how the test asks the software to do something for it. This could be for the purpose of putting the software into a specific state as part of setting up or tearing down the test fixture, or it could be to exercise the SUT. Some control points are provided strictly for the tests; they should not be used by the production code because they bypass input validation or short-circuit the normal life cycle of the SUT or some object on which it depends. 

An observation point is how the test finds out about the SUT’s behavior during the result verification phase of the test. Observation points can be used to retrieve the post-test state of the SUT or a DoC. They can also be used to spy on the interactions between the SUT and any components with which it is expected to interact while it is being exercised. Verifying these indirect outputs is an example of back door verification.

Both control points and observation points can be provided by the SUT as synchronous method calls; we call this “going in the front door”. Some interaction points may be via a ‘back door’ to the SUT; we call this back door manipulation. 

In the diagrams that follow, control points are represented by the arrowheads that pint to the SUT, whether from the test or from a DoC. Observation points are represented by the arrows whose heads point back to the test itself. These arrows typically start at the SUT or DoC or start at the test and interact with either the SUT or DoC  (an asynchronous observation point) before returning to the test (a synchronous observation point).

Interaction Styles and Testability Patterns

Tests can take one of two basic forms. A round-trip test interacts with the SUT in question only through its public interface. Both the control points and the observation points in a typical round-trip test are simple method calls. The nice thing about this approach is that it does not violate encapsulation. The test needs to know only the public interface of the software; it doesn’t need to know anything about how it is built.

Fig 6.7 A round-trip test interacts with the SUT only via the public interface. The test on the right replaces a DoC with a fake object to improve its repeatability or performance. 

The main alternative is the layer-crossing test in which we exercise the SUT through the API and keep an eye on what comes out the back door using some form of test double such as a test spy or mock object. This can be a very powerful testing technique for verifying certain kinds of mostly architectural requirements. Unfortunately, this approach can also result in over specified software if it is over used because changes in how the software implements its responsibilities can cause tests to fail.

Fig 6.8 A layer-crossing test can interact with the SUT via a back door. The test on the left controls the SUT’s indirect inputs using a test stub. The test on the right verifies its indirect outputs using a mock object. 

In fig 6.8, the test on the left uses a test stub that stands in for the DoC as a control point. The test on the right uses a mock object that stands in for the DoC as the observation point. Testing in this style implies a Layered Architecture, which in turn opens the door to using layer tests to test each layer of the architecture independently. An even more general concept is the use of component tests to test each component within a layer independently.

Fig 6.9. A pair of layer test each testing a different layer of the system. Each layer of a layered architecture can be tested independently using a distinct set of layer tests. This ensures good separation of concerns, and the tests reinforce the layered architecture.

Whenever we want to write layer-crossing tests, we need to ensure that we have built in a substitutable dependency mechanism for any components on which the SUT depends but that we want to test independently. 

Fig 6.10 A test double being injected into a SUT by a test. A test can use dependency injection to replace a DoC with an appropriate test double. The DoC is passed to the SUT by the test as or after it has been created.

Asynchronous test interacts with the SUT through real messaging. Because the responses to these requests also come asynchronously, these tests must include some kind of interprocess synchronization such as calls to wait. Unfortunately, the need to wait for message responses that might never arrive can cause these tests to take a very long time to execute.

Humble Executable Pattern

It can remove the need to conduct asynchronous test. It involves putting the logic that handles the incoming message into a separate class of component, which can then be tested synchronously using either a round-trip or layer-crossing style.

Fig 6.11 A humber executable making testing easier. It can improve the repeatability and speed of verifying logic that would otherwise have to be verified via asynchronous  tests.

A related issue is the testing of business logic through the UI. In general such indirect testing is a bad idea because changes to the UI code will break tests that are trying to verify the business logic behind it. Because the UI tends to change frequently, especially on agile projects, this strategy will greatly increase test maintenance costs. Another reason that is a bad idea is that UIs are inherently asynchronous. Tests that exercise the system through the UI have to be asynchronous tests along with all the issues that come with them.

Divide and Test

We can turn almost any hard to test code into easily tested code through refactoring as long as we have enough tests in place to ensure that we do not introduce bugs during this refactoring.

We can avoid using the UI for customer tests by writing those tests as Subcutaneous Tests. These tests bypass the UI layer of the system and exercise the business logic via a Service Facade that exposes the necessary synchronous interaction points to the test. The UI relies on the same facade, enabling us to verify that the business logic works correctly even before we hook up the UI logic. The layered architecture also enables us to test the UI logic before the business logic is finished; we can replace the Service Facade with a test double that provides completely deterministic behavior that our tests can depend on.

The test double should be independent of the real implementation so that the UI tests need to know only which data to use to evoke specific behaviors from the service facade, not the logic behind it.

To unit test non trivial UIs we can use a humble dialog to move the logic that makes decisions about the UI out of the visual layer, which is difficult to test synchronously, and into a layer of supporting objects, which can be verified with standard unit-testing techniques. This approach allows the presentation logic behavior to be tested as thoroughly as the business logic behavior.

Any UI that contains state information or support  conditional display or enabling of elements should be considered non trivial.

Fig 6.12 A humble dialog reducing the dependency of the test on the UI framework. The logic that controls the state of UI components can be very difficult to test. Extracting it into a testable component leaves behind a humble dialog that requires very little testing.

From a test automation strategy perspective, the key thing is to make the decision about which test - SUT interaction styles should be used and which ones should be avoided, and to ensure that the software is designed to support that decision.

</Text>
        </Document>
        <Document ID="113">
            <Title>Test Strategy Patterns</Title>
            <Text>Minimal Fixture
Minimal Context 

Which fixture strategy should we use?

We use the smallest and simplest fixture possible for each test.

Figure …

Every test needs some kind of test fixture. A key part of understanding a test is understanding the test fixture and recognizing how it influences the expected outcome of the test. Tests are much easier to understand if the fixture is small and simple.

A minimal fixture is important for achieving tests as documentation and for avoiding slow tests. A test that uses a minimal fixture will be easier to understand than one that uses a fixture containing unnecessary or irrelevant information. 

We design a fixture that includes only those objects that are absolutely necessary to express the behavior that the test verifies. Another way to phrase this is “If the object is not important to understand the test, it is important not to include it in the fixture.”

To build a minimal fixture, we ruthlessly remove anything from the fixture that does not help the test communicate how the SUT should behave. Two forms of minimization can be considered:

We can eliminate objects entirely. That is, we don’t even build the objects as part of the fixture. If the object isn’t necessary to prove something about how the SUT behaves, we don’t include it at all.

We can hid unnecessary attributes of the object when they don’t contribute to the understanding of the expected behavior.

A simple way to find out whether an object is necessary as part of the fixture is to remove it. If the test fails as a result, the object was probably necessary in some way. Of course, it may have been necessary only as argument to some method we are not interested in or as an attribute that is never used (even though the object to which the attribute belongs is required for some reason). Including these kinds of objects as part of fixture setup definitely contributes to obscure tests. 

We can eliminate these unnecessary objects in one of two ways:

1. By hiding them
2. By eliminating the need for them by passing in dummy objects or using entity chain snipping. If the SUT actually accesses the object as it is executing the logic under test, however, we may be forced to include the object as part of the test fixture. 

Having determined that the object is necessary for the execution of the test, we must now ask whether the object is helpful in understanding the test. If we were to initialize it off-stage, would that make it harder to understand the test? Would the object lead to an obscure test by acting as a mystery guest? If so, we want to keep the object visible. Boundary values are a good example of a case in which we do want to keep the objects and attributes that take on the boundary values visible.

If we have established that the object or attribute isn’t necessary for understanding the test, we should make every effort to eliminate it from the test method, albeit not necessarily from the test fixture. Creation methods are a common way of achieving this goal. We can hid the attributes of objects that don’t affect the outcome of the test but that are needed for construction of the object by using creation methods to fill in all the don’t care attributes with meaningful default values. We can also hide the creation of necessary depended-on objects within the creation methods. A good example of this occurs when we write tests that require badly formed objects as input (for testing the SUT with invalid inputs). In this case we don’t want to confuse the issue by showing all valid attributes of the object being passed to the SUT; there could be many of these extraneous attributes. Instead, we want to focus on the invalid attribute. To do so, we can use the one bad attribute pattern to build malformed objects with a minimum of code by calling a creation method to construct a valid object and then replacing a single attribute with the invalid value that we want to verify the SUT will handle correctly.

Unit Test Rules

A test is not a unit test if:

It talks to the database
It communicates across a network
It touches the file system
It can’t run correctly at the same time as any of your other unit tests
You have to do special things to your environment (such as editing config files) to run it

Layer Crossing Test

Back Door Manipulation

How can we verify logic independently when we cannot use a round-trip test?

We set up the test fixture or verify the outcome by going through a back door (such as direct database access)

Fig….

Every test requires a starting point (the test fixture) and an expected finishing point (the expected results). The normal approach is to set up the fixture and verify the outcome by using the API of the SUT itself. In some circumstances this is either not possible or not desirable.

In some situations we can use back door manipulation to set up the fixture and / or verify the SUT’s state.

The state of the SUT can be stored in memory, on disk as files, in a database, or in other applications with which the SUT interacts. Whatever form it takes, the pre-conditions of a test typically require that the state of the SUT is a specific state. Likewise, at the end of the test we often want to do state verification of the SUT’s state.

If we have access to the state of the SUT from outside the SUT, the test can set up the pre-test state of the SUT by bypassing the normal API of the SUT and interacting directly with whatever is holding that state via a back door. When exercising of the SUT has been completed, the test can similarly access the post-test state of the SUT via a back door to compare it with expected outcome. For customer tests, the back door is most commonly a test database, but it could also be some other component on which the SUT depends. For unit tests, the back door is some other class or object or an alternative interface of the SUT that exposes the state in a way normal clients wouldn’t use. We can replace a DoC with a suitably configured test double instead of using the real thing it that makes the job easier.

Layer Test

Single Layer Test, Testing by Layers, Layered Test
How can we verify logic independently when it is part of a layered architecture

We write separate tests for each layer of the layered architecture

Fig…

It is difficult to obtain good test coverage when testing an entire application in a top-to-bottom fashion; we are bound to end up doing indirect testing on some parts of the application. 

An application with a layered architecture can be tested more effectively by testing each layer in isolation.

We design the SUT using a layered architecture that separates the presentation logic from the business logic and from any persistence mechanism or interfaces to other systems. We put all business logic into a service layer that exposes the application functionality to the presentation layer as an API. We treat each layer of the architecture as a separate SUT. We write component tests for each layer independent of the other layers of the architecture. That is, for layer n of the architecture, the tests will take the place of layer n+1; we may optionally replace layer n-1 with a test double.

We can use a Layer Test whenever we have a layered architecture and we want to provide good test coverage of the logic in each layer. It can be much simpler to test each layer independently than it is to test all the layers at once. This is especially true when we want to do defensive coding for return values of calls across the layer boundary. In software that is working correctly, these errors should never happen; in real life, they do. To make sure our code handles these errors, we can inject these never happen scenarios as indirect inputs to our layer.

Layer Tests are very useful when we want to divide up the project team into subteams based on the technology in which the team members specialize. Each layer of an architecture tends to require different knowledge and often uses different technologies; therefore, the layer boundaries serve as natural team boundaries. Layer Tests can be a good way to nail down and document the semantics of the layer interfaces. 

Even when we choose to use a Layer Test strategy, it is a good idea to include a few top-to-bottom tests just to verify that the various layers are integrated correctly. These tests need to cover only one or two basic scenarios; we don’t need to test every business test condition because all of them have already been tested in the Layer Tests for at least one of the layers.

Subcutaneous Test

A Subcutaneous test is a degenerate form of Layer Test that bypasses the presentation layer of the system to interact directly with the Service Layer. 

Component Test 

This is the most general form of Layer Test, in that we can think of the layers being made up of individual components that act as micro-layers. They are a good way to specify or document the behavior of individual components when we are doing component-based development and some of the components must be modified or built from scratch.

Round-Trip Tests

A good starting point for Layer Tests is the round-trip test, as it should be sufficient for most simple success tests. These tests can be written such that they do not care whether we have fully isolated the layer of interest from the layers below. We can either leave the real components in place so that they are exercised indirectly or we can replace them with fake objects. The latter option is useful when database or asynchronous mechanisms in the layer below lead to slow tests. 

Controlling Indirect Inputs

We can replace a lower layer of the system with a test stub that returns canned results based on what the client layer passes in a request (customer 1 is a valid customer, 2 is a dormant customer, 3 has three accounts). This technique allows us to test the client logic with well-understood indirect inputs from the layer below. It is particularly useful when we are automating expected exception tests or when we are exercising behavior that depends on data that arrives from an upstream system. The alternative is to use back door manipulation to set up the indirect inputs.

Verifying Indirect Outputs

When we want to verify the indirect outputs of the layer of interest, we can use a mock object or test spy to replace the components in the layer below the SUT. We can then compare the actual calls made to the DoC with the expected calls. The alternative is to use back door manipulation to verify the indirect outputs of the SUT after they have occurred.


</Text>
        </Document>
        <Document ID="120">
            <Title>Maintaining the Test Driven Cycle</Title>
            <Text>Start Each Feature with an Acceptance Test

Start work on a new feature by writing failing acceptance tests that demonstrate that the system does not yet have the feature we’re about to write and track our progress towards completion of the feature.

Fig Each TDD cycle starts with a failing acceptance test


We write the acceptance test using only terminology from the application’s domain, not from the underlying technologies such as databases and web servers. This helps us understand what the system should do, without tying us to any of our initial assumptions about the implementation or complicating the test with technological details. This also shields our acceptance test suite from changes to the technical infrastructure. 

Unit tests exercise objects or small clusters of objects in isolation. They’re important to help us design classes and give us confidence that they work, but they don’t say anything about whether they work together with the rest of the system. Acceptance tests test the integration of unit tested objects and push the project forwards.

Separate Tests that Measure Progress from Those That Catch Regressions

When we write acceptance tests to describe a new feature, we expect them to fail until that feature has been implemented. Once passing, the acceptance tests now represent completed features and should not fail again.

We organize our test suites to reflect the different roles that the tests fulfill. Unit and integration tests support the development team, should run quickly and should always pass. Acceptance tests might take longer to run. 

If requirements change, we must move any affected acceptance tests out of the regression suite back into the in-progress suite, edit them to reflect the new requirements, and change the system to make them pass again.

Start Testing with the Simplest Success Case

Where do we start when we have to write a new class or feature? Degenerate cases don’t add value to the system and don’t give us enough feedback about the validity of our ideas. 

Start by testing the simplest success case. Once that’s working, we’ll have a better idea of the real structure of the solution and can prioritize failure and success cases. 

Write the Test That You’d Want to Read

We want each test to be clear as possible an expression of the behavior to be performed by the system or object. While writing the test, we ignore the fact that test won’t run, or even compile, and just concentrate on its text; we act as if the supporting code to let us run the test already exists.

When the test reads well, we then build up the infrastructure to support the test. We know we’ve implemented enough of the supporting code when the test fails in the way we’d expect, with a clear error message describing what needs to be done. Only then do we start writing the code to make the test pass. 

Watch the Test Fail

We always watch the test fail before writing the code to make it pass, and check the diagnostic message. If the test fails in a way we didn’t expect, we know we’ve misunderstood something or the code is incomplete, so we fix that. When we get the right failure, we check that the diagnostics are helpful. If the failure description isn’t clear, someone will struggle when the code breaks later. We adjust the test code and rerun the tests until the error messages guide us to the problem with the code.

Fig 5.2 Improving the diagnostics as part of the TDD cycle


As we write the production code, we keep running the test to see our progress and to check the error diagnostics as the system is built up behind the test. Where necessary, we extend or modify the support code to ensure the error messages are always clear and relevant.

Develop from the Inputs to the Outputs

We start developing a feature by considering the events coming into the system that will trigger the new behavior. The end-to-end tests for the feature will simulate these event arriving. At the boundaries of the system, we will need to write one or more objects to handle these events. As we do so, we discover that these objects need supporting services from the rest of the system to perform their responsibilities. We write more objects to implement these services, and discover what services these new object need in turn.

Unit Test Behavior Not Methods

Focus on the features that the object under test should provide, each of which may require collaboration with its neighbors and calling more than one of its methods. We need to know how to use the class to achieve the goal, not how to exercise all the paths through its code. Choose test names that describe how the object behaves in the scenario being tested.

Listen to the Tests

Stay alert for areas of the code that are difficult to test. Don’t just ask how to test it, but also why is it difficult to test.

The likely cause is poor design. The same structure that makes the code difficult to test now will make it difficult to change in the future. 

If we find it hard to write the next failing test, improve the design by refactoring the code. 

Fig 5.3 :

Expect unexpected changes guides development. If we keep up the quality of the system by refactoring when we see a weakness in the design, we will be able to make it respond to whatever changes turn up. The alternative is the software rot where the code decays until the team cannot respond to the needs of its customers.

Tuning the Cycle

There’s a balance between exhaustively testing execution paths and testing integration. If we test at too large a grain, the combinatorial explosion of trying all the possible paths through the code will bring development to a halt. Throwing exceptions will be impractical to test from that level. On the other hand, if we test at too fine a grain - just at the class level for example - the testing will be easier but we’ll miss problems that arise from objects not working together.

Designing for Maintainability

Separation of concerns

Higher levels of abstraction

Cockburn’s Port and Adapters architecture. Eric Evans anti-corruption layer is related to this.

Fig 6.1 An application’s core domain model is mapped onto technical infrastructure


No And, Or, or But

Every object should have a single, clearly defined responsibility. We should be able to describe what an object does without using any conjunctions (and, or). This principle also applies when we’re combining objects into new abstractions. If we’re packaging up behavior implemented across several objects into a single construct, we should be able to describe its responsibility clearly.

Composite Simpler than the Sum of Its Parts

When composing objects into a new type, we want the new type to exhibit simpler behavior than all of its component parts considered together. The composite object’s API must hide the existence of its component parts and the interactions between them, and expose a simpler abstraction to its peer. Think of a mechanical clock.

The API of a composite object should not be more complicated than that of any of its components. This rule raises the level of abstraction.

Context Independence

This rule helps us decide whether an object hides too much or hides the wrong information. A system is easier to change if its objects are context-independent; that is, if each object has no built-in knowledge about the system in which it executes. This allows us to take units of behavior (objects) and apply them in new situations. To be context-independent, whatever an object needs to know about the larger environment it’s running in must be passed in. Those relationships might be permanent (passed in on construction) or transient (passed in to the method that needs them).

Each object is told just enough to do its job and wrapped up in an abstraction that matches its vocabulary. Eventually, the chain of objects reaches a process boundary, which is where the system will find external details such as host names, ports and user interface event.

A class that uses terms from multiple domains is violating context independence unless it’s part of a bridging layer.

Hiding the Right Information

Example: encapsulate the name of application’s log file in the pricing policy class. Hiding details of the log file in PricingPolicy violates context independence - they’re concepts from different levels in the Russian doll structure of nested domains. If the log file is necessary, it should be passed in from a level that understands external configuration.



</Text>
        </Document>
        <Document ID="121">
            <Title>Achieving Object Oriented Design</Title>
            <Text>How Writing a Test First Helps the Design

1. Starting with a test means that we have to describe what we want to achieve before we consider how. This focus helps us maintain the right level of abstraction for the target object. It also helps us with information hiding as we have to decide what needs to be visible from outside the object.

2. To keep unit tests understandable, we have to limit their scope. Unit tests that are dozens of lines long makes the test obscure. It tells us that the component tested is too large and needs breaking up into smaller components. The resulting composite object should have a clearer separation of concerns as we tease out its implicit structure, and we can write simpler tests for the extracted objects.

3. To construct an object for a unit test, we have to pass its dependencies to it, which means we have to know what they are. This encourages context independence, since we have to be able to set up the target object’s environment before we can unit-test it - a unit test is just another context. We’ll notice that an object with implicit (or just too many) dependencies is painful to prepare for testing - and make a point of cleaning it up.

Communication over Classification

Communication patterns between objects are more important. Interfaces define the available messages between objects, but we also need to define their patterns of communication - their communication protocols. We do what we can with naming and convention, but there’s nothing in the language to describe relationships between interfaces or methods within an interface, which leaves a significant part of the design implicit.

Interface and Protocol

An interface describes whether two components will fit together, while protocol describes whether they will work together.

We use TDD with mock objects as a technique to make these communication protocols visible, both as a tool for discovering them during development and as a description when revisiting the code. For example, the unit test: given a certain input message, the translator should call listener.auction_closed exactly once and nothing else. 

TDD with mock objects also encourages information hiding. We should mock an object’s peers but not its internals. Tests that highlight an object’s neighbors help us to see whether they are peers or should instead be internal to the target object. A test that is clumsy or unclear is a hint that we’ve exposed too much implementation, and that we should rebalance the responsibilities between the object and its neighbors.

Value Types

Values are immutable, so they’re simpler and have no meaningful identity. Objects have state, so they have identity and relationships with each other. Define types to represent value concepts in the domain. It helps to create a consistent domain model that is self-explanatory. 

The discovery of value types is motivated by applying design principles rather than by responding to code stresses when writing tests.

Breaking Out : Splitting a Large Object into a Group of Collaborating Objects

When we find that the code in an object is becoming complex, it’s a sign that it’s implementing multiple concerns and that we can break out coherent units of behavior into helper types. 

Break up an object if it becomes too large to test easily, or if its test failures become difficult to interpret. Then unit-test the new parts separately.

Budding Off : Defining a New Service that an Object Needs and Adding a New Object to Provide It

When we want to mark a new domain concept in code, we often introduce a placeholder type that wraps a single field, or maybe has no fields at all. As the code grows, we fill in more detail in the new type by adding fields and methods. With each type that we add, we’re raising the level of abstraction.

When the code is more stable and has some degree of structure, we often discover new types by pulling them into existence. We might be adding behavior to an object and find that, following our design principles, some new feature doesn’t belong inside it.

Create an interface to define the service that the object needs from the object’s point of view. Write tests for the new behavior as if the service already exists, using mock objects to help describe the relationship between the target object and its new collaborator.

The development cycle goes like this. When implementing an object, we discover that it needs a service to be provided by another object, we give the new service a name and mock it out in the client object’s unit tests, to clarify the relationship between the two. Then we write an object to provide that service and in doing so, discover what services that object needs. We follow this chain (or perhaps a directed graph) of collaborator relationships until we connect up to existing objects, either our own or from a third-party API. This is how we implement “Develop from the Inputs to the Outputs”.

This is “on-demand” design. We pull interfaces and their implementations into existence from the needs of the client.

When writing a test, we ask ourselves, “If this worked, who would know?”. If the right answer to that question is not in the target object, it’s time to introduce a new collaborator.

Bundling Up : Hiding Related Objects into a Containing Object

When we notice that a group of values are always used together, we take that as a suggestion that there’s a missing construct. A first step might be to create a new type with fixed public fields - just giving the group a name highlights the missing concept. Later we can migrate behavior to the new type, which might eventually allow us to hid its fields behind a clean interface, satisfying the composite simpler than the sum of its parts rule.

This is the application of the Composite Simpler than the Sum of its Parts rule. When we have a cluster of related objects that work together, we can package them up in a containing object. The new object hides the complexity in an abstraction that allows us to program at a higher level.

The process of making an implicit concept concrete has benefits. First, we have to name it which helps us understand the domain better. Second, we can scope dependencies more clearly, since we can see the boundaries of the concept. Third, we can be more precise with our unit testing. We can test the new composite object directly, and use a mock implementation to simplify the tests for code from which it was extracted (since we added an interface for the role the new object plays).

When the test for an object becomes too complicated to set up - when there are too many moving parts to get the code into the relevant state - consider bundling up some of the collaborating objects.

Identify Relationships with Interfaces

Use interfaces to emphasize on the relationships between objects, as defined by their communication protocols. Use interfaces to name the roles that objects can play and to describe the messages they’ll accept.

Compose Objects to Describe System Behavior

TDD at the unit level guides us to decompose our system into value types and loosely coupled computational objects. The tests give us a good understanding of how each object behaves and how it can be combined with others. We then use lower-level objects as the building blocks of more capable objects; this is the web of objects.

We assemble a description of the expected calls for a test in a context object called a Mockery. During a test run, the Mockery will pass calls made to any of its mocked objects to its Expectations, each of which will attempt to match the call. If an Expectation matches, that part of the test succeeds. If none matches, then each Expectation reports its disagreement and the test fails. At runtime the assembled objects look like:

Fig Mock expectations are assembled from many objects

The advantage of this approach is that we end up with a flexible application structure built from relatively little code. It’s particularly suitable where the code has to support many related scenarios. For each scenario, we provide a different assembly of components to build, in effect, a subsystem to plug into the rest of the application. Such designs are also easy to extend - just write a new plug compatible component and add it in. You just need to write the new matchers.

For example, we have a Mock check that a method example.do_something() is called exactly once with an argument of type string:

Building Up to a Higher-Level Programming

Clearly express what the expectation is testing. Conceptually, assembling a web of objects is straightforward. Code should help us understand the behavior of the system we’re assembling and express intent clearly. We can start from the declarative code we’d like to have and work down to fill in its implementation.

Our purpose is to achieve more with less code. We aspire to raise ourselves from programming in terms of control flow and data manipulation, to composing programs from smaller programs - where objects from the smallest unit of behavior. It’s the same concept as building up layers of language in LISP or programming UNIX by composing utilities with pipes.

</Text>
        </Document>
        <Document ID="70">
            <Title>Tek Pub Screencast</Title>
            <Text>Tek Pub Screencast

Name of the test method:

unit_scenario_expected_behavior

Rules:

1. Do not change the source code unless a test demands it. Not applicable to Refactoring step.</Text>
        </Document>
        <Document ID="107">
            <Title>Transient Fixture Management</Title>
            <Text>In Implicit Setup, all the fixture creation logic goes into setup method. This approach can make the tests more difficult to understand because we cannot see how the pre-conditions of the test (the test fixture) correlate with the expected outcome within the test method.

Misuse of the setup Method

The setup method is most prone to misuse when it is applied to build a general fixture with multiple distinct parts each of which is dedicated to a different test method. This can lead to slow tests. More importantly, it can lead to obscure tests by hiding the cause - effect relationship between the fixture and the expected outcome of exercising the SUT. 

If we do not adopt the practice of grouping the test methods into test case classes based on identical fixtures but we do use the setup method, we should build only the lowest common denominator part of the fixture in the setup method. That is, only the setup logic that will not cause problems in any of the tests should be placed in the setup method.

A general fixture is a common cause of slow tests because each test spends much more time than necessary building the test fixture. It also tends to produce obscure tests because the reader cannot easily see which part of the fixture a particular test method depends on. A general fixture often evolves into a fragile fixture as the relationship between its various elements and the tests that use them is forgotten over time. Changes made to the fixture to support a newly added test may then cause existing tests to fail.

</Text>
        </Document>
        <Document ID="114">
            <Title>xUnit Basic Patterns</Title>
            <Text>Test Method

We organize the test logic following one of the standard test method templates to make the type of test easily recognizable. In a simple success test, we have a purely linear flow of control from fixture setup through exercising the SUT to result verification. In an expected exception test, language based structures direct us to error-handling code. If we reach that code, we pass the test; if we don’t, we fail it.

We follow the standard test templates to keep test methods as simple as possible. This greatly increases their utility as system documentation by making it easier to find the description of the basic behavior of the SUT. It is a lot easier to recognize which tests describe this basic behavior if only expected exception tests contain error-handling language constructs.

Simple Success Test

Most software has an obvious success scenario (happy path). A simple success test verifies the success scenario in a simple and easily recognized way. We create an instance of the SUT and call the method that we want to test. We then assert that the expected outcome has occurred. We follow the normal steps of a four phase test. What we don’t do is catch any exceptions that could happen. Another benefit of avoiding exception construct in the code is that when errors occur, it is easier to track them down. 

Expected Exception Test

Writing software that passes the simple success test is pretty straightforward. Most of the defects in software appear in the various alternative paths - especially the ones that relate to error scenarios, because these scenarios are often untested requirements.

An expected exception test helps us verify that the error scenarios have been coded correctly. We set up the test fixture and exercise the SUT in each way that should result in an error. We ensure that the expected error has occurred by using language construct to catch the error. If the error is raised, flow will pass to the error-handling block. This diversion may be enough to let the test pass, but if the type or message contents of the exception or error is important (such as when the error message will be shown to a user), we can use an equality assertion to verify it. If the error is not raised, we call fail to report that the SUT failed to raise an error as expected.

We should write an expected exception test for each kind of exception that the SUT is expected to raise. It may raise the error because the client (i.e., our test) has asked it to do something invalid, or it may translate or pass through an error raised by some other component it uses. We should not write an expected exception test for exceptions that the SUT might raise but that we cannot force to occur on cue, because these kinds of errors should show up as test failures in the simple success tests. If we want to verify that these kinds of error are handled properly, we must find a way to force them to occur. The most common way to do so is to use a test stub to control the indirect input of the SUT and raise the appropriate errors in the test stub.

Ruby can provide special assertions to which we pass the block of code to be executed as well as the expected exception / error object. 

M def test_invalid_mileage_input
 M  flight = Flight.new
M   assert_raises(RuntimeError, “Should have raised error”) do
 M     flight.mileage(-10)
M   end
Mend

The code between the do/end pair is a closure that is executed by the assert_raises method. If it doesn’t raise an instance of the first argument (the class RuntimeError), the test fails and presents the error message supplied.

Four-Phase Test

How do we structure our test logic to make what we are testing obvious?
We structure each test with four distinct parts executed in sequence.

We design each test to have four distinct phases that are executed in sequence: fixture setup, exercise SUT, result verification, and fixture teardown.
In the first phase, we set up the test fixture (the before picture) that is required for the SUT to exhibit the expected behavior as well as anything you need to put in place to be able to observe the actual outcome (such as using a test double)
In the second phase, we interact with the SUT.
In the third phase, we do whatever is necessary to determine whether the expected outcome has been obtained.
In the fourth phase, we tear down the test fixture to put the world back into the state in which we found it.

The reader must be able to quickly determine what behavior the test is verifying. Clearly identifying the four phases makes the intent of the test much easier to see.

The fixture setup phase of the test establishes the SUT’s state prior to the test, which is an important input to the test. The exercise SUT phase is where we actually run the software we are testing. When reading the test, we need to see which software is being run. The result verification phase of the test is where we specify the expected outcome. The final phase, fixture teardown, is all about housekeeping. We wouldn’t want to obscure the important test logic with it because it is completely irrelevant from the perspective of tests as documentation.

We should avoid the temptation to test as much functionality as possible in a single test method because that can result in obscure tests. In fact it is preferable to have many small single condition tests. Using comments to mark the phases of a Four-Phase Test is a good source of self-discipline, in that it makes it very obvious when our tests are not single condition tests. It will be self-evident if we have multiple exercise SUT phases separated by result verification phases or if we have interspersed fixture setup and exercise SUT phases. Sure, the tests may work - but they will provide less defect localization than if we have a bunch of independent single condition tests.

Four-Phase Test - Inline

All four phases of the test are included as in-line code.

Four-Phase Test - Implicit Setup/Teardown

The fixture setup and teardown is moved out of the test method.

Assertion Method

How do we make tests self-checking?

We call a utility method to evaluate whether an expected outcome has been achieved.

A key part of writing fully automated tests is to make them self-checking tests to avoid having to inspect the outcome of each test for correctness each time it is run. This strategy involves finding a way to express the expected outcome so that it can be verified automatically by the test itself.

Assertion methods give us a way to express the expected outcome in a way that is both executable by the computer and useful to the human reader, who can then use tests as documentation.

We encode the expected outcome of the test as a series of assertions that state what should be true for the test to pass. The assertions are realized as calls to assertion methods that encapsulate the mechanism that causes the test to fail. The assertion methods may be provided by the test automation framework or by the developer as custom assertions.

How to call the assertion methods
How to choose the best assertion method to call
What information to include in the assertion message

Assertion Messages

Assertion methods typically take an optional assertion message as a text parameter that is included in the output when the assertion fails. This structure allows the developer to explain to the maintainer exactly which assertion method failed and to better explain what should have occurred. The error detected by the test will be much easier to debug if the assertion method provides more information about why it failed.

Choosing the Right Assertion

We have two goals for the calls to assertion methods:
Fail the test when something other than the expected outcome occurs
Document how the SUT is supposed to behave

To achieve these goals we pick the most appropriate assertion method. Assertions fall into the following categories:
Single-Outcome assertions such as fail; these take no arguments because they always behave the same way.
Stated Outcome assertions such as assert_not_nil(object) and assert_true(boolean_expression); these compare a single argument to an outcome implied by the method name.
Expected Exception assertions such as assert_raises(expected_error) { code to execute }; these evaluate a block of code and a single expected exception argument.
Equality assertions such as assert_equal(expected, actual); these compare two objects or values for equality.
Fuzzy Equality assertions such as assert_equal(expected, actual, tolerance); these determine whether two values are close enough to each other by using tolerance or comparison mask.

Equality Assertion

Equality assertions are the most common examples of assertion methods. They are used to compare the actual outcome with an expected outcome that is expressed in the form of a constant literal value or an expected object. By convention, the expected value is specified first and the actual value follows it. 

The diagnostic message that is generated by the test automation framework makes sense only when they are provided in this order. The equality of the two objects is determined by invoking the equals method on the expected object. If the SUT’s definition of equals is not what we want to use in our tests, either we can make equality assertions on individual fields of the object or we can implement our test specific equality on a test specific subclass of the expected object.

Fuzzy Equality Assertion

When we cannot guarantee an exact match due to variations in precision or expected variations in value, we use a Fuzzy Equality Assertion. Typically, these assertions look just like Equality assertions with the addition of an extra tolerance or comparison map parameter that specifies how close the actual argument must be to the expected one. The most common example is the comparison of floating-point numbers where the limitations of arithmetic precision need to be accounted for by providing a tolerance (the maximum acceptable distance between the two values).

The same approach can be used when comparing XML documents. In this case, the fuzz specification is a comparison schema that specifies which fields need to match or which fields should be ignored. This is similar to asserting that a string conforms to a regular expression or other form of pattern matching.

Stated Outcome Assertion

Stated outcome assertions are a way of saying exactly what the outcome should be without passing an expected value as an argument. The outcome must be common enough to warrant a special assertion method. The most common examples are: assert_true(boolean_expression) which fails if the expression evaluates to false; assert_not_null(object), which fails if the object doesn’t refer to a valid object.

They are often used as guard assertions to avoid conditional test logic.

Expected Exception Assertion

A variation of the stated outcome assertion that takes an additional parameter specifying the kind of exception we expect. We can use this expected exception assertion to say “Run this block and verify that the following exception is thrown”.

assert_raises(expected_error) { code to execute } 

Single-Outcome Assertion

A single outcome assertion always behaves the same way. The most commonly used single outcome assertion is fail, which causes a test to be treated as a failure. It is typically used in two circumstances:
As an unfinished test assertion when a test is first identified and implemented as a nearly empty test method. By including a call to fail, we have the test runner remind us that we still have a test to finish writing.
As part of a rescue block in an expected exception test by including a call to fail in the begin block immediately after the call that is expected to throw an exception. If we don’t want to assert something about the exception that was caught, we can avoid an empty rescue block by using the single outcome assertion success to document that this is the expected outcome.

One circumstance in which we really should not use single outcome assertions is in conditional test logic. There is no good reason to include conditional logic in a test method, as there is usually a more declarative way to handle this situation using other styles of assertion methods. For example, use of guard assertions results in tests that are more easily understood and less likely to yield incorrect results.

Notes: The method fail_not_equals is a test utility method that fails the test and provides a diagnostic assertion message.

Equality Assertion

Assert_equals(x,y)

Fuzzy Equality Assertion

To compare two floating-point numbers (which are rarely ever really equal), we specify the acceptable differences using a fuzzy equality assertion:
Assert_equals(3.1415, diameter/2/radius, 0.001)
A assert_equals(expected_xml, actual_xml, elements_to_compare)

Stated Outcome Assertion

To insist that a particular outcome has occurred, we use a stated outcome assertion: assert_nut_null(a); assert_true(b > c); assert_not_zero(a)

Expected Exception Assertion

Here is an example of how we verify that the correct exception was raised :

A assert_raised(RuntimeError, “Should have raised error”) { flight.mileage(-10) }

Single Outcome Assertion

To fail a test, use the single outcome assertion: fail(“Expected an exception”); unfinished_test

Assertion Message

How do we structure our test logic to know which assertion failed?
We include a descriptive string argument in each call to an assertion method.

We make tests self-checking by including calls to assertion methods that specify the expected outcome. When a test fails, the test runner writes an entry to the test result log.

A well-crafted assertion message makes it easy to determine which assertion failed and exactly what the symptoms were when the failure happened.

Every assertion method takes an optional string parameter that is included in the failure log. It is useful to take a moment as we write each assertion and ask ourselves what the person reading the failure log would hope to get out of it.

Assertion Identifying Message

Use the name of the variable or attribute being asserted on as the message. 

Expectation Describing Message

What should have happened? Include a description of the expectation in the assertion message. While this is done automatically for an equality assertion, we need to provide this information ourselves for any stated outcome assertions.

A assert_true(“Expected a > b but a was #{a} and b was #{b}”, a > b)

The output would be even more meaningful if the variables had intent revealing names.

Argument Describing Message

Include the expression that was being evaluated (including the actual values) as part of the assertion message. The reader can then examine the failure log and determine what was being evaluated and why it caused the test to fail.

Testcase Class Discovery

Using common location and test case superclass

The following example finds all files with .rb extension in the tests directory and requires them from this file. This causes Test::Unit to look for all tests in each file because the Testcase class in each file extends Test::Unit::TestCase

Dir[’tests/*.rb’].each do |file|
  Require file
End

The Dir[‘tests/*.rb’] returns a collection of files over which the each method iterates with the block containing “requires file” to implement Testcase class discovery. The ruby interpreter and Test::Unit finish the job by doing test method discovery on each required class.



</Text>
        </Document>
        <Document ID="71">
            <Title>Franc</Title>
            <Text>$5 + 10 CHF = $10 if rate is 2:1
5 CHF *2 = 10 CHF

How are we going to approach the first test on that list? That’s the test that ’s most interesting. It still seems to be a big leap. I’m not sure I can write a test that I can implement in one little step. A pre-requisite seems to be having an object like Dollar, but to represent Francs. If we can get Francs working like Dollars work now, we’ll be closer to being able to write and run the mixed addition test.

We can copy and edit the Dollar test:

Test_franc_multiplication
Five = Franc.new(5)
Assert(Franc(10), five.times(2))
Assert(Franc(15), five.times(3))

Simplification we made in the last chapter made our job here easier. What short step will get us to a green bar? Copying the Dollar code and replacing “Dollar” with “Franc”.

Copy and paste reuse? Remember, our cycle has different phases (they go by quickly, often in seconds, but they are phases):

1. Write a test
2. Make it compile
3. Run it to see that it fails
4. Make it run
5. Remove duplication

The different phases have different purposes. They call for different styles of solution, different aesthetic viewpoints. The first three phases need to go by quickly, so we get to a known state with the new functionality. You can commit any number of sins to get there, because speed trumps design, just for that brief moment.

This does not mean that you abandon all the principles of good design. The cycle is not complete.  The first four steps of the cycle won’t work without the fifth. Make it run, make it right.

Class Franc

Def initialize(amount)
  @Amount=amount
End

Times(multiplier)
Return Franc.new(amount.multiplier)
End

Def equals(object)
  Return amount == franc.amount
End

Because the step to running code was so short, we were even able to skip the “make it compile” step.

New we have the duplication galore, and we have to eliminate it before writing our next test. We’ll start by generalizing equals. However, we can cross off an item, even though we have to add two more. Reviewing, we:
Couldn’t tackle a big test, so we invented a small test that represented progress
Wrote the test by shamelessly duplicating and editing
Even worse, made the test work by copying and editing model code
Promised ourselves we wouldn’t go home until the duplication was gone.

$5 * 2 = $10
</Text>
        </Document>
        <Document ID="122">
            <Title>Building on Third Party Code</Title>
            <Text>The critical point about third-party code is that we don’t control it, so we cannot use our process to guide its design. Instead we must focus on the integration between our design and the external code.

In integration, we have an abstraction to implement, discovered while we developed the rest of the feature. With the third-party API pushing back at our design, we must find the best balance between elegance and practical use of someone else’s ideas. We must check that we are using the third-party API correctly, and adjust our abstraction to fit if we find that our assumptions are incorrect.

Only Mock Types that You Own. Don’t Mock Types You Can’t Change

Tests that mock external libraries often need to be complex to get the code into the right state for the functionality we need to exercise. The mess in such tests is telling us that the design isn’t right but, instead of fixing the problem by improving the code, we have to carry the extra complexity in both code and test.

A second risk is that we have to be sure that the behavior we stub or mock matches what the external library will actually do. The difficulty of this depends on the quality of the library - whether it’s specified and implemented well enough for us to be certain that our unit tests are valid. Even if we get it right once, we have to make sure that the tests remain valid when we upgrade the libraries.

Write an Adapter Layer

If we don’t want to mock an external API, how can we test the code that drives it? We will have used TDD to design interfaces for the services our objects need - which will be defined in terms of our domain, not the external library.

We write a layer of adapter objects that uses the third-party API to implement these interfaces as in Fig 8.1. Mockable adapters to third-party objects.



We keep this layer as thin as possible, to minimize the amount of potentially brittle and hard-to-test code. We test these adapters with focused integration tests to confirm our understanding of how the third-party API works. There will be relatively few integration tests compared to the number of unit tests, so they should not get in the way of the build even if they’re not as fast as the in-memory unit tests.

Following this approach consistently produces a set of interfaces that define the relationship between our application and the rest of the world in our application’s terms and discourages low-level technical concepts from leaking into the application domain model. 

There are some exceptions where mocking third-party libraries can be helpful. We might use mocks to simulate behavior that is hard to trigger with the real library, such as throwing exceptions. Similarly, we might use mocks to test a sequence of calls, for example making sure that a transaction is rolled back if there’s a failure. There should not be many tests like this in a test suite.

This pattern does not apply to value types because, of course, we don’t need to mock them. We still, however, have to make design decisions about how much to use third-party value types in our code. They might be so fundamental that we just use them directly. Often, however, we want to follow the same principles of isolation as for third-party services, and translate between value types appropriate to the application domain and to the external domain.

Mock Application Objects in Integration Tests

Adapter objects are passive, reacting to calls from our code. Sometimes, adapter objects must call back to objects from the application. Event based libraries, for example, usually expect the client to provide a callback object to be notified when an event happens. In this case, the application code will give the adapter its own event callback (defined in terms of the application domain). The adapter will then pass an adapter callback to the external library to receive external events and translate them for the application callback.

In these cases, we do use mock objects when testing objects that integrate with third-party code - but only to mock the callback interfaces defined in the application, to verify that the adapter translates events between domains correctly.

Fig 8.2 Using mock objects in integration tests


Fig goes here…</Text>
        </Document>
        <Document ID="72">
            <Title>Common Equals</Title>
            <Text>We got a new test case working. However, we sinned mightily in copy/paste of code so we could do it quickly. Now it is time to clean up.

One possibility is to make one of our classes extend the other. I tried it, and it hardly saves any code at all. Instead, we are going to find a common superclass for the two classes. What if we had a Money class to capture the common equals code? We can start small:
Class Money

All the tests still run (not that we could possibly have broken anything, but that’s a good time to run the tests anyway)

If Dollar extends Money, that can’t possibly break anything.

Class Dollar &lt; Money

Can it? No, the tests still all run. Now we can move the “amount” instance variable up to Money:

Class Money
Def initialize(amount)
@Amount=amount
End

Class Dollar &lt; Money

Now we can move up the equals code:

Class Money now has equals method. 

All the tests still run.

Now we need to eliminate Franc#equals. First we notice that the tests for equality don’t cover comparing Francs to Francs. Before we change the code, we’ll write the tests that should have been there in the first place.

You will often be TDDing in code that doesn’t have adequate tests. When you don’t have enough tests, you are bound to come across refactorings that aren’t supported by tests. You could make a refactoring mistake and tests would all still run. What do you do?

Write the test you wish you had. If you don’t, you will eventually break something while refactoring. Then you’ll get bad feelings about refactoring and stop doing it so much. Then your design will deteriorate. So, retroactively test before refactoring.

Fortunately, here the tests are easy to write. We just copy the tests for Dollar:

Test_equality

Assert(Dollar.new(5).equals(Dollar.new(5))
AssertFalse(Dollar.new(5).equals(Dollar.new(6))
Assert(Franc.new(5).equals(Franc.new(5))
AssertFalse(Franc.new(5).equals(Franc.new(6))

More duplication, two lines more.

Tests in place, we can have Franc extend Money

Class Franc &lt; Money

Remove equals from Franc. And run the tests. They run. 

What happens when we compare Francs and Dollars? We’ll get to that in the next chapter. Reviewing what we did here, we:
Stepwise moved common code from one class (Dollar) to a superclass (Money)
Made a second class (Franc) also a subclass
Reconciled two implementations (equals)
</Text>
        </Document>
        <Document ID="108">
            <Title>Result Verification</Title>
            <Text>Options for verifying that the SUT has behaved correctly, including exercising the SUT and comparing the actual outcome with the expected outcome.

There is a lot more to writing good tests than just calling the built-in assertion methods. We also need to learn key techniques for making tests easy to understand and for avoiding and removing test code duplication.

A key challenge in coding the assertions is getting access to the information we want to compare with the expected results. This is where observation points come into play; they provide a window into the state or behavior of the SUT so that we can pass it to the assertion methods. 

Verify State or Behavior

Ultimately, test automation is about verifying the behavior of the SUT. Some aspects of the SUT’s behavior can be verified directly; the value returned by a function is a good example. Other aspects of the behavior are more easily verified indirectly by looking at the state of some object. We can verify the actual behavior of the SUT in our tests in two ways:

1. We can verify the states of various objects affected by the SUT by extracting each state using an observation point and using assertions to compare it to the expected state.
2. We can verify the behavior of the SUT directly by using observation points inserted between the SUT and its DoC to monitor its interactions (in the form of the method calls it makes) and comparing those method calls with what we expected.

State Verification is done using assertions and is the simpler of the two approaches. Behavior Verification is more complicated and builds on the assertion techniques we use for verifying state.

State Verification

The normal way to verify the expected outcome has occurred is called state verification. First we exercise the SUT; then we examine the post-exercise state of the SUT using assertions. We may also examine anything returned by the SUT as a result of the method call we made to exercise it. What is most notable is what we do not do: we do not instrument the SUT in any way to detect how it interacts with other components of the system. That is, we inspect only direct outputs and we use only direct method calls as our observation points.

Fig 10.1 State Verification. In state verification, we assert that the SUT and any objects it returns are in the expected state after we have exercised the SUT. We pay no attention to the man behind the curtain.

State verification can be done in tow slightly different ways. Procedural state verification involves writing a sequence of assertions that pick apart the end state of the SUT and verify that it is as expected. Expected object is a way of describing the expected state in such a way that it can be compared with a single assertion method call. This approach minimizes test code duplication and increases test clarity. With both strategies we can use either “built-in” assertions or custom assertions.

Using Built-in Assertions

There are different assertion methods:

Stated Outcome Assertions such as assert(boolean_expression)

Simple Equality Assertions such as assert(expected, actual)

Fuzzy Equality Assertions such as assert(expected, actual, tolerance) which are used for comparing floats.

Each test should make it very clear that “When the system is in state S1 and I do X, the result should be R and system should be in state S2”. We put the system into state S1 in our fixture setup logic. “I do X” corresponds to the exercise SUT phase of the test. “The result is R” and “the system is in state S2” are implemented using assertions. Thus we want to write our assertions in such a way that they succinctly describe R and S2. When the test fails we want the failure message to tell us enough to enable us to identify the problem. Therefore, we should always include an assertion message. It makes integration build failures much easier to reproduce and fix. It also makes troubleshooting broken tests easier by telling us what should have happened; the actual outcome tells us what did happen.

We can make the assertion output much more specific by using an argument describing message constructed by incorporating useful bits of data into the message. A good start is to include each of the values in the expression passed as the assertion method’s arguments.

Verifying Behavior

Verifying behavior is more complicated than verifying state because behavior is dynamic. We have to catch the SUT in the act as it generates indirect outputs to the objects it depends on. Two basic styles of behavior verification are : Procedural Behavior Verification and Expected Behavior. Both require a mechanism to access the outgoing method calls of the SUT (its indirect outputs). 

Fig 10.2 Behavior Verification. In behavior verification, we focus our assertions on the indirect outputs (outgoing interfaces) of the SUT. This typically involves replacing the DoC with something that facilitates observing and verifying the outgoing calls.

Procedural Behavior Verification

In procedural behavior verification, we capture the behavior of the SUT as it executes and save that data for later retrieval. The test then compares each output of the SUT one by one with the corresponding expected output. Thus in procedural behavior verification, the test executes a procedure (a set of steps) to verify the behavior.

The key challenge in procedural behavior verification is capturing the behavior as it occurs and saving it until the test is ready to use this information. This task is accomplished by configuring the SUT to use a fake object instead of the depended-on class. After the SUT has been exercised, the test retrieves the recording of the behavior and verifies it using assertions.

Expected Behavior Specification

Expected behavior is often used in conjunction with layer-crossing tests to verify the indirect outputs of an object or component. We configure a mock object with the method calls we expect the SUT to make to it and install this object before exercising the SUT.

Reducing Test Code Duplication

In result verification logic, test code duplication usually shows up as a set of repeated assertions. Techniques to reduce the number of assertions : Expected Objects, Custom Assertions, Verification Methods

Avoiding Conditional Test Logic

Conditional test logic is bad because the same test may execute differently in different circumstances. The only way to verify our test method is to manually edit the SUT so that it produces the error we want to be detected. 

Eliminating if Statements

Use Guard Assertion

Eliminating Loops

Loops in the test method creates three problems:
It introduces untestable test code because the looping code, which is part of the test, cannot be tested with fully automated tests.
It leads to obscure tests because all that looping code obscures the real intent.
Complexity of writing the looks can discourage the developer from writing the self-checking test.

A better solution is to delegate this logic to a test utility method with an intent-revealing name which can be both tested and reused.

Techniques for Writing Easy to Understand Tests

Working Backward, Outside-In

A useful little trick for writing very intent revealing code is to work backward. Start with an end in mind. To do so, we write the last line of the test first. For a function, its whole reason for existence is to return a value; for a procedure, it is to produce one or more side effects by modifying something. For a test, the purpose is to verify that the expected outcome has occurred (by making assertions).

Working backward means we write these assertions first. We assert on the values of suitably named local variables to ensure that the assertion is intent-revealing. The reset of writing the test simply consists of filling in whatever is needed to execute those assertions: We declare variables to hold the assertion arguments and initialize them with the appropriate content. Because at least one argument should have been retrieved from the SUT, we must, of course, invoke the SUT. To do so, we may need some variables to use as SUT arguments. Declaring and initializing a variable after it has been used forces us to understand the variable better when we introduce it. This scheme also results in better variable names and avoid meaningless names.

Working outside-in (top-down) means staying at a consistent level of abstraction. The test method should focus on what we need to have in place to induce the relevant behavior in the SUT. The mechanics of how we reach that place should be delegated to a lower layer of test software. In practice, we code this behavior as calls to test utility methods, which allows us to stay focused on the requirements of the SUT as we write each test method. We don’t need to worry about how we will create that object or verify that outcome; we merely need to describe what that object or outcome should be. The utility method we just used but haven’t yet defined acts as a placeholder for the unfinished test automation logic. We should always give this method an intent revealing name and stub it out with a call to the fail assertion to remind ourselves that we still need to write the method’s body. We can move on to writing the other tests we need for this SUT while they are still fresh in our minds. Later, we can switch to our toolsmith hat and implement the test utility methods.

Using TDD to Write Test Utility Methods

Once we are finished writing the test method that used the test utility method, we can start the process of writing the test utility method itself. Along the way, we can take advantage of TDD by writing test utility tests. It doesn’t take very long to write these unit tests that verify the behavior of our test utility methods and we will have much more confidence in them.

We start with a simple case (say, asserting the equality of two identical collections that hold the same item) and work up to the most complicated case that the test methods actually require (say, tow collections that contain the same two items but in different order). TDD helps us to find the minimal implementation of the test utility method, which may be much simpler than a complete generic solution. There is no point in writing generic logic that handles cases that aren’t actually needed but it may be worthwhile to include a guard assertion or two inside the custom assertion to fail tests in cases it doesn’t support.

</Text>
        </Document>
        <Document ID="115">
            <Title>Growing OO Sotware Guided by Tests</Title>
            <Text>Testing is no longer just about keeping defects from the users; instead, it’s about helping the team to understand the features that the users need and to deliver those features reliably and predictably.

What is the Point of TDD?

One must learn by doing the thing; for though you think you know it, you have no certainty, until you try it.

Software Development as a Learning Process

Everyone involved in a software project has to learn as it progresses. For the project to succeed, the people involved have to work together just to understand what they’re supposed to achieve and to identify and resolve misunderstanding along the way. They all know there will be changes, they just don’t what changes. They need a process that will help them cope with uncertainty as their experience grows - to anticipate unanticipated changes.

Feedback is the Fundamental Tool

The best approach a team can take is to use empirical feedback to learn about the system and its use, and then apply that learning back to the system. A team needs repeated cycles of activity. In each cycle it adds new features and gets feedback about the quantity and quality of the work already done. The team members split the work into time boxes, within which they analyze, design, implement, and deploy as many features as they can.

The fundamental TDD Cycle

As we develop the system, we use TDD to give us feedback on the quality of both its implementation (Does it work?) and design (Is it well structured?). 

Writing tests:

Makes us clarify the acceptance criteria for the next piece of work - we have to ask ourselves how we can tell when we’re done (design)
Encourages us to write loosely coupled components, so they can easily be tested in isolation and, at higher levels, combined together (design)
Adds an executable description of what the code does (design)
Adds to a complete regression suite (implementation)

Running tests:

Detects errors while the context is fresh in our mind (implementation)
Lets us know when we’ve done enough, discouraging gold plating and unnecessary features (design)

The Bigger Picture

When implementing a feature, start by writing an acceptance test, which exercises the functionality we want to build. While it’s failing, an acceptance test demonstrates that the system does not yet implement that feature; when it passes, we’re done. When working on a feature, we use its acceptance test to guide us as to whether we actually need the code we’re about to write - we only write code that’s directly relevant. Underneath the acceptance test, we follow the unit level test/implement/refactor cycle to develop the feature; the whole cycle looks like:

Fig 1.2 Inner and Outer Feedback Loops in TDD

The outer test loop is a measure of demonstrable progress, and the growing suite of tests protects us against regression failures when we change the system. Acceptance tests often take a while to make pass, certainly more than one check-in episode, so we usually distinguish between acceptance tests we’re working on (which are not yet included in the build) and acceptance tests for the features that have been finished (which are included in the build and must always pass).

The inner loop supports the developers. The unit tests help us maintain the quality of the code and should pass soon after they’ve been written. 

Testing End-to-End

Wherever possible, an acceptance test should exercise the system end-to-end without directly calling its internal code. An end-to-end test interacts with the system only from the outside: through its user interface, by sending messages as if from third-party systems, by invoking its web services, by parsing reports and so on. The whole behavior of the system includes its interaction with its external environment. This is often the riskiest and most difficult aspect; we ignore it at our peril. We try to avoid acceptance tests that just exercise the internal objects of the system, unless we really need the speed-up and already have a stable set of end-to-end tests to provide cover.

Levels of Testing

Acceptance : Does the whole system work?
Integration : Does our code work against code we can’t change?
Unit : Do our objects do the right thing, are they convenient to work with?

We use acceptance tests to help us, with the domain experts, understand and agree on what we are going to build next. We also use them to make sure that we haven’t broken any existing features as we continue developing. 

Integration tests refer to the tests that check how some of our code works with code from outside the team that we can’t change. It might be a public framework or library from another team within our company. The distinction is that integration tests make sure that any abstractions we build over third-party code work as we expect. They help tease out configuration issues and to give quicker feedback than the slower acceptance tests.

A unit test for an objects needs to create the object, provide its dependencies, interact with it, and check that it behaved as expected. So, for a class to be easy to unit-test, the class must have explicit dependencies that can easily be substituted and clear responsibilities that can easily be invoked and verified. This means that the code must be loosely coupled and highly cohesive.</Text>
        </Document>
        <Document ID="80">
            <Title>Addition</Title>
            <Text> I’m not sure how to write the story of the whole addition, so we’ll start with a simpler
example—$5 + $5 = $10.
public void testSimpleAddition() {
Money sum= Money.dollar(5).plus(Money.dollar(5));
assertEquals(Money.dollar(10), sum);
}
 We could fake the implementation by just returning “Money.dollar(10)”, but the
implementation seems obvious. We’ll try:
Money
Money plus(Money addend) {
return new Money(amount + addend.amount, currency);
}

 Where the design isn’t obvious I will still fake the implementation
and refactor. I hope you will see through this how TDD gives you control over the
size of steps.

 Having said that I was going to go much faster, I will immediately go much slower,
not in getting the tests working, but in writing the test itself. There are times and
tests that call for careful thought. How are we going to represent multi-currency
arithmetic? This is one of those times for careful thought.

The most difficult design constraint is that we would like most of the code in the
system to be unaware that it is (potentially) dealing with multiple currencies. One
possible strategy is to immediately convert all money values into a reference currency. 
However, this doesn’t allow exchange rates to vary easily.
Instead we would like a solution that lets us conveniently represent multiple
exchange rates, and still allows most arithmetic-like expressions to look like, well,
arithmetic.

Objects to the rescue. When the object you have doesn’t behave like you want,
make another object with the same external protocol (an Imposter), but a different
implementation.

This probably sounds a bit like magic. How do you know to think of creating an
imposter here? There is no formula for flashes of design insight. 
TDD can’t guarantee that you will have flashes of insight at the right moment. However, confidence-giving tests and carefully factored code give you preparation for insight, and preparation for applying
that insight when it comes.

The solution is to create an object that acts like a Money, but represents the sum of
two Moneys. I’ve tried several different metaphors to explain this idea. One is to
treat the sum like a Wallet—you can have several different notes of different
denominations and currencies in the same wallet.

Another metaphor is “expressions”, as in “(2 + 3) * 5”, or in our case “($2 + 3
CHF) * 5”. A Money is the atomic form of an expression. Operations result in
Expressions, one of which will be a Sum. Once the operation (like adding up the
 value of a portfolio) is complete, the resulting Expression can be reduced back a
single currency given a set of exchange rates.

Applying this metaphor to our test, we know what we end up with:
public void testSimpleAddition() {
…
assertEquals(Money.dollar(10), reduced);
}
 The reduced Expression is created by applying exchange rates to an Expression.
What in the real world applies exchange rates? A bank.We would like to be able to
write:
public void testSimpleAddition() {
…
Money reduced= bank.reduce(sum, "USD");
assertEquals(Money.dollar(10), reduced);
}
 (It’s a little weird to be mixing the “bank” metaphor and the “expression” metaphor.
We’ll get the whole story told first, and then we’ll see what we can do about
literary value.)
We have made an important design decision here. We could just as easily have
written “…reduce= sum.reduce(“USD”, bank)”. Why make the Bank responsible?
One answer is “that’s the first thing that popped into my head,” but that’s not very
informative.Why did it pop into my head that reduction should be the responsibility
of the bank and not the expression? Here’s what I’m aware of at the moment:
•  Expressions seem to be at the heart of what we are doing. I try to keep the
objects at the heart as ignorant of the rest of the world as possible, so they stay
flexible as long as possible (and remain easy to test, and reuse, and understand.)
•  I can imagine there will be many operations involving Expressions. If we add
every operation to Expression, Expression will grow without limit.
That doesn’t seem like enough reasons to tip the scales permanently, but it is
enough for me to start in this direction. I’m also perfectly willing to move responsibility
for reduction to Expression if it turns out Bank’s don’t need to be involved.
The Bank in our simple example doesn’t really need to do anything. As long as we
have an object we’re okay:

public void  testSimpleAddition() {
…
Bank bank= new  Bank();
Money reduced= bank.reduce(sum, "USD" );
assertEquals(Money.dollar(10), reduced);
}
The sum of two Moneys should be an Expression:
public void  testSimpleAddition() {
…
Expression sum= five.plus(five);
Bank bank= new  Bank();
Money reduced= bank.reduce(sum, "USD" );
assertEquals(Money.dollar(10), reduced);
}
At least we know for sure how to get five dollars:
public void  testSimpleAddition() {
Money five= Money.dollar(5);
Expression sum= five.plus(five);
Bank bank= new  Bank();
Money reduced= bank.reduce(sum, "USD" );
assertEquals(Money.dollar(10), reduced);
}
How do we get this to compile? We need an interface Expression (we could have a
class, but an interface is even lighter weight):
Expression
interface  Expression
Money.plus() needs to return an Expression:
Money
 Expression plus(Money addend) {
return new  Money(amount + addend.amount, currency);
}
Which means that Money has to implement Expression (which is easy, since there
are no operations yet):

Money
class Money implements Expression
We need an empty Bank class:
Bank
class Bank
Which stubs out reduce():
Bank
Money reduce(Expression source, String to) {
return null;
}
Now it compiles, and fails miserably. Hooray! Progress! We can easily fake the
implementation, though:
Bank
Money reduce(Expression source, String to) {
return Money.dollar(10);
}
We’re back to a green bar, and ready to refactor. First, reviewing, we:

• Reduced a big test to a smaller test that represented progress ($5 + 10 CHF to $5
+ $5)
• Thought carefully about the possible metaphors for our computation
• Re-wrote our previous test based on our new metaphor
• Got the test to compile quickly
• Made it run
• Looked forward with a bit of trepidation to the refactoring necessary to make the implementation real</Text>
        </Document>
        <Document ID="73">
            <Title>Compare Francs to Dollars</Title>
            <Text>Compare Francs to Dollars

The thought struck us at the end of the last chapter - what happens when we compare Francs and Dollars? We dutifully turned our dreadful thought into an item on our to-do list. Let’s see what happens:

Test_equality
AssertTrue(Dollar.new(5).equals(Dollar.new(5))
AssertFalse(Dollar.new(5).equals(Dollar.new(6))
AssertTrue(Franc.new(5).equals(Franc.new(5))
AssertFalse(Franc.new(5).equals(Franc.new(6))
AssertFalse(Franc.new(5).equals(Dollar.new(5))

It fails. Dollars are Francs. The equality code needs to check that it isn’t comparing Dollars and Francs. We can do this right now by comparing the class of the two objects - two Moneys are equal only if their amounts and classes are equal.

Equals(object)
Return amount == money.amount &amp;&amp; object.class.equals(self.class)
End

Using classes like this in model code is a bit smelly. We would like to use a criteria that made sense in the domain of finance not the programming language domain. However, we don’t currently have anything like a currency, and this doesn’t seem like sufficient reason to introduce one, so this will have to do for the moment.

Now we really need to get rid of common times code, so we can get to mixed currency arithmetic. Before we do, let’s review, we:
Took an objection that was bothering us and turned it into a test
Made the test run a reasonable, but not perfect way
Decided not to introduce more design until we had a better motivation

Updated To-Do List

Currency?</Text>
        </Document>
        <Document ID="116">
            <Title>TDD with Objects</Title>
            <Text>A Web of Objects

OO design focuses more on the communication between objects than on the objects themselves. The key in making great systems is much more to design how its modules communicate rather than what their internal properties and behaviors should be.

An OO system is a web of collaborating objects. A system is built by creating objects and plugging them together so that they can send messages to one another. The behavior of the system is an emergent property of the composition of the objects - the choice of objects and how they are connected. 

This lets us change the behavior of the system by changing the composition of its objects - adding and removing instances, plugging different combinations together rather than writing procedural code. The code we write to manage this composition is a declarative definition of the how the web of objects will behave. It’s easier to change the system’s behavior because we can focus on what we want it to do, not how.

Values and Objects

Values model unchanging quantities and measurements. They are immutable instances that model fixed quantities. They have no individual identity, so two value instances are effectively the same if they have the same state.

Objects have an identity, might change state over time and model computational processes. They use mutable state to model their behavior over time. Two objects of the same type have separate identities even if they have exactly the same state now, because their states can diverge if they receive different messages in the future.

Values are treated functionally and objects implement the stateful behavior of the system.

Follow the Messages

We can benefit from the high-level, declarative approach only if our objects are designed to be easily pluggable. In practice, this means that they follow common communication patterns and that the dependencies between them are made explicit. A communication pattern is a set of rules that govern how a group of objects talk to each other: the roles they play, what messages they can send and when, and so on. 

The domain model is in these communication patterns because they give meaning to the universe of possible relationships between the objects. 

An object is an implementation of one or more roles; a role is a set of related responsibilities; and a responsibility is an obligation to perform a task or know information. A collaboration is an interaction of objects or roles (or both). 

Tell, Don’t Ask

The calling object should describe what it want in terms of the role that its neighbor plays, and let the called object decide how to make that happen. Objects make their decisions based only on the information they hold internally or that which case with the triggering message; they avoid navigating to other objects to make things happen. This style produces more flexible code because it’s easy to swap objects that play the same role. The caller sees nothing of their internal structure or the structure of the rest of the system behind the role interface. This wraps all the implementation detail up behind a single call. This reduces the risk that a design change might cause ripples in remote parts of the codebase.

Unit-Testing the Collaborating Objects

How can we test when we don’t expose any internal state of an object?

Fig 2.4 Unit-Testing an object in isolation 

We can replace the target object’s neighbors in a test with mock objects. We can specify how we expect the target object to communicate with its mock neighbors for a triggering event; we call these specifications expectations. During the test, the mock objects assert that they have been called as expected; they also implement any stubbed behavior needed to make the rest of the test work.

Fig 2.5 Testing an object with mock objects

This implies that we’re just testing the target object and that we already know what its neighbors look like. In practice, those collaborators don’t need to exist when we’re writing a unit test. We can use the test to help us tease out the supporting roles our object needs and fill in real implementations as we develop the rest of the system. We call this interface discovery.

Support for TDD with Mock Objects

To support this style of TDD we need to create mock instances of the neighboring objects, define expectations on how they’re called and then check them, and implement any stub behavior we need to get through the test. In practice, the runtime structure of a test with mock objects looks like fig 2.6

Fig 2.6 Testing an object with mock objects

We use the term mockery for the object that holds the context of a test, creates mock objects, and manages expectations and stubbing for the test. The essential structure of a test is:

Create any required mock objects
Create any real objects, including the target object
Specify how you expect the mock objects to be called by the target object
Call the triggering methods on the target object
Assert that any resulting values are valid and that all the expected calls have been made

The unit test makes explicit the relationship between the target object and its environment. It creates all the objects in the cluster and makes assertions about the interactions between the target object and its collaborators. The important point is to make clear the intention of every test, distinguishing between the tested functionality, the supporting infrastructure and the object structure.

Test Fixtures

A test fixture is the fixed state that exists at the start of a test. A test fixture ensures that a test is repeatable - every time a test is run it starts in the same state so it should produce the same results. A fixture may be set up before the test runs and torn down after it has finished. 

Expectations

An expectation block is designed to stand out from the test code that surrounds it, making an obvious separation between the code that describes how neighboring objects should be invoked and the code that actually invokes objects and test the results. The code within an expectation block acts as a little declarative language that describes the expectations.

 

</Text>
        </Document>
        <Document ID="123">
            <Title>Passing the First Test</Title>
            <Text>The Necessary Minimum

Iteration zero requires focus to get the walking skeleton working. The point is to design and validate the initial structure of the end-to-end system - where end-to-end includes deployment to a working environment - to prove that our choices of packages, libraries, and tooling will actually work. A sense of urgency will help the team to strip the functionality down to the absolute minimum sufficient to test their assumptions.

Iteration zero usually brings up project chartering issues as the team looks for criteria to guide its decisions, so the project’s sponsors should expect to field some deep questions about its purpose.

We have something visible we can present as a sign of progress, so we can cross off the first item on our list. Single item join, lose without bidding. The next step is to start building out real functionality.</Text>
        </Document>
        <Document ID="130">
            <Title>Test Flexibility</Title>
            <Text>Test Flexibility

We can reduce the ongoing cost of tests by making them easy to read and generating helpful diagnostics on failure. We also want to make sure that each test fails only when its relevant code is broken. Otherwise, we end up with brittle tests that slow down development and inhibit refactoring. Common causes of test brittleness include:

The tests are too tightly coupled to unrelated parts of the system or unrelated behavior of the objects they are testing.
The tests over specify the expected behavior of the target code, constraining it more than necessary.
There is duplication when multiple tests exercise the same production code behavior.

Test brittleness is not just an attribute of how the tests are written; it’s also related to the design of the system. If an object is difficult to decouple from its environment because it has many dependencies or its dependencies are hidden, its tests will fail when distant parts of the system change. It will be hard to judge the knock-on effects of altering the code. So, we can use test brittleness as a valuable source of feedback about design quality.

There’s a virtuous relationship with test readability and resilience. A test that is focused, has clean set-up, and has minimal duplication is easier to name and is more obvious about its purpose.

Specify Precisely What Should Happen and No More

Specify just what we want from the target code. The more precise we are, the more the code can flex in other unrelated dimensions without breaking tests misleadingly. The other benefit of keeping tests flexible is that they’re easier for us to understand because they are clearer about what they’re testing - about what is and is not important in the tested code.

Test for Information Not Representation

A test might need to pass a value to trigger the behavior it’s supposed to exercise in its target object. The value could either be passed in as a parameter to a method on the object, or returned as a result from a query the object makes on one of its neighbors stubbed by the test. If the test is structured in terms of how the value is represented by other parts of the system, then it has a dependency on those parts and will break when they change.

For example, we have a system that uses a CustomerBase to store and find information about our customers. One of its features is to look up a customer given an email address; it returns null if there’s no customer with the given address. When we test the parts of the code that search for customers by email address, we stub CustomerBase as a collaborating object. In some of those tests, no customer will be found so we return null. There are two problems with this use of null in a test. First, we have to remember what null means here, and when its appropriate; the test is not self-explanatory. The second concern is the cost of maintenance.

Some time later, we experience a null pointer exception in production and track the source of the null reference down to the CustomerBase. We realize we’ve broken one of our design rules: Never Pass Null Between Objects. 

If, instead, we’d given the tests their own representation of “no customer found” as a single well-named constant instead of the literal null, we could have avoided this drudgery. We would have changed on line: NO_CUSTOMER_FOUND = null  to NO_CUSTOMER_FOUND = Maybe.nothing() without changing the tests themselves.

Tests should be written in terms of the information passed between objects, not of how that information is represented. Doing so will both make the tests more self-explanatory and shield them from changes in implementation controller elsewhere in the system. Significant values, like NO_CUSTOMER_FOUND, should be defined in one place as a constant. For more complex structures, we can hide the details of the representation in test data builders.

Precise Assertions

In a test, focus the assertions on just what’s relevant to the scenario being tested. Avoid asserting values that aren’t driven by the test inputs, and avoid reasserting behavior that is covered in other tests.

These heuristics guide us towards writing tests where each method exercises a unique aspect of the target code’s behavior. This makes the tests more robust because they’re not dependent on unrelated results, and there’ less duplication.

Most test assertions are simple checks for equality. Testing for equality doesn’t scale well as the value being returned becomes more complex. Different test scenarios may make the tested code return results that differ only in specific attributes, so comparing the entire result each time is misleading and introduces an implicit dependency on the behavior of the whole tested object.

There are a couple of ways in which a result can be more complex. First, it can be defined as a structured value type. This is straightforward since we can just reference directly any attributes we want to assert. For example, if we take the financial instrument from Use Structure to Explain, we might need to assert only its strike price, without comparing the whole instrument.

This tells the programmer that the only thing we really care about it that the new identifier is larger than the previous one - its actual value is not important in this test. The assertion also generates a helpful message when it fails.

The second source of complexity is implicit, but very common. We often have to make assertions about a text string. Sometimes we know exactly what the text should be, for example when we have the FakeAuctionServer look for specific messages in Extending the Fake Auction. Sometimes, however, all we need to check is that certain values are included in the text.

A frequent example is when generating a failure message. We don’t want all our unit tests to be locked to its current formatting, so that they fail when we add whitespace, and we don’t want to have to do anything clever to cope with timestamps. We just want to know that the critical information is included, so we write: message.contains(“strike_price = 98”) which asserts that all these strings occur somewhere in failure_message. That’s enough reassurance for us, and we can write other tests to check that a message is formatted correctly if we think it’s significant.

One interesting effect of trying to write precise assertions against text strings is that the effort often suggests that we’re missing an intermediate structure object - in this case perhaps an InstrumentFailure. Most of the code would be written in terms of an InstrumentFailure, a structured value that carries all the relevant fields. The failure would be converted to a string only at the last possible moment, and that string conversion can be tested in isolation.

Precise Expectations

Each mock object test should specify just the relevant details of the interactions between the object under test and its neighbors. The combined unit test for an object describe its protocol for communicating with the rest of the system.

Specify the communication between objects as precisely as it should be. The API of isolation framework is designed to produce tests that clearly express how objects relate to each other and that are flexible because they’re not too restrictive. This may require a little more test code than some of the alternatives, but we find that the extra rigor keeps the tests clear.

Precise Parameter Matching

We want to be as precise about the values passed in to a method as we are about the value it returns. For example, here is an expectation where one of the accepted arguments is any type of RuntimeException; the specific class doesn’t matter. Similarly another example where the method sniper_for_item returns a Matcher that checks only the item identifier when given an AuctionSniper. This test doesn’t care about anything else in the sniper’s state, such as its current bid or last price, so we don’t make it more brittle by checking those values.

The same precision can be applied to expecting input strings. If, for example, we have audit_trail object to accept failure message, we can write a precise expectation for that auditing: failure_message.contains(“id=14”).

Allowances and Expectations

Expectations must be met during a test, but allowances may be matched or not. The point of the distinction is to highlight what matters in a particular test. Expectations describe the interactions that are essential to the protocol we’re testing: if we send this message to the object, we expect to see it send this other message to this neighbor.

Allowances support the interaction we’re testing. We often use them as stubs to feed values into the object, to get the object into the right state for the behavior we want to test. We also use them to ignore other interactions that aren’t relevant to the current test. 

For example, the ignoring clause says that, in this test, we don’t care about messages sent to the auction; they will be covered in other tests. The allowing clause matches any call to sniper_state_changed with a Sniper that is currently bidding, but doesn’t insist that such a call happens. In this test, we use the allowance to record what the Sniper has told us about its state.

In other tests we attach action clauses to allowances, so that the call will return a value or throw an exception. For example, we might have an allowance that stubs the catalog to return a price that will be returned for use later in the test.

The distinction between allowances and expectations isn’t rigid, but this simple rule helps:

Allow Queries; Expect Commands

Commands are calls that are likely to have side effects, to change the world outside the target object. When we tell the audit_trail above to record a failure, we expect that to change the contents of some kind of log. The state of the system will be different if we call the method a different number of times.

Queries don’t change the world, so they can be called any number of times, including none. In our example above, it doesn’t make any difference to the system how many times we ask the catalog for a price.

The rule helps to decouple the test from the tested object. If the implementation changes, for example to introduce caching or use a different algorithm, the test is still valid. On the other hand, if we were writing a test for a cache, we would want to know exactly how often the query was made.

The number of times a call is expected is defined by the cardinality clause that starts the expectation.

Ignoring Irrelevant Objects

We can simplify a test by ignoring collaborators that are not relevant to the functionality being exercised. Isolation framework will not check any calls to ignored objects. This keeps the test simple and focused, so we can immediately see what’s important and changes to one aspect of the code do not break unrelated tests. With one ignore clause, the test can focus on the code’s domain behavior by disabling everything to do with irrelevant details.

Like all power tools, ignoring should be used with care. A chain of ignore objects might suggest that the functionality ought to be pulled out into a new collaborator. As programmers, we must also make sure that ignored features are tested somewhere, and there are higher-level tests to make sure everything works together. In practice, we usually introduce ignoring only when writing specialized tests after the basics are in place.

Invocation Order

Isolation framework allows invocations on a mock object to be called in any order; the expectations don’t have to be declared in the same sequence. The less we say in the tests about the order of interactions, the more flexibility we have with the implementation of the code. We also gain flexibility in how we structure the tests; for example, we can make test methods more readable by packaging up expectations in helper methods.

Only Enforce Invocation Order When it Matters

Sometimes the order in which calls are made is significant, in which case we add explicit constraints to the test. Keeping such constraints to a minimum avoids locking down the production code. It also helps us see whether each case is necessary - ordered constraints are so uncommon that each use stands out.

Isolation frameworks have two mechanisms for constraining invocation order: sequences, which define an ordered list of invocations, and state machines,  which can describe more sophisticated ordering constraints. Sequences are simpler to understand than state machines, but their restrictiveness can make tests brittle if used inappropriately.

Sequences are most useful for confirming that an object sends notifications to its neighbors in the right order. States and sequences can be used in combination. We rarely need such complexity - it’s most common when responding to external feeds of events where we don’t own the protocol - and we always take it as a hint that something should be broken up into smaller, simpler pieces.

The Power of Mock States

We can use it to model each of the three types of participants in a test: the object being tested, its peers and the test itself.

A states describes what the test finds relevant about the object, not its internal structure. We don’t want to constrain the object’s implementation.

We can represent the state of the test itself. For example, we could enforce that some interactions are ignored while the test is being setup.

Guinea Pig Objects



</Text>
        </Document>
        <Document ID="81">
            <Title>Removing Duplication</Title>
            <Text> We can’t mark our test for $5 + $5 done until we’ve removed all the duplication.
We don’t have code duplication, but we do have data duplication. The $10 in the
fake implementation:

Bank
Money reduce(Expression source, String to) {
return Money.dollar(10);
}

 is really the same as the “$5 + $5” in the test:
public void testSimpleAddition() {
Money five= Money.dollar(5);
Expression sum= five.plus(five);
Bank bank= new Bank();
Money reduced= bank.reduce(sum, "USD");
assertEquals(Money.dollar(10), reduced);
}

 Before when we’ve had a fake implementation, it’s been obvious how to work
backwards to the real implementation. It’s just been a matter of replacing constants
with variables. This time, though, it’s not obvious to me how to work backwards.
So, even though it feels a little speculative, we’ll work forwards.

 First, Money.plus() needs to return a real Expression, a Sum, not just a Money (perhaps
later we’ll optimize the special case of adding two identical currencies, but
that’s later.)

The sum of two Moneys should be a Sum:
public void testPlusReturnsSum() {
Money five= Money.dollar(5);
Expression result= five.plus(five);
Sum sum= (Sum) result;
assertEquals(five, sum.augend);
assertEquals(five, sum.addend);
}

( first argument to addition is called the “augend”)

 The test above is not one I would expect to live a long time. It is deeply concerned
with the implementation of our operation, not its externally visible behavior. However,
if we make it work, we expect we’ve moved one step closer to our goal.
To get it to compile, all we need is a Sum class with two fields, augend and addend:

Sum
class Sum {
Money augend;
Money addend;
}

 This gives us a ClassCastException, because Money.plus() is returning a Money,
not a Sum:

Money
Expression plus(Money addend) {
return new Sum(this, addend);
}

 Sum needs a constructor:
Sum
Sum(Money augend, Money addend) {
}

 And Sum needs to be a kind of Expression:

Sum
class Sum implements Expression
 Now the system compiles again, but the test is still failing, this time because the
Sum constructor is not setting the fields (we could fake the implementation by initializing
the fields, but I said I’d start going faster):

Sum
Sum(Money augend, Money addend) {
this.augend= augend;
this.addend= addend;
}

 Now Bank.reduce() is being passed a Sum. If the currencies in the Sum are all the
same, and the target currency is also the same, the result should be a Money whose
amount is the sum of the amounts:
public void testReduceSum() {
Expression sum= new Sum(Money.dollar(3), Money.dollar(4));
Bank bank= new Bank();
Money result= bank.reduce(sum, "USD");
assertEquals(Money.dollar(7), result);
}
 I carefully chose parameters that would break the existing test. When we reduce a
Sum, the result (under these simplified circumstances) should be a Money whose
amount is the sum of the amounts of the two Moneys and whose currency is the
currency to which we are reducing.
Bank
Money reduce(Expression source, String to) {
Sum sum= (Sum) source;
int amount= sum.augend.amount + sum.addend.amount;
return new Money(amount, to);
}

 This is immediately ugly on two counts:

•  The cast. This code should work with any Expression.
•  The public fields, and two levels of references at that

 Easy enough to fix. First, we can move the body of the method to Sum and get rid
of some of the visible fields.We are “sure” we will need the Bank as a parameter in
the future, but this is pure, simple refactoring, so we leave it out.

Bank
Money reduce(Expression source, String to) {
Sum sum= (Sum) source;
return sum.reduce(to);
}

Sum
public Money reduce(String to) {
int amount= augend.amount + addend.amount;
return new Money(amount, to);
}

 (Which brings up the point of how we are going to implement, er… test,
Bank.reduce() when the argument is a Money.)
Let’s write that test, since the bar is green and there is nothing else obvious to do
with the code above:

public void testReduceMoney() {
Bank bank= new Bank();
Money result= bank.reduce(Money.dollar(1), "USD");
assertEquals(Money.dollar(1), result);
}

Bank
Money reduce(Expression source, String to) {
if (source instanceof Money) return (Money) source;
Sum sum= (Sum) source;
return sum.reduce(to);
}

 Ugly, ugly, ugly. However, we now have a green bar, and refactoring is possible.
Any time you are checking classes explicitly, you should be using polymorphism
instead. Since Sum implements reduce(String), if Money implemented it, too, we
could then add it to the Expression interface.

Bank
Money reduce(Expression source, String to) {

if  (source instanceof  Money)
return  (Money) source.reduce(to);
Sum sum= (Sum) source;
return  sum.reduce(to);
}

Money
public  Money reduce(String to) {
return this ;
}

If we add reduce(String) to the Expression interface:
Expression
 Money reduce(String to);
We can eliminate all those ugly casts and class checks:

Bank
 Money reduce(Expression source, String to) {
return  source.reduce(to);
}

I’m not entirely happy with the name of the method being the same in Expression
and in Bank, but having different parameter types. I’ve never found a satisfactory
general solution to this problem in Java. In languages with keyword parameters,
communicating the difference between Bank.reduce(Expression, String) and
Expression.reduce(String) is well supported by the language syntax. With positional
parameters, it’s not so easy to make the code speak for you about how the
two are different.

Next we’ll actually exchange one currency for another. First, reviewing, we:

• Didn’t mark a test as done because the duplication had not been eliminated
• Worked forwards instead of backwards to realize the implementation
• Wrote a test to force the creation of an object we expected to need later (Sum)
• Started implementing faster (the Sum constructor)
• Implemented code with casts in one place, then moved the code where it belonged once the test were running
• Introduced polymorphism to eliminate explicit class checking

</Text>
        </Document>
        <Document ID="74">
            <Title>Making Objects</Title>
            <Text>The two implementations of times are remarkably similar:

Def times(multiplier)
Return Franc.new(amount * multiplier)
End

Def times(multiplier)
Return Dollar.new(amount * multiplier)
End

The two subclasses of Money aren’t doing enough work to justify their existence, so we can eliminate them. We will not do it with on big step because we want an effective demonstration of TDD.

Ok, we would be one step closer to eliminating the subclasses if there were fewer references to the subclasses directly. We can introduce a Factory Method in Money that returns a Dollar: We would use it like this:

Test_multiplication
Five = Money.dollar(5)
Assert(Dollar.new(10), five.times(2))
Assert(Dollar.new(15), five.times(3))

The implementation creates and returns a Dollar:

Class Money
Dollar(amount)
Return Dollar.new(amount)
End

Our compiler complains that times is not defined for Money. Let’s define it in Money. The tests all run. We can now use our factory method everywhere in the tests:

Test_multiplication
Five = Money.dollar(5)
Assert(Money.dollar(10), five.times(2))
Assert(Money.dollar(15), five.times(3))

Test_equality
Assert(Money.dollar(5).equals(Money.dollar(5))
AssertFalse(Money.dollar(5).equals(Money.dollar(6))
Assert(Franc.new(5).equals(Franc.new(5))
AssertFalse(Franc.new(5).equals(Franc.new(6))
AssertFalse(Franc.new(5).equals(Money.dollar(5))

We are now in a slightly better position than before. No client code knows that there is a subclass called Dollar. By de-coupling the tests from the existence of the subclasses, we have given ourselves freedom to change inheritance without affecting client code.

Before we go blindly changing the test_franc_multiplication, we notice that it isn’t testing any logic that isn’t tested by the test for Dollar multiplication. If we delete the test, will we lose any confidence in the code? Still a little, so we leave it there. But it’s suspicious>

public void  testEquality() {
assertTrue(Money.dollar(5).equals(Money.dollar(5)));
assertFalse(Money.dollar(5).equals(Money.dollar(6)));
assertTrue(Money.franc(5).equals(Money.franc(5)));
assertFalse(Money.franc(5).equals(Money.franc(6)));
assertFalse(Money.franc(5).equals(Money.dollar(5)));
}
public void  testFrancMultiplication() {
Money five = Money.franc(5);
assertEquals(Money.franc(10), five.times(2));

 assertEquals(Money.franc(15), five.times(3));
}
The implementation is just like Money.dollar():
Money
static  Money franc(int  amount) {
return new  Franc(amount);
}

We’ll get rid of the duplication of times next. For now, reviewing, we:
Moved at least a declaration of the method to the common superclass
Decoupled test code from the existence of concrete subclasses by introducing factory methods
Noticed that when the subclasses disappear some tests will be redundant, but took no action

$5 + 10 CHF = $10 if rate is 2:1
$5 * 2 = $10
Make “amount” private
Dollar side-effects?
Money rounding?
equals()
hashCode()
Equal null
Equal object
5 CHF * 2 = 10 CHF
Dollar/Franc duplication
Common equals
Common times
Compare Francs to Dollars
Currency?
Delete testFrancMultiplication?

</Text>
        </Document>
        <Document ID="3">
            <Title>Rhythm of TDD</Title>
            <Text>My goal is for you to see the rhythm of test-driven development:

1.  Quickly add a test
2.  Run all tests and see the new one fail
3.  Make a little change
4.  Run all tests and see them all succeed
5.  Refactor to remove duplication

The surprises are likely to be:

1.  How each test can cover a small increment of functionality
2.  How small and ugly the changes can be to make the new tests run
3.  How often the tests are run
4.  How many teensy tiny steps make up the refactorings

</Text>
        </Document>
        <Document ID="109">
            <Title>Using Test Doubles</Title>
            <Text>For the most part we assumed that the SUT was designed such that it could be tested easily in isolation of other pieces of software. When a class does not depend on any other classes, testing it is relatively straightforward. When a class does depend on other classes, we have two choices: we can test it together with all the other classes it depends on or we can try to isolate it from the other classes so that we can test it by itself. We will now see techniques for isolating the SUT from the other software components on which it depends.

What Are Indirect Inputs and Outputs?

The problem with testing classes in groups or clusters is that it becomes very hard to cover all the paths through the code. The DoC may return values or throw exceptions that affect the behavior of the SUT, but it may prove difficult or impossible to cause certain cases to occur. The indirect inputs received from the DoC may be unpredictable (such as system clock or calendar). In other cases, the DoC may not be available in the test environment or may not even exist. How can we test dependent classes in these circumstances?

In other cases, we need to verify that certain side effects of executing the SUT have, indeed, occurred. If it is too difficult to monitor these indirect outputs of the SUT (or if it is too expensive to retrieve them), the effectiveness of our automated testing may be compromised.

The solution to these problems is often the use of a test double. We will start by looking how we can use test doubles to test indirect inputs and outputs. 

Why Do We Care about Indirect Inputs?

Calls to DoCs often return objects or values, update their arguments or even throw exceptions. Many of the execution paths within the SUT are intended to deal with these return values and to handle the possible exceptions. Leaving these paths untested leads to untested code. These paths can be the most challenging to test effectively but are also among the most likely to lead to catastrophic failures if exercised for the very first time in production.

It is desirable to have automated tests for exception-handling code. The testing challenge is to somehow cause the DoC to throw an exception so that the error path can be tested. The exception we expect the DoC to throw is a good example of an indirect input test condition. Our means of injecting this input is a control point. 

Fig 11.1 an indirect input being received by the SUT from a DoC. Not all inputs of the SUT come from the test. Some indirect inputs come from other components called by the SUT in the form of return values, updated parameters, or exceptions thrown. 

Why Do We Care about Indirect Outputs?

The concept of encapsulation often directs us to not care about how something is implemented. When testing, we try to verify the implementation precisely so our clients do not have to care about it.

If a method does not return anything or at least nothing that can be used to determine whether it has performed its function correctly. In this situation, we have no choice but to test through the back door. A good example of this is a message logging system. Calls to the API of a logger rarely return anything that indicates it did its job correctly. The only way to determine whether the message logging system is working as expected is to interact with it through some other interface - one that allows us to retrieve the logged messages.

A client of the logger may specify that the logger called when certain conditions are met. These calls will not be visible on the client’s interface but would typically be a requirement that the client needs to satisfy and therefore would be something we want to test. The circumstances that should result in a messaging being logged are indirect output test conditions for which we need to write tests so that we can avoid having untested requirements. Our means of seeing this output is an observation point.

Fig 11.2 An indirect output being received by the SUT. Not all outputs of the SUT are directly visible to the test. Some indirect outputs are sent to other components in the form of method calls or messages.

In other cases, the SUT does produce visible behavior that can be verified through the front door but also has some expected side effects. Both outputs need to be verified in our tests. Sometimes this testing is simply a matter of adding assertions for the indirect outputs to the existing tests to verify the untested requirement.

How Do We Control Indirect Inputs?

Testing with indirect inputs is a bit simpler than testing with indirect outputs because the techniques used to test outputs build on those used to test inputs. Let’s look at indirect inputs first.

To test the SUT with indirect inputs, we must be able to control the DoC to cause it to return every possible kind of return value. That implies the availability of a suitable control point. 

Examples of the kinds of indirect inputs we want to be able to induce via this control point include:

Return values of methods / functions
Values of updatable arguments
Exceptions that could be thrown

Often, the test can interact with the DoC to set up how it will respond to requests. For example, if a component provides access to data in a database, then we can use back door setup to insert specific values into a database that cause the component to respond in the desired ways (no items found, one item found, many items found). In this specific case, we can use the database itself as a control point.

Fig 11.3 Using back door manipulation to indirectly control and observe the SUT. When the SUT stores its state in another component, we may be able to manipulate that state by having the test interact directly with the other component via a back door.

In most cases this approach is not practical for the following reasons:

The real component cannot be manipulated to produce the desired indirect input. Only a true software error within the real component would result in the desired input to the SUT.
The real component could be manipulated to make the input occur but doing so would not be cost-effective.
The real component could be manipulated to make the input occur but doing so could have unacceptable side effects.
The real component is not yet available for use.

If we cannot use the real component as a control point, then we have to replace it with one that we can control. This replacement can be done in a number of different ways. The most common approach is to configure a test stub with a set of values to return from its functions and then to install this test stub into the SUT. During execution of the SUT, the test stub receives the calls and returns the previously configured responses. It has become our control point.

Fig 11.4 Using a test stub as a control point for indirect inputs. One way to use a control point to inject indirect inputs into the SUT is to install a test stub in place of the DoC. Before exercising the SUT, we tell the test stub what it should return to the SUT when it is called. This strategy allows us to force the SUT through all its code paths.

How Do we Verify Indirect Outputs?

In normal usage, as the SUT is exercised, it interacts naturally with the components upon which it depends. To test the indirect outputs, we must be able to observe the calls that the SUT makes to the API of the DoC. Furthermore, if we need the test to progress beyond that point, we need to be able to control the values returned.

Fig 11.5 Using behavior verification to verify the indirect outputs of the SUT. When we care about exactly what calls our SUT makes to other components, we may have to do behavior verification rather than simply verifying the post-test state of the SUT.

In many cases, the test can use the DoC as an observation point to find out how it has been used. For example:

We can ask the file system for the contents of a file that the SUT has written to verify that it exists and was written with the expected contents.
We can ask the database for the contents of a table or specific record to verify that the SUT wrote the expected records to the database.
We can interact directly with the e-mail sending component to ask whether the SUT had asked it to send a particular email.

These are examples of back door verification. Some DoCs allow us to configure their behavior in such a way that we can use them to keep the test informed of how they are being used:

We can ask the file system to notify the test whenever a file is created or modified so we can verify its contents.
We can use a database trigger to notify the test when a record is written or deleted.
We can configure the email sending component to deliver all outgoing email to the test.

Sometimes it is not practical to use the real component as an observation point. We need to replace the real component with a test-specific alternative. For example:
The calls to (or the internal state of) the DoC cannot be queried.
The real component can be queried but doing so is cost-prohibitive
The real component can be queried but doing so has unacceptable side effects.
The real component is not yet available for use.

There are two basic styles of indirect output verification. Procedural behavior verification captures the calls to a DoC (or their results) during SUT execution and then compares them with the expected calls after the SUT has finished executing. This verification involves replacing a substitutable dependency with a test spy. During execution of the SUT, the test spy receives the calls and records them. After the test method has finished exercising the SUT, it retrieves the actual calls from the test spy and uses assertion methods to compare them with the expected calls.

Fig 11.6 Using a test spy as an observation point for indirect outputs of the SUT. One way to implement behavior verification is to install a test spy in place of the target of the indirect outputs. After exercising the SUT, the test asks the test spy for information about how it was used and compares that information to the expected behavior using assertions.

Expected behavior involves building a behavior specification during the fixture setup phase of the test and then comparing the actual behavior with this expected behavior. It is typically done by loading a mock object with a set of expected procedure call descriptions and installing this object into the SUT. During execution of the SUT, the mock object receives the calls and compares them to the previously defined expected calls (the behavior specification). As the test proceeds, if the mock object receives an unexpected call, it fails the test immediately. The test failure traceback will show the exact location in the SUT where the problem occurred because the assertions methods are called from the mock object, which is in turn called by the SUT. We can also see exactly where in the test method the SUT was being exercised.

Fig 11.7 Using a mock object as an observation point for indirect outputs of the SUT. Another way to implement behavior verification is to install a mock object in place of the target of the indirect outputs. As the SUT makes calls to the DoC, the mock objects uses assertions to compare the actual calls and arguments with the expected calls and arguments.

When we use a test spy or a mock object, we may also have to employ it as a control point for any indirect inputs on which the SUT depends after the test spy or mock object has been called to allow test execution to continue.

Testing with Doubles

How to replace inflexible and uncooperative real components with something that makes it easier to control the indirect inputs and to verify the indirect outputs?

To test the indirect inputs, we must be able to control the DoC to cause it to return every possible kind of return value (valid, invalid and exception). To test indirect outputs, we must be able to track the calls the SUT makes to other components. A test double object is used for this purpose.

Types of Test Doubles

A test double is any object that we install in place of the real component for the purpose of running a test. Depending on the reason whey we are using it, a test double can behave in one of four ways:

A dummy object is a placeholder object that is passed to the SUT as an argument (or an attribute of an argument) but is never actually used.
A test stub is an object that replaces a real component on which the SUT depends so that the test can control the indirect inputs of the SUT. It allows the test to force the SUT down paths it might not otherwise exercise. A test spy is a more capable version of a test stub used to verify the indirect outputs of the SUT by giving the test a way to inspect them after exercising the SUT.
A mock object is an object that replaces a real component on which the SUT depends so that the test can verify its indirect outputs.
A fake object is an object that replaces the functionality of the real DoC with an alternative implementation of the same functionality.

Fig 11.8 Several kinds of test doubles exist. Dummy objects are really an alternative to the value patterns. Test stubs are used to verify indirect inputs; test spies and mock objects are used to verify indirect outputs. Fake objects emulate the behavior of the real DoC but with test-friendly characteristics.

Dummy Objects

They are a degenerate form of test double. They exist solely to be passed around from method to method; they are never used. That is dummy objects are not expected to do anything except exist. Often we can get away with using nil or nothing; at other times, we may be forced to create a real object because the code expects something non-null. In dynamically types languages, almost any real object will do. For example we pass an instance of dummy customer to the invoice constructor to satisfy a mandatory argument. We do not expect the dummy customer to be used by the code we are testing.

Dummy object is not the same as a Null Object. A dummy object is not used by the SUT, so its behavior is irrelevant. By contrast, a Null Object is used by the SUT but is designed to do nothing. 

Test Stubs

A test stub is an object that acts as a control point to deliver indirect inputs to the SUT when the test stub’s methods are called. Its use allows us to exercise untested code paths in the SUT that might otherwise be impossible to traverse during testing. A Responder is a basic test stub that is used to inject valid and invalid indirect inputs into the SUT via normal returns from method calls. A Saboteur is a special test stub that raises exceptions or errors to inject abnormal indirect inputs into the SUT. 

Test Spies 

A test spy is an object that can act as an observation point for the indirect outputs of the SUT. To the capabilities of a test stub, it adds the ability to quietly record all calls made to its methods by the SUT. The verification part of the test performs procedural behavior verification on those calls by using a series of assertions to compare the actual calls received by the test spy with the expected calls.

Mock Objects

A mock object is also an object that can act as an observation point for the indirect outputs of the SUT. Like a test stub it may need to return information in response to method calls. Also like a test spy, it pays attention to how it was called by the SUT. It differs from a test spy, in that it compares actual calls received with the previously defined expectations using assertions and fails the test on behalf of the test method. As a consequence we can reuse the logic employed to verify the indirect outputs of the SUT across all tests that use the same mock object. 

Like test stubs, mock objects often support configuration with any indirect inputs required to allow the SUT to advance to the point where it would generate the indirect outputs they are verifying.

Fake Objects

A fake object is neither directly controlled nor observed by the test. It is used to replace the functionality of the real DoC in a test for reasons other than verification of indirect inputs and outputs. Typically a fake object implements the same functionality or a subset of the functionality of the real DoC in a much simpler way. The most common reasons for using them are that the real DoC has not yet been built, is too slow or is not available in the test environment.
Example: Encapsulate all database access behind a persistence layer interface and then replace the persistence layer component with one that used in-memory hash tables instead of a real database, thereby making tests run faster.

Dependency Injection 

Dependency injection is a class of design decoupling in which the client tells the SUT which DoC to use at runtime. This makes it possible to reuse the SUT more broadly because it removes knowledge of the dependency from the SUT; often the SUT will be aware of only a generic interface that the DoC must implement. 

Don’t fall into the new hammer trap. Overuse of test doubles (especially mock objects or test stubs_ can lead to over specified software by encoding implementation-specific information about the design in our tests. The design may be then much more difficult to change if many tests are affected by the change simply because they use a test double that has been affected by the design change.

</Text>
        </Document>
        <Document ID="82">
            <Title>Change</Title>
            <Text> Reduce Money with conversion

we are thinking about a much simpler form of
change—we have 2 francs and we want a dollar. That sounds like a test case
already:

public void testReduceMoneyDifferentCurrency() {
Bank bank= new Bank();
bank.addRate("CHF", "USD", 2);
Money result= bank.reduce(Money.franc(2), "USD");
assertEquals(Money.dollar(1), result);
}

 When I go from francs to dollars, I divide by two (we’re still studiously ignoring all
those nasty numerical problems.) We can make the bar green in one piece of ugliness:

Money
public Money reduce(String to) {
int rate = (currency.equals("CHF") &amp;&amp; to.equals("USD"))
? 2
: 1;
return new Money(amount / rate, to);
}

 Now, suddenly, Money knows about exchange rates. Yuck. The Bank should be the
only place we care about exchange rates. We’ll have to pass the Bank as a parameter
to Expression.reduce() (see, we knew  we would need it, and we were right. In
the words of the grandfather in The Princess Bride , “You’re very clever…”) First
the caller:

Bank
Money reduce(Expression source, String to) {
return source.reduce(this, to);
}

 Then the implementors:
Expression
Money reduce(Bank bank, String to);
Sum
public Money reduce(Bank bank, String to) {
int amount= augend.amount + addend.amount;
return new Money(amount, to);
}

Money
public Money reduce(Bank bank, String to) {
int rate = (currency.equals("CHF") &amp;&amp; to.equals("USD"))
? 2
: 1;
return new Money(amount / rate, to);
}

 The methods have to be public because methods in interfaces have to be public (for
some excellent reason, I’m sure.)
Now we can calculate the rate in the Bank:
Bank
int rate(String from, String to) {
return (from.equals("CHF") &amp;&amp; to.equals("USD"))
? 2
: 1;
}
 And ask the bank for the right rate:

Money
public Money reduce(Bank bank, String to) {
int rate = bank.rate(currency, to);
return new Money(amount / rate, to);
}

That pesky “2” still appears in both the test and the code. To get rid of it,we need to
keep a table of rates in the Bank and look up a rate when we need it. We could use
a Hashtable mapping pairs of currencies to rates. Can we use a two element array
containing the two currencies as the key? Does Array.equals() check to see if the
elements are equal?

public void testArrayEquals() {
assertEquals(new Object[] {"abc"}, new Object[] {"abc"});
}

Nope. The test fails, so we have to create a real object for the key:
Pair
private class Pair {
private String from;
private String to;
Pair(String from, String to) {
this.from= from;
this.to= to;
}
}

Because we are using Pairs as keys, we have to implement equals() and hash-
Code(). I’m not going to write tests for these, because we are writing this code in
the context of a refactoring. If we get to the payoff of the refactoring and all the
tests run, we expect the code to have been exercised. If I was programming with
someone who didn’t see exactly where we were going with this, or if the logic
became the least bit complex, I would begin writing separate tests.

Pair
public boolean equals(Object object) {
Pair pair= (Pair) object;
return from.equals(pair.from) &amp;&amp; to.equals(pair.to);
}

public int  hashCode() {
return  0;
}

“0” is a terrible hash value, but it has the advantage that it’s easy to implement and
it will get us running quickly. Currency lookup will look like linear search. Later,
when we get lots of currencies, we can do a more thorough job with real usage data.
We need somewhere to store the rates:

Bank
private  Hashtable rates= new  Hashtable();
We need to set the rate when told:
Bank
void  addRate(String from, String to, int  rate) {
rates.put(new  Pair(from, to), new  Integer(rate));
}

And then we can look up the rate when asked:
Bank
int  rate(String from, String to) {
Integer rate= (Integer) rates.get(new  Pair(from, to));
return  rate.intValue();
}

Wait a minute!?We got a red bar. What happened? A little snooping around tells us
that if we ask for the rate from USD to USD, we expect the value to be 1. Since this
was a surprise, let’s write a test to communicate what we discovered:

public void  testIdentityRate() {
assertEquals(1, new  Bank().rate("USD" , "USD" ));
}

Now we have three errors, but we expect them all to be fixed with one change:
Bank
int  rate(String from, String to) {
if  (from.equals(to)) return  1;

 Integer rate= (Integer) rates.get(new  Pair(from, to));
return  rate.intValue();
}

Green bar!
Next we’ll implement our last big test, $5 + 10 CHF. Several significant techniques
have slipped into this chapter:

• Added a parameter, in seconds, that we expected we would need
• Factored out the data duplication between code and tests
• Wrote a test (testArrayEquals) to check an assumption about the operation of
Java
• Introduced a private helper class without distinct tests of its own
• Made a mistake in a refactoring and chose to forge ahead, writing another test to
isolate the problem</Text>
        </Document>
        <Document ID="4">
            <Title>Structure</Title>
            <Text>Structure for Presentation

Learning Objective
Concept : Explain
Demo Example : Recorded screencast
Q &amp; A
Exercise : Work as a pair and finish the exercise within given time limit.
Review and Q &amp; A : Where did you have difficulty?

Structure for Book

Objective for the chapter.
Introduction to the concepts (if required).
Progressive lessons for a specific objective.
Code centric. Example of anti-patterns. Example of best practices. Before / After picture.
Illustrate near miss when refactoring from anti-patterns to best practice.
Chapter Summary
Exercises

Appendix
Answer Key
References

Three Levels in Learning

1. Copy actions of someone else blindly, even if it feels stupid. Use simple rules without realizing there are benefits to them.
2. Understand why you are doing things you are doing. Why they make sense?
3. To improvise. Change the way you work but keep the concepts that you now know the same. I don’t like TDD the way you taught us. I like BDD better. You will have the same concepts but you will express it in a different way.

Four Fluency Levels in TDD

TDD Level
Focus
Characteristics
Tarzan at TDD
Code
Can write unit test that will execute in the runner.
Assert.isTrue().  May start with a test, but soon drifts into
code first development. Tests may break when ‘not on my box’.
Getting to TDD
Coverage
Uses red, green, refactor cycle. Begins to see defect
reduction, less ‘silly’ bugs.  Tests may have duplicate setup
or code. Long ‘work-flow’ tests with many assertions. Inappropriate use of file or db resources. Test files and class files may be many to many relationship. CI build may begin to take a longer time.
What happened at TDD?
Maintainability
Learns from TDD experience how to write better tests.
Code quality of tests as good as core code. Effective use of
setup and teardown. Organizes tests effectively, parity between tests and classes. Uses mocks effectively. Tests and Code still
seen as separate steps in a process. Tests run in CI in effective time
frame.
Why do we have TDD?
Design
Uses tests to express intent that causes simple, effective code to emerge. Sees and factors to patterns effectively. Strong cohesion between test and class. Can safely check-in after every cycle.


Exercise 1 

Calculator : Add feature. Work in pairs. The implementer is lazy and writes ugly code. Tester forces generalized solution and is the designer of the API. Can the tester force internal quality of the code?

——————————————————————————————————————————————————
Overtime our daily habits become refined skills and abilities, and translate into our expertise.
						— Brain Fitness : Peak Performance
——————————————————————————————————————————————————

Keith Braithwaite’s rules to force the pair to allow the design to evolve:

The Rules
	1.	Write exactly one new test, the smallest test you can that seems to point in the direction of a solution
	2.	See it fail
	3.	Make the test from (1) pass by writing the least implementation code you can in the test method. 
	4.	Refactor to remove duplication, and otherwise as required to improve the design. Be strict about using these moves:
	1.	you want a new method—wait until refactoring time, then… create new (non-test) methods by doing one of these, and in no other way:
	1.	preferred: do Extract Method on implementation code created as per (3) to create a new method in the test class, or
	2.	if you must: move implementation code as per (3) into an existing implementation method
	2.	you want a new class—wait until refactoring time, then… create non-test classes to provide a destination for a Move Method and for no other reason
	1.	populate implementation classes with methods by doing Move Method, and no other way
The member of the pair without their hands on the keyboard must be very strict in enforcing these rules, especially 4.1 and 4.2
Practice #1

TDD Kata- an exercise in coding, refactoring and test-first, that you should apply daily for at least 15 minutes.

	▪	Try not to read ahead.
	▪	Do one task at a time. The trick is to learn to work incrementally.
	▪	Make sure you only test for correct inputs. There is no need to test for invalid inputs for this kata.
 
String Calculator
	1.	Create a simple String calculator with a method int add(string numbers)
	1.	The method can take 0, 1 or 2 numbers, and will return their sum (for an empty string it will return 0) for example “” or “1” or “1,2”
	2.	Start with the simplest test case of an empty string and move to 1 and two numbers
	3.	Remember to solve things as simply as possible so that you force yourself to write tests you did not think about
	4.	Remember to refactor after each passing test
	2.	Allow the Add method to handle an unknown amount of numbers
	3.	Allow the Add method to handle new lines between numbers (instead of commas).
	1.	the following input is ok:  “1\n2,3”  (will equal 6)
	2.	the following input is NOT ok:  “1,\n” (not need to prove it - just clarifying)
	1.	Support different delimiters
	2.	to change a delimiter, the beginning of the string will contain a separate line that looks like this:   “//[delimiter]\n[numbers…]” for example “//;\n1;2” should return three where the default delimiter is ‘;’ .
	3.	the first line is optional. all existing scenarios should still be supported
	4.	Calling Add with a negative number will throw an exception “negatives not allowed” - and the negative that was passed. If there are multiple negatives, show all of them in the exception message. Stop here if you are a beginner. Continue if you can finish the steps so far in less than 30 minutes.
	5.	Numbers bigger than 1000 should be ignored, so adding 2 + 1001  = 2
	6.	Delimiters can be of any length with the following format:  “//[delimiter]\n” for example: “//[***]\n1***2***3” should return 6
	7.	Allow multiple delimiters like this:  “//[delim1][delim2]\n” for example “//[*][%]\n1*2%3” should return 6.
	8.	make sure you can also handle multiple delimiters with length longer than one char

http://osherove.com/tdd-kata-1/


</Text>
        </Document>
        <Document ID="117">
            <Title>Kick-Starting the Test-Driven Cycle</Title>
            <Text>First, Test a Walking Skeleton

A walking skeleton is an implementation of the thinnest possible slice of real functionality that we can automatically build, deploy and test end-to-end. It should include just enough of the automation, the major components, and communication mechanisms to allow us to start working on the first feature. We keep the skeleton’s application functionality so simple that it’s obvious and uninteresting, leaving us free to concentrate on the infrastructure. For example, for a database driven web app, a skeleton would show a flat web page with fields from the database.

When we write the test for the first feature, we need to write the test you want to read to make sure that it’s a clear expression of the behavior of the system. </Text>
        </Document>
        <Document ID="124">
            <Title>First Real Functionaliy</Title>
            <Text>Outside In Development

This failure defines the target for our next coding episode. It tells us, at a high level, what we’re aiming for - we just have to fill in implementation until it passes.

Our approach to TDD is to start with the outside event that triggers the behavior we want to implement and work our way into the code an object at a time, until we reach a visible effect (such as a sent message or log entry) indicating that we’ve achieved our goal. The end-to-end test shows us the end points of that process, so we can explore our way through the space in the middle.

How can we hope to catch all the configuration options in an entire system? At some level we can’t, and this is at the heart of what professional testers do. What we can do is push to exercise as much as possible of the system as early as possible, and to do so repeatedly. We can also help ourselves cope with total system complexity by keeping the quality of its components high and by constantly pushing to simplify.

Use nil When an Argument Doesn’t Matter

Pass in a nil value to satisfy the compiler but use a named constant to make clear its significance.

Discovering Further Work

We don’t want to break the flow of getting features to work, so we add error handling to the to-do list to come back to it later.

Finish the Job

Decide what we want to say and how to say it: we write a high-level end-to-end test to describe what the system should implement. We use descriptive test names to tell us what a class does; we extract new classes to tease apart fine-grained aspects of the functionality; and we write lots of little method to keep each layer of code at a consistent level of abstraction. But first, we write a rough implementation to prove that we know how to make the code do what’s required and then we refactor. 

First-cut code is not finished. It’s good enough to sort out our ideas and make sure we have everything in place, but it’s unlikely to express its intentions cleanly. That will make it a drag on productivity as it’s read repeatedly over the lifetime of the code. 

Focus, Focus, Focus

Once again, we’ve noticed complexity in a class and used that to tease out a new concept from our initial skeleton implementation.

How Should We Describe Expected Values

We’ve specified the expected bid value by adding the price and increment. There are different opinions about whether test values should just be literals with obvious values, or expressed in terms of the calculation they represent. Writing out the calculation may make the test more readable but risks reimplementing the target code in the test, and in some cases the calculation will be too complicated to reproduce. Here, we decide that the calculation is so trivial that we can just write it into the test.

This is our first test with more than one expectation, so we’ll point out that the order in which expectations are declared does not have to match the order in which the methods are called in the code. If the calling order does matter, the expectations should include a sequence clause.

Null Implementation

We define a null implementation as a temporary empty implementation, introduced to allow the programmer to make progress by deferring effort and intended to be replaced.

The End-to-End Tests Pass

Now the end-to-end tests pass. We can cross off another item on the to-do list, but that includes just catching and printing XYZException. Normally, we regard this as a very bad practice but we wanted to see the tests pass and get some structure into the code - and we know that the end-to-end tests will fail anyway if there’s a problem sending a message. To make sure we don’t forget, we add another to-do item to find a better solution.

Defer Decisions

Introduce a null implementation of a method (or even a type) to get us through the next step. This helps us focus on the immediate task without getting dragged into thinking about the next significant chunk of functionality.

Keep the Code Compiling

Minimize the time when we have to code that does not compile by keeping changes incremental.  When we have compilation failures compilers cannot tell us where the boundaries of our changes are. This means we cannot check-in often. The more code we have open, the more we have to keep in our heads which, ironically, usually means we move more slowly. One of the great discoveries of TDD is just how fine-grained our development steps can be.

Emergent Design

We’re growing a design from what looks like an unpromising start. We alternate between adding features and reflecting on and cleaning up the code that results. The cleaning up stage is essential, since without it we would end up with an unmaintainable mess. We’re prepared to defer refactoring code if we’re not yet clear what to do, confident that we will take the time when we’re ready. In the meantime, we keep our code as clean as possible, moving in small increments and using techniques such as null implementation to minimize the time when it’s broken.

</Text>
        </Document>
        <Document ID="5">
            <Title>Money Example</Title>
            <Text>Let’s start with the object multi-currency money. We have a report like this:

#
To make a multi-currency report, we need to add currencies:

￼
We also need to specify exchange rates:
￼
What behavior do we need to produce the revised report?
What is the set of tests which, when passed, will demonstrate the presence of code we are confident will compute the report correctly?

Requirements

We need to be able to add amounts in two different currencies and convert the result given a set of exchange rates.
We need to be able to multiply an amount (price per share) by a number (number of shares) and receive an amount.

To-Do List

$5 + 10 CHF = $10 if rate is 2:1
$5 * 2 = $10

Thinking Process

What test do we need first? Looking at the list, first test looks complicated. Start small or not at all.
Multiplication is easy, we will work on that first.

When we write a test, we imagine the perfect interface for our operation. We are telling ourselves a story about how the operation will look from the outside. Our story won’t always come true, but better to start from the best possible API and work backwards.

Here’s a simple example of multiplication

def test_multiplication
  five =  Dollar.new(5)
  five.times(2)
  assert(10, five.amount) 
end

Take small steps. Make a note of the stinkiness and move on. We have a failing test and we want it to go green as quickly as possible.

Updated To-Do List

$5 + 10 CHF = $10 if rate is 2:1
$5 * 2 = $10
Dollar side-effects?
Money rounding?

Thinking Process

The test gives us the following error:

Unknown constant Dollar

We can get rid of this error by defining the class Dollar

class Dollar; end;

Now we need the constructor but it doesn’t have to do anything just to get past the current error:

Class Dollar

Def initialize(amount)
End

End

We need a stub implementation of times(). Again we’ll do the least work possible just to get past the current error:

Def times(multiplier)
End

--------------------------------------------------------------------------------------------------------------
We define a null implementation as a temporary empty implementation, introduced to allow the programmer to make progress by deferring effort and intended to be replaced.
- Growing Object Oriented Software Guided by Tests
--------------------------------------------------------------------------------------------------------------

Finally, we need an amount field:

Def initialize(amount)
@amount = amount
End

Now we can run the test and watch it fail. We expected 10 as a result we saw 0. This failure is progress. Now we have a concrete measure of failure. That’s better than just vaguely knowing we are failing. Our programming problem has been transformed from “give me multi-currency” to “make this test work, and then make the rest of the tests work”. Much simpler. Much smaller scope for fear. We can make this test work.

You probably aren’t going to like the solution, but the goal right now is not to get the perfect answer, the goal is to pass the test. Here’s the smallest change that would cause our test to pass:


Xxx

Now we get the green. The cycle isn’t complete. There are very few inputs in the world that will cause such a limited, such a smelly, such a naive implementation to pass. We need to generalize before we move on. Remember, the cycle is:

1. Add a little test
2. Run all tests and fail
3. Make a little change
4. Run the tests and succeed
5. Refactor to remove duplication

Our goal is to be able to write another test that “makes sense” to us, without having to change the code, something that is not possible with the current implementation. 

If dependency is the problem, duplication is the symptom. Objects are excellent for abstracting away the duplication of logic. Eliminating duplication in programs eliminates dependency. That’s why the second rule appears in TDD. By eliminating duplication before we go on to the next test, we maximize our chance of being able to get the next test running with one and only one change.

We have run items 1-4. Now we are ready to remove duplication. But where is the duplication? Usually you see duplication between two pieces of code. Here the duplication is between the data in the test and the data in the code.

Amount = 5 * 2

We did the multiplication in our heads and did not notice that 5 and 2 are now in two places. However, what if we move the setting of the amount from object initialization to the times method?

def times(multiplier) 
  amount = 5 *2
End

The test still passes, we stay in green.

Do these steps seem too small to you? Remember, TDD is not about taking teensy tiny steps, it’s about being able to take teensy tiny steps. I would not code day-to-day with steps this small. But when things get the least bit weird, I’m glad I can. Try teensy tiny steps with an example of your own choosing. If you can make steps too small, you can certainly make steps the right size. If you only take larger steps, you’ll never know if smaller steps are appropriate.

Now, let’s get back to the job of getting rid of duplication between the test code and the working code. Where can we get a 5? That was the value passed to the constructor, so if we save it in the amount variable:

  initialize(amount)
@Amount = amount
End

We can use it in times:

Times(multiplier)
  Amount = amount * 2
End

The value of the parameter ‘multiplier’ is 2, so we can substitute the parameter for the constant:

def times(multiplier)
  amount *= multiplier
end

We can now mark off the first test as done. Next, we’ll take care of those strange side effects. First, let’s review. We:

Made a list of the tests we knew we needed to have working.
Told a story with a snippet of code about how we wanted to view one operation.
Ignored the details of MiniTest for the moment.
Made the test run with stubs
Made the test run by committing horrible sins
Gradually generalized the working code, replacing constants with variables
Added items to our to-do list rather than addressing them all at once

Updated To-Do List

$5 + 10 CHF = $10 if rate is 2:1
$5 * 2 = $10
Dollar side-effects?
Money rounding?
Make ‘amount’ private



 </Text>
        </Document>
        <Document ID="69">
            <Title>Private</Title>
            <Text>Make “amount” private

Now that we have defined equality, we can use it to make our tests more expressive. Conceptually the operation times should return a Dollar whose value is the value of the receiver times the multiplier. Our test doesn’t exactly say that:

Test_multiplication
Five = Dollar.new(5)
Product = five.times(2)
Assert(10, product.amount)
Product = five.times(3)
Assert(15, product.amount)

We can rewrite the first assertion to compare Dollars to Dollars

Test_multiplication
Five = Dollar.new(5)
Product = five.times(2)
Assert(Dollar.new(10), product)
Product = five.times(3)
Assert(15, product.amount)

That looks better, so we rewrite the second assertion, too:

Test_multiplication
  Five = Dollar.new(5)
  Product = five.times(2)
  Assert(Dollar.new(10), product)
Product = five.times(3)
Assert(Dollar.new(15, product))

Now the temporary variable “product” isn’t helping much, so we can inline it:

Test_multiplication
Five = Dollar.new(5)
Assert(Dollar.new(10), five.times(2))
Assert(Dollar.new(15), five.times(3))

This test speaks to us more clearly, as if it were an assertion of truth, not a sequence of operations.

With these changes to the test, Dollar is now the only class using its “amount” instance variable, so we can make it private:

Code goes here…

And we cross another item off the list. Notice that we have opened ourselves up to a risk. If the test for equality fails to accurately check that equality is working, the test for multiplication could also fail to accurately check that multiplication is working. That is a risk you actively manage in TDD. We aren’t striving for perfection. By saying everything two ways, as both code and tests, we hope to reduce our defects enough to move forward with confidence. From time to time our reasoning will fail us and a defect will slip through. When that happens, we learn our lesson about the test we should have written and move on. The rest of the time we go forward boldly under green.

Reviewing, we:

Used functionality just developed to improve a test
Noticed that if two tests fail at once we’re sunk
Proceeded in spite of the risk
Used new functionality in the object under test to reduce coupling between the tests and the code.
</Text>
        </Document>
        <Document ID="90">
            <Title>Counting</Title>
            <Text>I was going to implement making sure tear_down is called regardless of exceptions during the test method. However, we have to catch exception to make the test work. If we make a mistake implementing this, we won’t be able to see the mistake because the exceptions won’t be reported. In general, the order of implementing the tests is important. When I pick the next test to implement, I find a test that will teach me something but which I have confidence I can make it work. If I get that test working but get stuck on the next one, I consider backing up two steps.

Reviewing, we:
Wrote a fake implementation, and gradually began making it real by replacing constants with variables.
Wrote another test.
When that test failed, we wrote yet another test, at a smaller scale to support making the failing test work.</Text>
        </Document>
        <Document ID="83">
            <Title>Mixed Currencies</Title>
            <Text> Now we are finally ready to add the test that started it all, $5 + 10 CHF:

public void testMixedAddition() {
Expression fiveBucks= Money.dollar(5);
Expression tenFrancs= Money.franc(10);
Bank bank= new Bank();
bank.addRate("CHF", "USD", 2);
Money result= bank.reduce(fiveBucks.plus(tenFrancs), "USD");
assertEquals(Money.dollar(10), result);
}

 This is what we’d like to write. Unfortunately, there are a host of compile errors.
When we were generalizing from Money to Expression, we left a lot of loose ends
laying around. I was worried about them, but I didn’t want to disturb you. It’s disturbing
time, now.

We won’t be able to get the test above to compile quickly.We will make the first
change that will ripple to the next and the next.We have two paths forward.We can
make it work quickly by writing a more specific test and then generalizing, or we
can trust our compiler not to let us make mistakes. I’m with you—let’s go slow (in
practice I would probably just fix the rippling changes one at a time).

public void testMixedAddition() {
Money fiveBucks= Money.dollar(5);

 Money tenFrancs= Money.franc(10);
Bank bank= new Bank();
bank.addRate("CHF", "USD", 2);
Money result= bank.reduce(fiveBucks.plus(tenFrancs), "USD");
assertEquals(Money.dollar(10), result);
}

The test doesn’t work.We get 15 USD instead of 10 USD. It’s as if Sum.reduce()
isn’t reducing the arguments. It isn’t:
Sum
public Money reduce(Bank bank, String to) {
int amount= augend.amount + addend.amount;
return new Money(amount, to);
}

If we reduce both of the arguments, the test should pass:
Sum
public Money reduce(Bank bank, String to) {
int amount= augend. reduce(bank, to).amount +
addend. reduce(bank, to).amount;
return new Money(amount, to);
}

And it does. Now we can begin pecking away at Moneys that should be Expressions.
To avoid the ripple effect, we’ll start at the edges and work our way back to
the test case. For example, the augend and addend can now be Expressions:

Sum
 Expression augend;
 Expression addend;
The arguments to the Sum constructor can also be Expressions:
Sum
Sum( Expression augend,  Expression addend) {
this.augend= augend;
this.addend= addend;
}

(Sum is starting to remind me of Composite, but not so much that I want to generalize.
The moment we want a Sum with other than two parameters, though, I’m ready
to transform it.) So much for Sum. How about Money?

The argument to plus() can be an Expression:
Money
Expression plus(Expression addend) {
return new Sum(this, addend);
}

Times() can return an Expression:
Money
Expression times(int multiplier) {
return new Money(amount * multiplier, currency);
}

This suggests that Expression should include the operations plus() and times().
That’s all for Money. We can now change the argument to plus() in our test case:

public void testMixedAddition() {
Money fiveBucks= Money.dollar(5);
Expression tenFrancs= Money.franc(10);
Bank bank= new Bank();
bank.addRate("CHF", "USD", 2);
Money result= bank.reduce(fiveBucks.plus(tenFrancs), "USD");
assertEquals(Money.dollar(10), result);
}

When we change fiveBucks to an Expression, we have to make several changes.
Fortunately we have the compiler’s to-do list to keep us focused. First we make the
change:

public void testMixedAddition() {
Expression fiveBucks= Money.dollar(5);
Expression tenFrancs= Money.franc(10);
Bank bank= new Bank();
bank.addRate("CHF", "USD", 2);
Money result= bank.reduce(fiveBucks.plus(tenFrancs), "USD");
assertEquals(Money.dollar(10), result);
}

 We are politely told that plus() is not defined for Expressions. We define it:
Expression
Expression plus(Expression addend);
 And then we have to add it to Money and Sum. Money? Yes, it has to be public in
Money:

Money
public Expression plus(Expression addend) {
return new Sum(this, addend);
}
 We’ll just stub out the implementation in Sum, and add it to our list:
Sum
public Expression plus(Expression addend) {
return null;
}

 Now that the program compiles, the tests all run.
We are ready to finish generalizing Money to Expression, but first we’ll review. We:

•  Wrote the test we wanted, then backed off to make it achievable in one step
•  Generalized (used a more abstract declaration) from the leaves back to the root
(the test case)
•  Followed the compiler when we made a change (Expression fiveBucks) which
caused changes to ripple (added plus() to Expression, etc.)</Text>
        </Document>
        <Document ID="75">
            <Title>References</Title>
            <Text>1. Test Driven Development by Example - Kent Beck
2. The Art of Unit Testing with Examples in .NET
3. Growing Object Oriented Software Guided by Tests
4. xUnit Test Patterns    
5. Brain Fitness : Peak Performance

Recommended Books

1. Refactoring by Marin Fowler
2. Object Oriented Design Heuristics by Arthur Reihl
3. Object Oriented Software Construction by Bertrand Meyer
4. Design Patterns 

http://softwaregreenhouses.com/2011/01/30/using-wayk-to-describe-tdd-fluency/

	▪	We verify stubs by checking state after an interaction.
	▪	We tell mocks to verify interactions.
	▪	Sometimes stubs just make the system run.

Isolation from Non-Determinism:  randomness, time etc.
Isolation from External Dependencies:  Database, Network, Third Party API etc
Polymorphic Collaborators: E.g., employee that knows how to pay itself, uses a strategy. 
payment_strategy = mock() 
employee = E.new(payment_strategy) 
payment_strategy.expects(:pay) 
employee.pay
When are message expectations helpful?
Side Effects: background processing
Caching: only call a network zipcode lookup once
Interface discovery: tool to discover the parts of the system that you haven't really worked out yet. Mock something out that doesn't exist yet, while designing its interface.

All of these concepts are Isolation Testing - testing an object in isolation from others. This is a good fit when you have lots of little objects.

BDD Approach : 
Customer specs are implemented as end to end testing.
Developer specs are implemented as isolation tests.

	▪	Keep things simple
	▪	Focus on Roles
	▪	Try to avoid tight coupling
	▪	Complex setup is a red flag for design issues
	▪	Don't stub and mock the object that you are testing
	▪	Concern: impedes refactoring (but some say refactoring is improving design without changing behavior, so tests should not change. This really depends what level you are refactoring at).
	▪	Concern: false positives

Stubs - a common helper to testing environments. There is a difference in how test results are verified: a distinction between state verification and behavior verification. Focusing on one element of the software at a time -hence the common term unit testing. The problem is that to make a single unit work, you often need other units.

	•	Dummy objects are passed around but never actually used. Usually they are just used to fill parameter lists.
	•	Fake objects actually have working implementations, but usually take some shortcut which makes them not suitable for production (an in memory database is a good example).
	•	Stubs provide canned answers to calls made during the test, usually not responding at all to anything outside what's programmed in for the test. Stubs may also record information about calls, such as an email gateway stub that remembers the messages it 'sent', or maybe only how many messages it 'sent'.
	•	Mocks are what we are talking about here: objects pre-programmed with expectations which form a specification of the calls they are expected to receive.

Only mocks insist upon behavior verification. The other doubles can, and usually do, use state verification.

The classical TDD style is to use real objects if possible and a double if it's awkward to use the real thing. A mockist TDD practitioner, however, will always use a mock for any object with interesting behavior.
An acknowledged issue with state-based verification is that it can lead to creating query methods only to support verification. It's never comfortable to add methods to the API of an object purely for testing, using behavior verification avoids that problem.
</Text>
        </Document>
        <Document ID="76">
            <Title>Eliminating Subclasses</Title>
            <Text>What is there on our list that might help us eliminate the useless subclasses? What about currency? What would happen if we introduced the notion of currency?

How do we want to implement currencies at the moment? Oops. How do we want to test for currencies at the moment?

We may want to have complicated objects representing currencies, with flyweight factories to ensure we create no more objects than we really need. However, for the moment Strings will do:

public void  testCurrency() {
assertEquals("USD" , Money.dollar(1).currency());
assertEquals("CHF" , Money.franc(1).currency());
}
First we declare currency() in Money:
Money
abstract  String currency();

 Then we implement it in both subclasses:
Franc
String currency() {
return "CHF";
}
Dollar
String currency() {
return "USD";
}

 We want the same implementation to suffice for both classes.We could store the
currency in an instance variable and just return the variable.

Franc
private String currency;
Franc(int amount) {
this.amount = amount;
currency = "CHF";
}
String currency() {
return currency;
}
 We can do the same with Dollar:
Dollar
private String currency;
Dollar(int amount) {
this.amount = amount;
currency = "USD";
}
String currency() {
return currency;
}
 Now we can push up the declaration of the variable and the implementation of currency(),
since they are identical:

Money
protected String currency;
String currency() {
return currency;
}
If we move the constant strings “USD” and “CHF” to the static factory methods,
the two constructors will be identical and we can create a common implementation.
First we’ll add a parameter to the constructor:
Franc
Franc(int amount, String currency) {
this.amount = amount;
this.currency = "CHF";
}
This breaks the two callers of the constructor:
Money
static Money franc(int amount) {
return new Franc(amount, null);
}
Franc
Money times(int multiplier) {
return new Franc(amount * multiplier, null);
}
Wait a minute! Why is Franc.times() calling the constructor instead of the factory
method? Do we want to make this change now, or will we wait? The dogmatic
answer is that we’ll wait, not interrupting what we’re doing. The answer in my
practice is that I will entertain a brief interruption, but only a brief one, and I will
never interrupt an interruption. To be realistic, we’ll clean up times() before proceeding:
Franc
Money times(int multiplier) {
return Money.franc(amount * multiplier);
}
Now the factory method can pass “CHF”:

Money
static Money franc(int amount) {
return new Franc(amount, "CHF");
}
And finally we can assign the parameter to the instance variable:
Franc
Franc(int amount, String currency) {
this.amount = amount;
this.currency = currency;
}
I’m feeling defensive again about taking such teeny-tiny steps. Am I recommending
that you actually work this way? No. I’m recommending that you be able to
work this way. What I actually did just now was I worked in larger steps and made
a stupid mistake half way through. I unwound a minute’s worth of changes, shifted
to a lower gear, and did it over with little steps. I’m feeling better now, so we’ll see
if we can make the analogous change to Dollar in one swell swoop:
Money
static Money dollar(int amount) {
return new Dollar(amount, "USD");
}
Dollar
Dollar(int amount, String currency) {
this.amount = amount;
this.currency = currency;
}
Money times(int multiplier) {
return Money.dollar(amount * multiplier);
}
And it worked first time.
This is the kind of tuning you will be doing constantly with TDD. Are the teeny tiny
steps feeling restrictive? Take bigger steps. Are you feeling a little unsure?
Take smaller steps. TDD is a steering process—a little this way, a little that way.
There is no right step size.
The two constructors are now identical, so we can push up the implementation:

Money
Money(int amount, String currency) {
this.amount = amount;
this.currency = currency;
}
Franc
Franc(int amount, String currency) {
super(amount, currency);
}
Dollar
Dollar(int amount, String currency) {
super(amount, currency);
}

We’re almost ready to push up the implementation of times() and eliminate the subclasses,
but first, to review, we:

• Were a little stuck on big design ideas, so we worked on something small we noticed earlier
• Reconciled the two constructors by moving the variation to the caller (the factory method)
• Interrupted a refactoring for a little twist (using the factory method in times())
• Repeated an analogous refactoring (doing to Dollar what we just did to Franc) in one big step
• Pushed up the identical constructors</Text>
        </Document>
        <Document ID="91">
            <Title>Dealing with Failure</Title>
            <Text>Reviewing, we:
Made our small scale test work
Reintroduced the larger scale test
Made the larger test work quickly using the mechanism demonstrated by the smaller test
</Text>
        </Document>
        <Document ID="118">
            <Title>Result Verification Patterns</Title>
            <Text>State Verification

State Based Testing

How do we make tests self-checking when there is state to be verified?
We inspect the state of the SUT after it has been exercised and compare it to the expected state.
Fig goes here.

We exercise the SUT by invoking the methods of interest. Then, as a separate step, we interact with the SUT to retrieve its post-exercise state and compare it with the expected end state by calling assertion methods.

We should use state verification when we care about only the end state of the SUT - not how the SUT got there. Taking such a limited view helps us maintain encapsulation of the implementation of the SUT.

State verification comes naturally when we are building the software inside out. That is, we build the innermost objects first and then build the next layer of objects on top of them. Of course, we may need to use test stubs to control the indirect inputs of the SUT to avoid production bugs caused by untested code paths. Even then, we are choosing not to verify the indirect outputs of the SUT.

When we do care about the side effects of exercising the SUT that are not visible in its end state (its indirect outputs), we can use behavior verification to observe the behavior directly. We must be careful, however, not to create fragile tests by over specifying the software.

There are two basic styles of implementing state verification.

Procedural State Verification

We write a series of calls to assertion methods that pick apart the state information into pieces and compare those bits of information to individual expected values. Most people who are new to automating tests take such a path of least resistance. The major disadvantage of this approach is that it can result in obscure tests owing to the number of assertions it may take to specify the expected outcome. When the same sequence of assertions must be carried out in many tests or many times within a single test method we also have test code duplication.

Expected State Specification

Expected Object

When doing Expected State Specification, we construct a specification for the post-exercise state of the SUT in the form of one or more objects populated with the expected attributes. We then compare the actual state directly with these objects using a single call to an equality assertion. This tends to result in more concise and readable tests. We can use and Expected State Specification whenever we need to verify several attributes and it is possible to construct an object that looks like the object we expect the SUT to return. The more attributes we have that need to be compared and the more test that need to compare them, the more compelling the argument for using an Expected State Specification. In the most extreme cases, when we have a lot of data to verify, we can construct an expected table and verify that the SUT contains it. FIT’s row fixtures offer a good way to do this in customer tests.

The Expected State Specification is most often an instance of the same class that we expect to get back from the SUT. We may have difficulty using Expected State Specification if the object doesn’t implement equality in a way that involves comparing the values of attributes (e., by comparing the object references with each other) or if our test-specific definition of equality differs from that implemented by the equals method.

In these cases, we can still use an Expected State Specification if we create a custom assertion that implements test specific equality. Alternatively, we can build the Expected State Specification from a class that implements our test specific equality. This class can either be a test specific subclass that overrides the equals method.

When the class is difficult to instantiate, we can define a fake object that has the necessary attributes plus an equals method that implements test specific equality. These last few tricks are made possible by the fact that equality assertions usually ask the expected state specification to compare itself to the actual result, rather than the reverse.

We can build the Expected State Specification either during the result verification phase of the test immediately before it is used in the equality assertion or during the fixture setup phase of the test. The latter strategy allows us to use attributes of the expected state specification as parameters to the SUT or as the base for derived values when building other objects in the test fixture. This makes it easier to see the cause - effect relationship between the fixture and the expected state specification, which in turn helps us achieve tests as documentation. It is particularly useful when the expected state specification is created out of sight of the reader such as when using creation methods to do the construction.

Behavior Verification

Interaction Testing

How do we make tests self-checking when there is no state to verify?

We capture the indirect outputs of the SUT as they occur and compare them to the expected behavior. A self-checking test must verify that the expected outcome has occurred without manual intervention. But what do we mean by expected outcome? The SUT may not be stateful; if it is stateful, it may not be expected to end up in a different state after it has been exercised. The SUT may also be expected to invoke methods on other objects.

Behavior verification involves verifying the indirect outputs of the SUT as it is being exercised.

Each test specifies not only how the client of the SUT interacts with it during the exercise SUT phase of the test, but also how the SUT interacts with the components on which it should depend. This ensures that the SUT really is behaving as specified rather than just ending up in the correct post-exercise state.

Behavior verification involves interacting with or replacing a DoC with which the SUT interacts at runtime. The line between behavior verification and state verification can get a bit blurry when the SUT stores its state in the DoC because both forms of verification involves layer-crossing tests. We can distinguish between the two cases based on whether we are verifying the post-test state in the DoC (State Verification) or whether we are verifying the method calls made by the SUT on the DoC (Behavior Verification).

Behavior Verification is primarily a technique for unit tests and component tests. We can use behavior verification whenever the SUT calls methods on other objects or components. We  must use behavior verification whenever the expected outputs of the SUT are transient and cannot be determined simply by looking at the post-exercise state of the SUT or the DoC. This forces us to monitor these indirect outputs as they occur.

A common application of behavior verification is when we are writing our code in an outside-in manner. This approach, which is often called need-driven development, involves writing the client code before we write the DoC. It is a good way to find out exactly what the interface provided by the DoC needs to be based on real, concrete examples rather than on speculation. The main objection to this approach is that we need to use a lot of test doubles to write these tests. That could result in fragile tests because each test knows so much about how the SUT is implemented. Because the tests specify the behavior of the SUT in terms of its interactions with the DoC, a change in the implementation of the SUT could break a lot of tests. This kind of over specified software could lead to high test maintenance cost.

In most cases state verification is clearly necessary; in some cases behavior verification is clearly necessary. One reason why behavior verification came about : Some functionality of the SUT is not visible within the end state of the SUT itself, but can be seen only if we intercept the behavior at an internal observation point between the SUT and the DoC or if we express the behavior in terms of state changes for the objects with which the SUT interacts.

Before we exercise the SUT by invoking the methods of interest, we must ensure that we have a way of observing its behavior. Sometimes the mechanisms that the SUT uses to interact with the components surrounding it make such observation possible; when this is not the case, we must install some sort of test double to monitor the SUT’s indirect outputs. We can use a test double as long as we have a way to replace the DoC with the test double. This could be via dependency injection.

There are two fundamentally different ways to implement behavior verification. The mock object is commonly used approach for expected behavior specification.

Procedural Behavior Verification

We capture the method calls made by the SUT as it executes and later get access to them from within the test method. Then we use equality assertions to compare them with the expected results.

The most common way of trapping the indirect outputs of the SUT is to install a test spy in place of the DoC during fixture setup phase. During the result verification phase of the test, we ask the test spy how it was used by the SUT during the exercise SUT phase. 

Expected Behavior Specification

Instead of waiting until after the fact to verify the indirect output of the SUT by using a sequence of assertions, we load the Expected Behavior Specification into a mock object and let it verify that the method calls are correct as they are received.

We can use an Expected Behavior Specification when we know exactly what should happen ahead of time and we want to remove all procedural behavior specification from the test method. This tends to make the test shorter (assuming we are using a compact representation of the expected behavior) and can be used to cause the test to fail on the first deviation from the expected behavior if we so choose.

Custom Assertion

How do we make tests self-checking when we have test-specific equality logic? How do we reduce test code duplication when the same assertion logic appears in many tests? How do we avoid conditional test logic?
We create a purpose-built assertion method that compares only those attributes of the object that define test-specific equality.

Fig

This test would be so much easier to write if I just had an assertion that did… We hide the complexity of whatever it takes to prove the system is behaving correctly behind an assertion method with an intent revealing name.

We encapsulate the mechanics of verifying that something is true (an assertion) behind an intent revealing name. To do so, we factor out all the common assertion code within the tests into a custom assertion that implements the verification logic. A custom equality assertion takes two parameters: an expected object and the actual object.

Typically custom assertions are created by identifying common patters of assertions in our tests. We might also call a nonexistent custom assertion because it makes writing our test easier; this tactic lets us focus on the part of the SUT that needs to be tested rather than the mechanics of how the test would be carried out.

Custom assertions are created when:

We find ourselves writing the same assertion logic in test after test.
We find ourselves writing conditional test logic in the result verification part of our tests. That is, our calls to assertion methods are embedded in if statements or loops.
The result verification parts of our tests suffer from obscure test because we use procedural rather than declarative result verification in the tests.
We find ourselves doing frequent debugging whenever assertions fail because they do not provide enough information.

Custom Equality Assertion

The custom assertion must be passed an expected object and the actual object to be verified. It should also take an assertion message.

Domain Assertion

A domain assertion is a stated outcome assertion that states something that should be true in domain specific terms

Diagnostic Assertion

A special kind of custom assertion that may look like one of the built-in assertions but provide more information about what is different between the expected and actual values than a built-in assertion.

Verification Method

A form of custom assertion that interact directly with the SUT, thereby relieving their callers from this task. This simplifies the tests significantly and leads to a more declarative style of outcome specification. 

Custom Assertion Test

The assertion method is the SUT, the exercise SUT and verify outcome phases of the Four-Phase Test are combined into a single phase.

Delta Assertion

How do we make tests self-checking when we cannot control the initial contents of the fixture? We specify assertions based on differences between the pre and post exercise state of the SUT.

Fig.

Guard Assertion

How do we avoid conditional test logic? We replace an if statement in a test with an assertion that fails the test if not satisfied.

Fig.

Some verification logic may fail because information returned by the SUT is not initialized as expected. When a test encounters an unexpected problem, it may produce a test error rather than a test failure. 

We should use a guard assertion whenever we want to avoid executing statements in our test method because they would cause an error if they were executed when some condition related to values returned by the SUT is not true. We take this step instead of putting an if then else fail code construct around the sensitive statements. Normally a guard assertion is placed between the exercise SUT and the verify outcome phases of a Four-Phase test.</Text>
        </Document>
        <Document ID="125">
            <Title>Second Functionality</Title>
            <Text>Making Steady Progress

We made steady progress by adding little slices of functionality. First we made the Sniper show when it’s winning, then when it has won. We used empty implementation to get us through the compiler when we weren’t ready to fill in the code, and we stayed focused on the immediate task.

20/20 Hindsight

Forehead slapping moments that make us wonder why we didn’t see it the first time around. Surely, if we’d spent more time on the design, we wouldn’t have to change it now? Sometimes that’s true. Our experience, however, is that nothing shakes out a design like trying to implement it. It’s difficult to get design always right. Our coping mechanism is to get into the critical areas of the code early and to allow ourselves to change our collective mind when we could do better. We rely on our skills, on taking small steps, and on the tests to protect us when we make changes.

A Defect Exception

Define a runtime exception DefectException or ProgrammerMistakeExpection. Throw this when the code reaches a condition that could only be caused by a programming error, rather than a failure in the runtime environment.

Keyhole Surgery for Software

We repeatedly used the practice of adding little slices of behavior all the way through the system: replace label with a table, get that working; show the sniper bidding, get that working; add the other values, get that working; In all of these cases, we’ve figured out where we want to get to (always allowing that we might discover a better alternative along the way), but we want to avoid ripping the application apart to get there. Once we start a major rework, we can’t stop until it’s finished, we can’t check in without branching, and merging with rest of the team is harder. 

Programmer Hyper-Sensitivity

We have a well-developed sense of the value of our own time. We keep an eye out for activities that don’t seem to be making the best of our talents, such as boiler-plate copying and adapting code: if we had the right abstraction, we wouldn’t have to bother. Sometimes this just has to be done, especially when working with existing code - but there are fewer excuses when it’s our own. Deciding when to change the design requires a good sense for tradeoffs, which implies both sensitivity and technical maturity: “I’m about to repeat this code with minor variations, that seems dull and wasteful” as against “This may not be the right time to rework this, I don’t understand it yet.”

Developers should have a habit of reflecting on their activity, on the best way to invest their time for the rest of a coding session. This might mean carrying on exactly as before, but at least they will have thought about it.

Celebrate Changing Your Mind

We renamed several features in code. This is an essential part of our development process. Just as we learn more about what the structure should be by using the code we’ve written, we learn more about the names we’ve chosen when we work with them. We see how the type and method names fit together and whether the concepts are clear, which stimulates the discovery of new ideas. If the name of a feature isn’t right, the only smart thing to do is change it and avoid countless hours of confusion for all who will read the code later.

The End of Off-by-One Errors

Getting indexing right can be tricky, except in the simplest cases, and writing tests first clarifies the boundary conditions and then checks that our implementation is correct.

Making Progress While We Can

We can make development progress whilst the design is being sorted out. We can build to the team’s current understanding of the features and keep our code (and attitude) flexible to respond to design ideas as they firm up - and perhaps even feed our experience back into the process.

A Design Moment

To review our position: we have a broken acceptance test pending, we have the user interface structure but no behavior. This is a distinction we’d like to maintain, since it keeps the responsibilities of the two classes focused. We stop for a moment to think about the structure of the code, using the CRC cards to help us visualize our ideas.

TDD Confidential

It took us couple of attempts to get this design pointing in the right direction because we were trying to allocate behavior to the wrong objects. What kept us honest was that for each attempt to write tests that were focused and made sense, the setup and our assertions kept drifting apart. Once we’d broken through our inadequacies as programmers, the tests became much clearer.

Finding a Role

How do you know if an object has too many responsibilities? One clue is to look at its imports. Is it importing code from unrelated packages?

Incremental Architecture

We arrived at this design incrementally, by adding features and repeatedly following heuristics. Although we rely on our experience to guide our decisions, we reached this solution almost automatically by just following the code and taking care to keep it clean.

Three Point Contact

We can do significant refactorings incrementally. When we’re not sure what to do next or how to get there from here, one way of coping is to scale down the individual changes we make. By repeatedly fixing local problems in code, we find we can explore the design safely, never straying more than a few minutes from working code. Usually this is enough to lead us towards a better design, can we can always backtrack and take another path if it doesn’t work out.

One way to think of this is the rock-climbing rule of three-point contact. Trained climbers only move one limb at a time to minimize the risk of falling off. Each move is minimal and safe, but combining enough of them will get you to the top of the route. With experience, we’ve learned to recognize fault lines in code so we can often take a more direct route.

Dynamic as Well as Static Design

Refactoring is a design activity. We should consider more than one view when refactoring code. We still need all the design skills. Refactoring is so focused on static structure (classes and interfaces) that it’s easy to lose sight of an application’s dynamic structure (instances and threads). Sometimes we just need to step back and draw out, say, an interaction diagram.	
Distinguishing between Test Setup and Assertions

We use the allowing clause to distinguish between the test setup (getting the SUT into the right state) and the significant test assertion. Expressiveness is the only way for the tests to remain meaningful, and therefore useful, over time.

Other Modeling Techniques Still Work

TDD works best when it’s based on skill and judgment acquired from as wide an experience as possible - which includes taking advantage of older techniques and formats. State transition diagrams are one example of taking another view. What’s nice about state transition diagrams is that they map directly onto tests, so we can show that we’ve covered all the possibilities.

The trick is to understand and use other modeling techniques for support and guidance, not as an end in themselves - which is how they got a bad name in the first place. When we’re doing TDD and we’re uncertain what to do, sometimes stepping back and opening a pack of index cards, or sketching out the interactions, can help us regain direction.

Domain Types Are Better Than String

It’s often better to define domain types to wrap not only strings but other built-in types too, including collections. </Text>
        </Document>
        <Document ID="84">
            <Title>Abstraction</Title>
            <Text> We need to implement Sum.plus() to finish Expression.plus, and then we need
Expression.times(), and then we’re finished with the whole example. Here’s the test
for Sum.plus():

public void testSumPlusMoney() {
Expression fiveBucks= Money.dollar(5);
Expression tenFrancs= Money.franc(10);
Bank bank= new Bank();
bank.addRate("CHF", "USD", 2);
Expression sum= new Sum(fiveBucks, tenFrancs).plus(fiveBucks);
Money result= bank.reduce(sum, "USD");
assertEquals(Money.dollar(15), result);
}

 We could have created a Sum by adding fiveBucks and tenFrancs, but the form
above, where we explicitly create the Sum, communicates more directly. You are
writing these tests not just to make your experience of programming more fun and
rewarding, but also as a Rosetta Stone for future generations to appreciate your
genius. Think, oh think, of your readers.

The test, in this case, is longer than the code. The code is the same as the code in
Money (do I hear an abstract class in the distance?):

Sum
public Expression plus(Expression addend) {
return new Sum(this, addend);
}

You will likely end up with about the same number of lines of test code as model
code when TDDing. For TDD to make economic sense, either you will have to be
able to write twice as many lines per day as before, or write half as many lines for
the same functionality. You’ll have to measure and see what effect TDD has on
your own practice. Be sure to factor debugging, integrating, and explaining time
into your metrics, though.

If we can make Sum.times() work, then declaring Expression.times() will be one
simple step. The test is:
public void testSumTimes() {
Expression fiveBucks= Money.dollar(5);
Expression tenFrancs= Money.franc(10);
Bank bank= new Bank();
bank.addRate("CHF", "USD", 2);
Expression sum= new Sum(fiveBucks, tenFrancs).times(2);
Money result= bank.reduce(sum, "USD");
assertEquals(Money.dollar(20), result);
}

Again, the test is longer than the code (you JUnit geeks will know how to fix that—
the rest of you will have to read Fixture):

Sum
Expression times(int multiplier) {
return new Sum(augend.times(multiplier),
addend.times(multiplier));
}

Since we abstracted augend and addend to Expressions in the last chapter, we now
have to declare times() in Expression before the code will compile:

Expression
Expression times(int multiplier);
Which forces us to raise the visibility of Money.times() and Sum.times():

Sum
public Expression times(int multiplier) {
return new Sum(augend.times(multiplier),
addend.times(multiplier));
}

Money
public Expression times(int multiplier) {
return new Money(amount * multiplier, currency);
}

And it works.
The only loose end to tie up is to experiment with returning a Money when we add
$5 + $5. The test would be:

public void testPlusSameCurrencyReturnsMoney() {
Expression sum= Money.dollar(1).plus(Money.dollar(1));
assertTrue(sum instanceof Money);
}

This test is a little ugly, because it is testing the guts of the implementation, not the
externally visible behavior of the objects. However, it will drive us to make the
changes we need to make, and this is only an experiment, after all. Here is the code
we would have to modify to make it work:
Money
public Expression plus(Expression addend) {
return new Sum(this, addend);
}

There is no obvious, clean way (not to me, anyway, I’m sure you could think of
something) to check the currency of the argument if and only if it is a Money. The
experiment fails, we delete the test (which we didn’t like much anyway), and away
we go.

Reviewing, we:

• Wrote a test with future readers in mind
• Suggested an experiment comparing TDD with your current programming style
• Once again had changes of declarations ripple through the system, and once
again followed the compiler’s advice to fix them
 Tried a brief experiment, then discarded it when it didn’t work out</Text>
        </Document>
        <Document ID="67">
            <Title>TDD Cyle</Title>
            <Text>TDD Cycle

The general TDD cycle is :

1. Write a test. 

Think about how you would like the operation in your mind to appear in your code. You are writing a story. Invent the interface you wish you had. Include all the elements in the story that you imagine will be necessary to calculate the right answers.

2. Make it run. 

Quickly getting that bar green dominates everything else. If a clean, simple solution is obvious, type it in. If the clean, simple solution is obvious but it will take you a minute, make a note of it and get back to the main problem, which is getting the bar green in seconds. This shift in aesthetics is hard for some experienced software engineers. They only know how to follow the rules of good engineering. Quick green excuses all sins. But only for a moment.

3. Make it right.

Now that the system is behaving, put the sinful ways of the recent past behind you. Remove the duplication that you have introduced to get to quick green.

The goal is clean code that works. Divide and conquer. First we’ll solve the ‘that works’ part of the problem. Then we’ll solve the ‘clean code’ part. 

Updated To-Do List

$5 + 10 CHF = $10 if rate is 2:1
$5 * 2 = $10
Dollar side-effects?
Money rounding?

Thinking Process

We got one test working but in the process we noticed something strange - when we perform an operation on a Dollar, the Dollar changes. I would like to be able to write:

test_multiplication
  five = Dollar.new(5)
  five.times(2)
  assert(10, five.amount)
  five.times(3)
  assert(15, five.amount)
end

I can’t imagine a clean way to get this test working. After the first call to times, five isn’t five anymore, it’s really ten. If, however, we return a new object from times, we can multiple our original five bucks all day and never have it change. We are changing the interface of Dollar when we make this change, so we have to change the test. That’s ok. Our guesses about the right interface are no more likely to be perfect than our guesses about the right implementation.

test_multiplication
  five = Dollar.new(5)
  product = five.times(2)
  assert(10, product.amount)
  product = five.times(3)
  assert(15, product.amount)
end

Run the test.

Pg 31

Making the test requires that we return a new Dollar with the correct amount:

def times(multiplier)
  return Dollar.new(amount*multiplier)
end

In the last chapter when we make a test work we started with a bogus implementation and gradually made it real. Here, we typed in what we thought was the right implementation and prayed while the tests ran. Because we got lucky and the test ran, we can cross of another item.

Updated To-Do List

$5 + 10 CHF = $10 if rate is 2:1
$5 * 2 = $10
Dollar side-effects?
Money rounding?

Thinking Process

These are two of the three strategies I know for quickly getting to green:

1. Fake It - return a constant and gradually replace constants with variables until you have the real code.
2. Obvious implementation - type in the real implementation.

When I use TDD in practice, I commonly shift between these two modes of implementation. When everything is going smoothly and I know what to type, I put in obvious implementation after obvious implementation (running the tests all the time to ensure all tests pass). As soon as I get an unexpected red bar, I back up, shift to faking implementations and refactor to the right code. When my confidence is back, I go back to obvious implementations.

There is a third style of test-driving development, triangulation, which we will demonstrate later. To review, we:

1. Translated a design objection (side effects) into a test case that failed because of the objection.
2. Got the code to compile quickly with a stub implementation.
3. Made the test work by typing in what seemed like the right code.

The translation of a feeling (disgust at side effects) into a test (multiply the same dollar twice) is a common theme of TDD. The longer I do this, the better able I am to translate my aesthetic judgements into tests. When I can do this, my design discussions become much more interesting. First we can talk about whether the system should work like this or like that. Once we decide on the correct behavior, we can talk about the best way of achieving that behavior. We can speculate about truth and beauty all we want but while we are programming we can leave airy-fairy discussions behind and talk cases.

Review

Make it Fail
No code without a failing test

Make it Work
As simply as possible

Make it Better
Refactor</Text>
        </Document>
        <Document ID="78">
            <Title>Q &amp; A</Title>
            <Text>1. Can we write another test when we have a failing test?
2. Can we change the target code that is not driven by a test? Any change to the logic of the program should be driven by a failed test.
3. Is minimizing the time in red bar a good idea?
4. Can we change both the test and target code at the same time? Keep a tight feedback loop between the code and tests: don’t let one or the other get too far out in front.
5. What is the start state and end state for a refactoring step?
6. Do we always take very small steps in TDD? Why or Why not?
7. How can you verify the correctness of a test?
8. What should be the first test? Why?
9. Is the order of implementing tests important?
10. How to handle problems due to mocking?
11. How to keep Mocks and real objects in sync? Write contract tests. See Integration tests are scam video.
12. What does it mean when you add a new test and it passes without changing the target code? How can you ensure the correctness of a test?
13. When can we delete tests?
14. How can you verify the correctness of a test, when a test passes without modifying the target code?

Mocking can lead to tests that does not fail when the DoC changes. Refer 022-Test-Isolation-and-Refactoring

What happens when you change production code without changing or adding new tests?

Page 133 - The example code has calculation in the test. Is this not duplicate logic in the test that is supposed to be in production code also?

Clarifications

Why not use record and playback testing for external APIs like credit card processing?</Text>
        </Document>
        <Document ID="92">
            <Title>TDD Patterns</Title>
            <Text>There are some basic strategic question we need to answer before we can talk about the details of how to test:

What do we mean by testing?
When do we test?
How do we choose what logic to test?
How do we choose what data to test?

Isolated Test

How should the running of tests affect each other? Not at all. Tests should be able to ignore each other completely. If I had one test broken, I wanted one problem. If I had two tests broken, I wanted two problems.

One convenient implication of isolated tests is that the tests are order independent. If I want to grab a subset of tests and run them, I can do so without worrying that a test will break now because prerequisite test is gone.

Performance is the usual reason cited for having tests share data. A second implication of isolated tests is that you have to work, sometimes very hard, to break your problem into little orthogonal dimensions, so setting up the environment for each test is easy and quick. Isolating tests encourages you to compose solutions out of many highly cohesive, loosely coupled objects. I never knew exactly how to regularly achieve high cohesion and loose coupling until I started writing isolated tests.

Test List

What should you test? Before you begin, write a list of all the tests you know you will have to write.

The first part of our strategy for dealing with programming stress is to never take a step forward unless we know where our foot is going to land. When we sit down to a programming session, what is it we intend to accomplish?

One strategy for keeping track of what we’re trying to accomplish is to writing down everything. I got in the habit of writing down everything I wanted to accomplish over the next few hours on a slip of paper next to my computer. I had a similar list for weekly or monthly scope pinned on the wall. As soon as I had all that written down, I knew I wasn’t going to forget something. When a new item came up, I would quickly and consciously decide whether it belonged on the now list, the later list or didn’t really need to be done at all.

Applied to test-driven development, what we put on the list are the tests we want to implement. First, put on the list examples of every operation that you know you need to implement. Next, for those operations that don’t already exist, put the null version of that operation on the list. Finally, list all the refactorings that you think you will have to do to have clean code at the end of this session.

Instead of outlining the tests, we could just ago ahead and implement them all. There are a couple of reasons writing tests en masse hasn’t worked for me. First, every test you implement is a bit of inertia when you have to refactor. When you’ve implemented ten tests you are less likely to clean up. Second, if you have ten tests broken, you are a long way from the green bar. If you want to get to green quickly, you have to throw all ten tests away. If you want to get all the tests working, you are going to be at red state for a long time.

The pure form of TDD where you are never more than one change away from a green bar is like the mountain climbing three out of four rule. 

As you make the tests run, the implementation will imply new tests. Write the new tests down on the list. Likewise with refactorings. Put it on the list. We’ll get to it before we check in.

Items that are left on the list when the session is done need to be taken care of. If you are really half way through a piece of functionality, use the same list later. If you have discovered larger refactorings that are out of scope for the moment, move them to the later list. I can’t recall ever moving a test case to the later list. If I can think of a test that might not work, getting it to work is more important than releasing my code.

Test First

When should you write your tests? Before you write the code that is to be tested.

You won’t test after. Your goal as a programmer is running functionality. However, you need a way to think about design, you need a method for scope control.

When we test first, we reduce the stress, which makes us more likely to test. The immediate payoff for testing - a design and scope control tool - suggests that we will be able to start doing it and keep doing it even under moderate stress.

Assert First

When should you write the asserts? Try writing them first.
Where should you start building a system? With stories you want to be able to tell about the finished system.
Where should you start writing a bit of functionality? With the tests you want to pass with the finished code.
Where should you start writing a test? With the asserts that will pass when it is done.

When I test assert-first I find it has a powerful simplifying effect. When you are writing a test, you are solving several problems at once, even if you no longer have to think about the implementation.

Where does the functionality belong? Is it a modification of an existing method, a new method on an existing class, an existing method name implemented in a new place, or a new class?
What should the names be called?
How are you going to check for the right answer?
What is the right answer?
What other tests does this test suggest?

The two problems from the list that can be easily separated from the rest are “what is the right answer?” and “how am I going to check?”

Here’s an example. Suppose we want to communicate with another system over a socket. When we’re done, the socket should be closed and we should have read the string “abc”. 

Test_complete_transaction 
Assert(reader.closed?)
Assert(“abc”, reply.contents)

Where does the reply come from? The socket, of course:
…
Reply = reader.contents
…

And the socket? We create it by connecting to a server:

Reader = Socket.new(“localhost”, DEFAULT_PORT)
…

But before this, we need to open a server:
Writer = Server.new(PORT, “abc”)

Now we may have to adjust the names based on actual usage, but we have created the outlines of the test in teensy tiny steps, informing each decision with feedback within seconds.

Test Data

What data do you use for test-first tests? Use data that makes the tests easy to read and follow.

Don’t scatter data values. If there is a difference in the data, it should be meaningful. If there isn’t a conceptual difference between 1 and 2, use 1.

Test data isn’t a license to stop short of full confidence. If your system has to handle multiple inputs, your tests should reflect multiple inputs. However, don’t have a list of 10 items as the input data if a list of 3 items will lead you to the same design and implementation decisions.

One trick in test data is to try to never use the same constant to mean more than one thing. If I am testing an add method, it is tempting to test 2 + 2, since that is the classic example of addition, or 1 + 1, since that is so simple. What if we got the arguments reversed in the implementation? If we use 2 for the first argument we should use 3 for example for the second.

The alternative to test data is realistic data, where you use data from the real world. Realistic data is useful when:
You are testing real-time systems using traces of external events gathered from actual execution
You are matching the output of the current system with the output of a previous system (Parallel Testing)
You are refactoring a simulation and expect precisely the same answers when you are finished, particularly if floating point accuracy may be a problem.

Evident Data

How do you represent the intent of the data? Include expected and actual results in the test itself and try to make their relationship apparent.</Text>
        </Document>
        <Document ID="85">
            <Title>Money Retrospective</Title>
            <Text> Let’s take a look back at the Money example, both the process we used and the
results. We will look at:

•  What Next?
•  Metaphor—the dramatic effect metaphor has on the structure of the design
•  JUnit Usage—when we ran tests and how we used JUnit
•  Code Metrics—a numerical abstract of the resulting code
•  Process—we say red/green/refactor, but how much work goes into each step?
•  Test Quality—how do TDD tests stack up by conventional test metrics?

What Next?

 Is the code finished? No. There is that nasty duplication between Sum.plus() and
Money.plus(). If we made Expression a class instead of an interface (not the usual
direction, as classes more often become interfaces), we would have a natural home
for the common code.

I don’t believe in “finished”. TDD can be used as a way to strive for perfection, but
that isn’t its most effective use. If you have a big system, the parts that you touch all
the time should be absolutely rock solid, so you can make daily changes confidently.
As you drift out to the periphery of the system, to parts that don’t change
 often, the tests can be spottier and the design uglier without interfering with your
confidence.

When I’ve done all of the obvious tasks, I like running a code critic, like SmallLint
for Smalltalk. Many of the suggestions that come up I already know about, or I disagree
with. Automated critics don’t forget, though, so if I don’t delete an obsolete
implementation I don’t have to stress. The critic will point it out.

Another “what next?” question is, “What additional tests do I need?” Sometimes
you think of a test that “shouldn’t” work, and it does. Then you need to find out
why. Sometimes a test that shouldn’t work really doesn’t, and you can record it as a
known limitation or as work to be done later.
Finally, when the list is empty is a good time to review the design. Do the words
and concepts play together? Is there duplication that is difficult to eliminate given
the current design (lingering duplication is a symptom of latent design.)

Metaphor

 The biggest surprise for me in coding the Money example is how different it came
out this time. I have programmed Money in production at least three times that I
can think of. I have used it as an example in print another half dozen times. I have
programmed it live on stage (relax, it’s not as exciting as it sounds…) another fifteen
times. I coded another three or four times preparing for writing (I ripped out
Section I and rewrote it based on early reviews.) Then, while I was writing this, I
thought of using Expression as the metaphor and the design went in a completely
different direction than it has gone before.

I really didn’t expect the metaphor to be so powerful. A metaphor should just be a
source of names, shouldn’t it? Apparently not.
The metaphor Ward used for “several monies together with potentially different
currencies” was a vector, like a mathematic vector where the coefficients were currencies
instead of x2 . I used MoneySum for a while, then MoneyBag (which is nice
and physical), and finally Wallet (which is commoner in most folks’ experience).
All of these metaphors imply that the collection of Money’s is flat. For example, “2
USD + 5 CHF + 3 USD” would result in “5 USD + 5 CHF”. Two values with the
same currency would be merged.

 The Expression metaphor freed me from a bunch of nasty issues about merging
duplicated currencies. The code came out cleaner and clearer than I’ve ever seen it
before. I’m concerned about the performance of Expressions, but I’m happy to wait
until I see some usage statistics before I start optimizing.

What if I got to rewrite everything I ever wrote 20 times? Would I keep finding
insight and surprise every time? Is there some way to be more mindful as I program
so I can squeeze all the insight out of the first three times? The first time?

Process

The TDD cycle is:

• Write a test
• Make it compile, run it to see it fail
• Make it run
• Remove duplication

 The three items that come up time and again as surprises when teaching TDD are:

•  The three approaches to making a test work cleanly—fake it, triangulate, and
just typing in the right solution to begin with
•  Removing duplication between test and code as a way to drive the design
•  The ability to control the gap between tests to increase traction when the road
gets slippery and cruise faster when conditions are clear</Text>
        </Document>
        <Document ID="68">
            <Title>Equality</Title>
            <Text>If I have an integer and I add 1 to it, I don’t expect the original integer to change. I expect to to use the new value. Objects usually don’t behave that way. We can use objects as values, as we are using our Dollar now. The pattern for this is Value Object. One of the constraints of Value Objects is that the value of the instance variables of the object never change once they have been set in the constructor.

The advantage of using Value Objects is that there is no aliasing problems. Say I have one Check and I set its amount to $5, and then I set another Check’s amount to the same $5. Nastiest bugs can occur when changing the first Check’s value inadvertently change the second Check’s value. This is aliasing.

With value objects there is no aliasing. If I have $5, I am guaranteed that it will always be $5. If someone wants $7, they have to make an entirely new object. One implication of Value Object is all operations must return a new object, as we saw in the previous chapter. Another implication is that value objects should implement equals, since $5 is as good as another.

If you use Dollar as the key to a hash you have to implement hashcode if you implement equals. We’ll put that in the list and get to it when it’s a problem.

Updated To-Do List

$5 + 10 CHF = $10 if rate is 2:1
$5 * 2 = $10
Dollar side-effects?
Money rounding?
equals
Hascode

Thinking Process

Think about how to test equality. First, $5 should equal $5.

Etest_equality
  Assert(Dollar.new(5).equals(Dollar.new(5)))

We get a failing test. The fake implementation is to just return true:

Equals(object)
  Return true
End

You might know that true is really ‘5 == 5’ which is really ‘amount == 5’ which is really ‘amount == dollar.amount’. For the sake of demonstrating the third and most conservative implementation strategy, triangulation, we will stay away from that approach.

If two receiving stations at a known distance from each other can both measure the direction of a radio signal, there is enough information to calculate the range and bearing of the signal. This calculation is called triangulation.

By analogy, when we Triangulate, we only generalize code when we have two or more examples. We briefly ignore the duplication between test and model code. When the second example demands a more general solution, then and only then do we generalize.

So, to triangulate we need a second example. How about $5 != $6?

Assert(Dollar.new(5).equals(Dollar.new(5)))
Assert_false(Dollar.new(5).equals(Dollar.new(5)))

Now we need to generalize equality:

Equals(object)
Return amount == object.amount
End

We could have used triangulation to drive the generalization of times also. If we had $5 x 2 = $10 and $5 x 3 = $15 we would no longer have been able to return a constant.

Use Triangulation when you not sure how to refactor. If you can see how to eliminate duplication between code and tests and create the general solution, just do it. When the design thoughts just aren’t coming, triangulation gives you a chance to think about the problem from a slightly different direction. What axes of variability are you trying to support in your design? Make some of them vary and the answer may become clearer.

So equality is done for the moment. What about comparing with null and comparing with other objects? These are commonly used operations, but not necessary at the moment, so we’ll add them to the list.

Now that we have equality, we can directly compare Dollars to Dollars. That will let us make amount private, which is a good thing. Reviewing the above, we:

1. Noticed that our design pattern (value object) implied an operation
2. Tested for that operation
3. Implemented it simply
4. Didn’t refactor immediately, but instead tested further
5. Refactored to capture the two cases at once

Updated To-Do List

$5 + 10 CHF = $10 if rate is 2:1
$5 * 2 = $10
Dollar side-effects?
Money rounding?
equals
Hascode
Equal nil
Equal object



</Text>
        </Document>
        <Document ID="77">
            <Title>Refactoring times method</Title>
            <Text> When we are done with this chapter we will have a single class to represent Money. The two implementations of times() are close, but not identical.

Franc
Money times(int multiplier) {
return Money.franc(amount * multiplier);
}
Dollar
Money times(int multiplier) {
return Money.dollar(amount * multiplier);
}

 There’s not an obvious way to make them identical. Sometimes you have to go backwards to go forwards, a little like a Rubik’s Cube. What happens if we inline the factory methods? 

Franc
Money times(int multiplier) {
return new Franc(amount * multiplier, "CHF");
}

Dollar
Money times(int multiplier) {
return new Dollar(amount * multiplier, "USD");
}

In Franc, though, we know that the currency instance variable is always “CHF”, so we can write:

Franc
Money times(int multiplier) {
return new Franc(amount * multiplier, currency);
}

That works. The same trick works in Dollar:

Dollar
Money times(int multiplier) {
return new Dollar(amount * multiplier, currency);
}

We’re almost there. Does it really matter whether we have a Franc or a Money? We could carefully reason about this given our knowledge of the system. However, we have clean code and we have tests that give us confidence that the clean code works. Rather than apply minutes of suspect reasoning, we can just ask the computer by making the change and running the tests. In teaching TDD I see this situation all the time—excellent programmers spending 5-10 minutes reasoning about a question that can be answered by the computer in 15 seconds. Without the tests you
have no choice, you have to reason. With the tests you can decide whether an experiment would answer the question faster. Sometimes you should just ask the computer.

To run our experiment we change Franc.times() to return a Money:

Franc
Money times(int multiplier) {
return new Money(amount * multiplier, currency);
}

The compiler tells us that Money must be a concrete class:

Money
class Money
Money times(int amount) {
return null;
}

And we get a red bar. The error message says, “expected:&lt;Money.Franc@31aebf>
but was: &lt;Money.Money@478a43>”. Not as helpful as we would perhaps like. We
can define toString() to give us a better error message:

Money
public String toString() {
return amount + " " + currency;
}

Whoa! Code without a test? Can you do that? We could certainly have written a test
for toString() before we coded it. However:
• We are about to see the results on the screen
• Since toString() is only used for debug output, the risk of it failing is low
• We already have a red bar, and we’d prefer not to write a test when we have a
red bar
Exception noted.

Now the error message says: “expected:&lt;10 CHF> but was:&lt;10 CHF>”. That’s a
little better, but still confusing. We got the right data in the answer, but the class
was wrong—Money instead of Franc. The problem is in our implementation of
equals():

Money
public boolean equals(Object object) {
Money money = (Money) object;
return amount == money.amount
&amp;&amp; getClass().equals(money.getClass());
}

We really should be checking to see that the currencies are the same, not that the
classes are the same.
We’d prefer not to write a test when we have a red bar. However, we are about to
change real model code, and we can’t change model code without a test. The conservative 
course is to back out the change that caused the red bar so we’re back to
green. Then we can change the test for equals(), fix the implementation, and re-try
the original change.
This time, we’ll be conservative.

Franc
Money times(int multiplier) {
return new Franc(amount * multiplier, currency);
}
 That gets us back to green. The situation that we had was a Franc(10, “CHF”) and a
Money(10, “CHF”) that were reported to be not equal, even though we would like
them to be equal. We can use exactly this for our test:
public void testDifferentClassEquality() {
assertTrue(new Money(10, "CHF").equals(new Franc(10, "CHF")));
}
 It fails, as expected. The equals() code should compare currencies, not classes:
Money
public boolean equals(Object object) {
Money money = (Money) object;
return amount == money.amount
&amp;&amp; currency().equals(money.currency());
}
 Now we can return a Money from Franc.times() and still pass the tests:
Franc
Money times(int multiplier) {
return new Money(amount * multiplier, currency);
}
 Will the same will work for Dollar.times()?
Dollar
Money times(int multiplier) {
return new Money(amount * multiplier, currency);
}

 Yes! Now the two implementations are identical, so we can push them up.
Money
Money times(int multiplier) {
return new Money(amount * multiplier, currency);
}
 Multiplication in place, we are ready to eliminate the stupid subclasses. Reviewing,
we:

•  Reconciled two methods (times()) by first inlining the methods they called and then replacing constants with variables
•  Wrote a toString() without a test just to help us debug
•  Tried a change (returning Money instead of Franc) and let the tests tell us whether it worked
•  Backed out an experiment and wrote another test. Making the test work made the experiment work.
</Text>
        </Document>
        <Document ID="86">
            <Title>Set the Table</Title>
            <Text> When you begin writing tests, you will discover a common pattern (Bill Wake
coined the phrase 3A for this):

1.  Arrange—create some objects
2.  Act—stimulate them
3.  Assert—check the results

The arrange step is often the same test-to-test, while the stimulation and checking
steps are unique. I have a 7 and 9. If I add them, I expect 16. If I subtract them, I
expect –2, if I multiply them, I expect 63. The stimulation and expected results are
unique, the 7 and the 9 don’t change.

If this pattern repeats at different scales (and it does), then we’re faced with the
question of how often do we want to create new objects to test. Looking back at our
initial set of constraints, two constraints come into conflict:

•  Performance—we would like our tests to run as quickly as possible, so if we use
similar objects in several tests we would like to create them once for all tests
•  Isolation—we would like the success or failure of one test to be irrelevant to
other tests, and if one test changes the objects, following tests are likely to
change their results

 Test coupling has an obvious nasty effect, where breaking one test causes the next
ten to fail even though the code is correct. Test coupling can have a subtle really
nasty effect, where the order of tests matters. If I run A before B, they both work,
but if I run B before A, then A fails. Or even nastier, the code exercised by B is
wrong, but because A ran first, the test passes.

Test coupling—don’t go there. Let’s assume for the moment we can make object
creation fast enough. In this case, we would like to create the objects for a test every
time the test runs. We’ve already seen a disguised form of this in WasRun, where
we wanted to have a flag set to false before we ran the test. Taking steps towards
this, first we need a test:

TestCaseTest
def testSetUp(self):
test= WasRun("testMethod")
test.run()
assert(test.wasSetUp)

 Running this, (by adding the last line “TestCaseTest("testSetUp").run()” to our file)
Python politely informs us that there is no “wasSetUp” attribute. Of course not.We
haven’t set it. This method should do it.
WasRun
def setUp(self):
self.wasSetUp= 1
 It would if we were calling it. Calling setUp is the job of the TestCase, so we turn
there:
TestCase
def setUp(self):
pass
def run(self):
self.setUp()
exec "self." + self.name + "()"
 That’s two steps to get a test case running, which is too many in such ticklish circumstances.
We’ll see if it will work. Yes, it does pass. However, if you want to
learn something, try to figure out how we could have gotten the test to pass by
changing no more than one method at a time.


101
We can immediately use our new facility to shorten our tests. First, we can simplify
WasRun by setting the wasRun flag in setUp:

WasRun
def setUp(self):
self.wasRun= None
self.wasSetUp= 1

We have to simplify testRunning not to check the flag before running the test. Are
we willing to give up this much confidence in our code? Only if testSetUp is in
place. This is a common pattern—one test can be simple if and only if another test
is in place and running correctly.

TestCaseTest
def testRunning(self):
test= WasRun("testMethod")
test.run()
assert(test.wasRun)

We can also simplify the tests themselves. In both cases we create an instance of
WasRun, exactly that fixture we were talking about earlier.We can create the Was-
Run in setUp, and use it in the test methods. Each test method is run in a clean
instance of TestCaseTest, so there is no way the two tests can be coupled (assuming
the objects don’t interact in some incredibly ugly way, like setting global variables,
but we wouldn’t do that, not with all those other readers watching.)

TestCaseTest
def setUp(self):
self.test= WasRun("testMethod")
def testRunning(self):
self.test.run()
assert(self.test.wasRun)
def testSetUp(self):
self.test.run()
assert(self.test.wasSetUp)

Next we’ll run tearDown() after the test method. Reviewing this chapter, we:

• Decided simplicity of test writing was more important than performance for the
moment
• Tested and implemented setUp(). Used setUp() to simplify the example test case
•  Used setUp() to simplify the test cases checking the example test case</Text>
        </Document>
        <Document ID="79">
            <Title>Deleting Subclasses</Title>
            <Text> The two subclasses, Dollar and Franc, have only their constructors. Only a constructor
is not enough reason to have a subclass, so we want to delete the subclasses.
We can replace references to the subclasses by references to the superclass without
changing the meaning of the code. First Franc:
Franc
static Money franc(int amount) {
return new Money(amount, "CHF");
}
 Then Dollar:
Dollar
static Money dollar(int amount) {
return new Money(amount, "USD");
}
 Now there are no references to Dollar, so we can delete it. Franc still has one reference,
in the test we just wrote.
public void testDifferentClassEquality() {

 assertTrue(new  Money(10, "CHF" ).equals(new  Franc(10, "CHF" )));
}
Is equality covered elsewhere well enough that we can delete this test? Looking at
the other equality test:
public void  testEquality() {
assertTrue(Money.dollar(5).equals(Money.dollar(5)));
assertFalse(Money.dollar(5).equals(Money.dollar(6)));
assertTrue(Money.franc(5).equals(Money.franc(5)));
assertFalse(Money.franc(5).equals(Money.franc(6)));
assertFalse(Money.franc(5).equals(Money.dollar(5)));
}
it looks like we have the cases for equality well covered, too well covered, actually.
We can delete the third and fourth assertions since they duplicate the exercise of the
first and second assertions:
public void  testEquality() {
assertTrue(Money.dollar(5).equals(Money.dollar(5)));
assertFalse(Money.dollar(5).equals(Money.dollar(6)));
assertFalse(Money.franc(5).equals(Money.dollar(5)));
}
The test we wrote forcing us to compare currencies instead of classes only makes
sense if there are multiple classes. Since we are trying to eliminate the Franc class,
a test to ensure that the system works if there is a Franc class is a burden, not a help.
Away testDifferentClassEquality() goes, and Franc goes with it.

Similarly, there are separate tests for dollar and franc multiplication. Looking at the
code, we can see there is no difference in the logic at the moment based on the currency
(there was a difference when there were two classes). We can delete
testFrancMultiplication() without losing any confidence in the behavior of the system.
Single class in place, we are ready to tackle addition. First, to review, we:

• Finished gutting subclasses and deleted them
• Eliminated tests that made sense with the old code structure but were redundant
with the new code structure</Text>
        </Document>
        <Document ID="119">
            <Title>Test Double Patterns</Title>
            <Text>Test Double

How can we verify logic independently when code it depends on is unusable? How can we avoid slow tests? We replace a component on which the SUT depends with a test specific equivalent.

Fig ..

Sometimes it is hard to test the SUT because it depends on other components that cannot be used in the test environment. Such a situation may arise because those components are not available, they will not return the results needed for the test, or executing them would have undesirable side effects. In other cases, our test strategy requires us to have more control over or visibility of the internal behavior of the SUT.

When we are writing a test in which we cannot (or choose not to) use a real DoC, we can replace it with a test double. The test double does not have to behave exactly like the real DoC; it merely has to provide the same API as the real DoC so that the SUT thinks it is the real one.

During the fixture setup phase of our Four-Phase Test, we replace the real DoC with our test double. Depending on the kind of test we are executing, we may hard-code the behavior of the test double or we may configure it during the setup phase. When the SUT interacts with the test double, it won’t be aware that it isn’t taking to the real object, but we will have achieved our goal of making impossible tests possible. 

Regardless of which variation of test double we choose to use, we must keep in mind that we don’t need to implement the entire interface of the DoC. Instead, we provide only the functionality needed for our particular test. We can even build different test doubles for different tests that involve the same DoC.

We can use test double in the following cases:

If we have an untested requirement because neither the SUT nor its DoCs provide an observation point for the SUT’s indirect output that we need to verify using behavior verification.
If we have untested code and a DoC does not provide the control point to allow us to exercise the SUT with the necessary indirect inputs.
If we have slow tests and we want to be able to run our tests more quickly and hence more often.

We have to be careful when using test doubles because we are testing the SUT in a different configuration from the one that will be used in production. For this reason, we really should have at least one test that verifies the SUT works without a test double. We need to be careful that we don’t replace the parts of the SUT that we are trying to verify. Also excessive use of test doubles can result in fragile tests as a result of over specified software.

Test Double   - Verify 
------------------------------------
Test Stub       - Indirect Inputs
Test Spy        - Indirect Outputs
Mock Object  - Indirect Outputs 
Fake Object   - Alternative Implementation

Test Stub

We use a test stub to replace a real object on which the SUT depends so that the test has a control point for the indirect inputs of the SUT. Its inclusion allows the test to force the SUT down paths it might not otherwise execute. We can further classify test stubs by the kind of indirect inputs they are used to inject into the SUT. A responder injects valid values, while a saboteur injects error or exceptions.

Test Spy

A more capable version of test stub. It is an observation point for the indirect outputs of the SUT. Like a test stub it may need to provide values to the SUT in response to method calls, it also captures the indirect outputs of the SUT as it is exercised and saves them for later verification by the test. It is just a test stub with some recording capability. It is used for the same purpose as a mock object, the style of test we writing using a test spy looks much more like a test written with a test stub.

Mock Object

We can use a mock object as an observation point to verify the indirect outputs of the SUT as it is exercised. Typically, the mock object also includes the functionality of a test stub in that it must return values to the SUT if it hasn’t already failed the tests but the emphasis is on the verification of the indirect outputs. Therefore, a mock object is a lot more than just a test stub plus assertions.

Fake Object

We use a fake object to replace the functionality of a real DoC in a test for reasons other than verification of indirect inputs and outputs of the SUT. Typically, a fake object implements the same functionality as the real DoC but in a simpler way. While a fake object is typically built specifically for testing, the test does not use it as either a control point or an observation point.

The most common reason for using a fake object is that the real DoC is not available yet, is too slow, or cannot be used in the test environment because of side effects.

Considerations when building a test double:
Whether it should be specific to a single test or reusable 
Whether it should exist in code or generated on-the-fly
How we tell the SUT to use the test double (installation)

Fig 23.2 Types of test doubles with implementation choices. Only test stubs, test spies and mock objects need to be hard-coded or configured by the test. Dummy objects have no implementation; Fake objects are installed but not controlled by the test.

Unconfigurable Test Doubles

Neither dummy objects nor fake objects need to be configured, each for their own reason. Dummies should never be used by the receiver so they need no real implementation. Fake objects need a real implementation but one that is simpler than the object that they replace. Therefore, neither the test nor the developer will need to configure canned responses or expectations; we just install the test double and let the SUT use it as if it were real.

Test Stub

How can we verify logic independently when it depends on indirect inputs from other software components? We replace a real object with a test specific object that feeds the desired indirect inputs into the SUT.

Fig..

In many cases, the environment or context in which the SUT operates very much influences the behavior of the SUT. To get adequate control over the indirect inputs of the SUT, we may have to replace some of the context with something we can control - namely, a test stub.

First, we define a test specific implementation of an interface on which SUT depends. This implementation is configured to respond to calls from the SUT with the values (or exceptions) that will exercise the untested code within the SUT. Before exercising the SUT, we install the test stub so that the SUT uses it instead of the real implementation. When called by the SUT during test execution, the test stub returns previously defined values. The test can then verify the expected outcome in the normal way.

A key indication for using a test stub is having untested code caused by our inability to control the indirect inputs of the SUT. We can use a test stub as a control point that allows us to control the behavior of the SUT with various indirect inputs and we have no need to verify the indirect outputs. We can also use test stub to inject values that allow us to get past a particular point in the software where the SUT calls software that is unavailable in our test environment.

If we do need an observation point that allows us to verify the indirect outputs of the SUT, we should consider using a mock object or a test spy.

Responder

A test stub that is used to inject valid indirect inputs into the SUT so that it can go about its business is called a responder. They are commonly used in happy path testing when the real component is uncontrollable, is not yet available, or is unusable in the development environment. The tests will invariably be simple success tests.

Saboteur

A test stub that is used to inject invalid indirect inputs into the SUT is called a saboteur because its purpose is to derails whatever the SUT is trying to do so that we can see how the SUT copes under these circumstances. The derailment might be caused by returning unexpected values or objects or it might result from raising an exception or causing a runtime error. Each test may be either a simple success test or an expected exception test depending on how the SUT is expected to behave in response to the indirect input.

Temporary Test Stub

A temporary test stub stands in for a DoC that is not yet available. This kind of test stub typically consists of an empty shell of a real class with hard-coded return statements. As soon as the real DoC is available, it replaces the temporary test stub. TDD often requires us to create temporary test stubs as we write code from outside in; these shells evolve into the real classes as we add code to them. In need-driven development, we tend to use mock objects because we want to verify that the SUT calls the right methods on the temporary test stub; in addition, we typically continue using the mock object even after the real DoC becomes available.

Entity Chain Snipping

This is a special case of a responder that is used to replace a complex network of objects with a single test stub that pretends to be the network of objects. Its inclusion can make fixture setup go much more quickly (especially when the objects would normally have to be persisted into a database) and can make the tests much easier to understand.

We must be careful when using test stubs because we are testing the SUT in a different configuration from the one that will be used in production. We really should have at least one test that verifies the SUT works without a test stub. A common mistake made by developers who are new to stubs is to replace a part of the SUT that they are trying to test. For this reason, it is important to be really clear about what it playing the role of SUT and what is playing the role of test fixture. Also note that excessive use of test stubs can result in over specified software.

Hard Coded Test Stub

A hard coded test stub has its responses hard-coded within its program logic. These test stubs tend to be purpose built for a single test or a very small number of tests.

Configurable Test Stub

A test configures the configurable as part of its fixture setup phase. We avoid building a different hard coded test stub for each test.

We can achieve proper verification of the indirect inputs by getting control of the time. To do so, we can use the replace dependency with test double refactoring to replace the real system clock with a virtual clock. We then implement it as a test stub that is configured by the test with the time we want to use as the indirect input to the SUT.

Responder example: The following test verifies one of the happy path test conditions using a responder to get control over the indirect inputs of the SUT. Based on the time injected into the SUT, the expected result can be hard-coded safely.

Entity Chain Snipping example: In this example, we are testing the invoice but require a customer to instantiate the invoice. The customer requires an address, which in turn requires a city. Thus we find ourselves creating numerous additional objects just to set up the fixture. Suppose the behavior of the invoice depends on some attribute of the customer that is calculated from the address by calling the method get_zone on the customer:

Code…

In this test, we want to verify only the behavior of the invoice logic that depends on this zone attribute - not the way this attribute is calculated from the customer’s address. (There are separate customer unit tests to verify the zone is calculated correctly). All of the setup of the address, city and other crap merely distracts the reader.

Here’s the same test using a test stub instead of the customer. Note how much simpler the fixture setup has become as a result of entity chain snipping:

Code. ..

We have used mock to stub out the customer with a customer_stub that returns ZONE_3 when get_zone is called. This is all we need to verify the invoice behavior, and we have manage to get rid of all that distracting extra object construction. It is also much clearer from reading this test that invoicing behavior depends only on the value returned by get_zone and not any other attributes of the customer or address.

Test Spy

How do we implement behavior verification? How can we verify logic independently when it has indirect outputs to other software components?
We use a test double to capture the indirect output calls made to another component by the SUT for later verification by the test.

Fig

In many cases, the environment or context in which the SUT operates very much influences the behavior of the SUT. To get adequate visibility of the indirect outputs of the SUT, we may have to replace some of the context with something we can use to capture these outputs of the SUT. Use of a test spy is a simple and intuitive way to implement behavior verification via an observation point that exposes the indirect outputs of the SUT so they can be verified.

Before we exercise the SUT, we install a test spy as a stand-in for a DoC used by the SUT. The test spy is designed to act as an observation point by recording the method calls made to it by the SUT as it is exercised. During the result verification phase, the test compares the actual values passed to the test spy by the SUT with the values expected by the test.

A key indication for using a test spy is having an untested requirement caused by an inability to observe the side effects of invoking methods on the SUT. Test spies are a natural and intuitive way to extend the existing tests to cover these indirect outputs because the calls to the assertion methods are invoked by the test after the SUT has been exercised just like in normal tests. The test spy merely acts as the observation point that gives the test method access to the values recorded during the SUT execution. 

We should use a test spy in the following cases:

We are verifying the indirect outputs of the SUT and we cannot predict the values of all attributes of the interactions with the SUT ahead of time. 
We want the assertions to be visible in the test and we don’t think the way in which the mock object expectations are established is sufficiently intent-revealing
Out test requires test specific equality and tools do not give us control over the assertion methods being called.
We would like to have access to all the outgoing calls of the SUT before making any assertions on them.

If none of these criteria apply, we may want to consider using a mock object. If we are trying to address untested code by controlling the indirect inputs of the SUT, a simple test stub may be all we need.

Unlike a mock object, a test spy does not fail the test at the first deviation from the expected behavior. Thus our tests will be able to include more detailed diagnostic information in the assertion message based on information gathered after a mock object would have failed the test. 

The key characteristic in how a test uses a test spy relates to the fact that assertions are made from within the test method.

Mock Object

How do we implement behavior verification for indirect outputs of the SUT? How can we verify logic independently when it depends on indirect inputs from other software components?

We replace an object on which the SUT depends on with a test specific object that verifies it is being used correctly by the SUT.

Fig

In many cases, the environment or context in which the SUT operates very much influences the behavior of the SUT. In other cases, we must peer inside the SUT to determine whether the expected behavior has occurred.

Technically, the SUT is whatever software we are testing and doesn’t include anything it depends on; thus inside is somewhat of a misnomer. It is better to think of the DoC that is the destination of the indirect outputs as behind the SUT and part of the fixture.

A mock object is a powerful way to implement behavior verification while avoiding test code duplication between similar tests. It works by delegating the job of verifying the indirect outputs of the SUT entirely to a test double.

First we define a mock object that implements the same interface as an object on which the SUT depends. Then, during the test, we configure the mock object with the values with which it should respond to the SUT and the method calls (complete with expected arguments) it should expect from the SUT. Before exercising the SUT, we install the mock object so that the SUT uses it instead of the real implementation. When called during SUT execution, the mock object compares the actual arguments received with the expected arguments using equality assertions and fails the test if they don’t match. The test need not make any assertions at all.

We can use a mock object as an observation point when we need to do behavior verification to avoid having an untested requirement caused by our inability to observe the side effects of invoking methods on the SUT. This pattern is commonly used during endoscopic testing or need driven development. Although we don’t need to use a mock object when we are doing state verification, we might use a test stub or fake object. 

To use a mock object, we must be able to predict the values of most or all arguments of the method calls before we exercise the SUT.

The standard Four-Phase Test is altered when we use mock objects. The fixture setup phase of the test is broken down into three specific activities and the result verification phase more or less disappears, except for the possible presence of a call to the final verification method at the end of the test.

Fixture setup:
Test constructs mock object
Test configures mock object. 
Test installs mock object into sut
Exercise SUT:
SUT calls mock object; Mock object does assertions
Result Verification:
Test calls final verification method
Fixture teardown:
No impact

Construction
As part of the fixture setup phase of our Four-Phase Test, we must construct the mock object that we will use to replace the substitutable dependency.
Configuration with Expected Values
We need to configure the mock object with the expected method calls (and their parameters) as well as the values to be returned by any functions.
Installation
Dependency inject via a constructor.
Usage
When the SUT calls the methods of the mock object, these methods compare the method call with the expectations. If the method call is unexpected or the arguments are incorrect, the assertion fails the test immediately.
Final verification
Most of the result verification occurs inside the mock object as it is called by the SUT.

Fake Object

How can we verify logic independently when depended on objects cannot be used? How can we avoid slow tests?
We replace the component that the SUT depends on with a much lighter-weight implementation

Fig…

The SUT often depends on other components or systems. Although the interactions with these other components may be necessary, the side effects of these interactions as implemented by the real DoC may be unnecessary or even detrimental. 

A fake object is a much simpler and light-weigh implementation of the functionality provided by the DoC without the side effects we choose to do without.

We acquire or build a very lightweight implementation of the same functionality as provided by a component on which the SUT depends and instruct the SUT to use it instead of the real DoC. This implementation need not have any of the “ilities” that the real DoC needs to have (such as scalability); it need s to provide only the equivalent services to the SUT so that the SUT remains unaware it isn’t using the real DoC.

A fake object is a kind of test double that is similar to a test stub in many ways, including the need to install into the SUT a substitutable dependency. Whereas a test stub acts as a control point to inject indirect inputs into the SUT, however the fake object does not: IT merely provides a way for the interactions to occur in a self-consistent manner. These interactions (i.e., between the SUT and the fake object) will typically be many, and the values passed in as arguments of earlier method calls will often be returned as results of later method calls. Contrast this behavior with that of test stubs and mock objects where the responses are either hard-coded or configured by the test. There is less fear of over specified software if you depend only on interface between the SUT and DoC.

We should use a fake object whenever the SUT depends on other components that are unavailable or that make testing difficult or slow and the tests need more complex sequences of behavior than are worth implementing in a test stub or mock object. It must also be easier to create a lightweight implementation than to build and program suitable mock objects at least in the long run, if building a fake object is to be worthwhile.

Using a fake object helps us avoid over specified software because we do not encode the exact calling sequences expected of the DoC within the test. The SUT can vary how many times the methods of the DoC are called without causing tests to fail. 

If we need to control the indirect inputs or verify the indirect outputs of the SUT, we should probably use a mock object or test stub instead.

Fake Database

Replace the database with a set of in-memory hash tables.

In-Memory Database

Fake Web Service

When testing software that depends on other components that are accessed via Web services, we can build a small hard-coded or data driven implementation that can be used instead of the real Web service to make our tests more robust and to avoid having to create a test instance of the real Web service in our development environment.

Fake Service Layer

When testing user interfaces, we can avoid data sensitivity and behavior sensitivity of the tests by replacing the component that implements the service layer (including the domain layer) of our application with a fake object that returns remembered or data-driven results. This approach allows us to focus on testing the user interface without having to worry about the data being returned changing over time.

Introducing a fake object involves two basic concerns:
Building the fake object implementation
Installing the fake object

Building the Fake Object

Often, the fake object is used to replace a real implementation that suffers from latency issues owing to real messaging or disk I/O with a much lighter in-memory implementation.

A popular strategy is to start by building a fake object to support a specific set of test where the SUT requires only a subset of the DoC’s services. If this proves successful, we may consider expanding the fake object to handle additional tests. Over time, we may find that we can run all of our tests using the fake object.

One notable difference is that we do not need to configure the fake object with expectations or return values.

Hard Coded Test Double

How do we tell a test double what to return or expect? We build the test double by hard coding the return values and / or expected results.

Fig…

When the test double is very simple or very specific to a single test, the simplest solution is often to hard code the behavior into the test double.

The developer hard codes all of the test double’s behavior into the test double. For example, if the test double needs to return a value for a method call, the value is hard coded into the return statement. If it needs to verify that a certain parameter had a specific value, the assertion is hard-coded with the value that is expected.

We typically use a hard coded test double when the behavior of the test double is very simple or is very specific to a single test or test case class. In ruby using hard-coded test doubles can actually make the tests easier to read.

Test Double Subclass

We can also implement the hard coded test double by subclassing a real DoC and over riding the behavior of the methods we expect the SUT to call as we exercise it. Unfortunately, this approach can have unpredictable consequences if the SUT calls other DoC methods that we have not over ridden. It also ties our test code very closely to the implementation of the DoC and can result in over specified software. Using a test double subclass may be a reasonable option in very specific cases (while doing a spike or when it is the only option available to us), but this strategy isn’t recommended on a routine basis.

Self Shunt

Test Specific Subclass

How can we make code testable when we need to access private state of the SUT?


</Text>
        </Document>
        <Document ID="126">
            <Title>Handling Failure</Title>
            <Text>Constructing a real Chat is painful. Most of the mocking frameworks support creating a mock class, but it makes us uncomfortable because then we’re defining a relationship with an implementation, not a role - we’re being too precise about our dependencies.

Our alternative approach is to attach another object to the object that implements this disconnection policy. We’re emphasizing the single responsibility principle, which means each object does just one thing well and the system behavior comes from how we assemble those objects.

This new check only reassures us that we’ve fed a message through the system and into some kind of log record - it tells us that the pieces fit together. We’ll write a more thorough test of the contents of a log record later. The end-to-end test now fails because, of course, there’s no log file to read.

We already wrote that we don’t like to mock classes. How come we’re doing it here? What we care about in this test is the rendering of the values into a failure message with a severity level. The class is very limited, just a shim above the logging layer, so we don’t think it’s worth introducing another level of indirection to define the logging role. As we wrote before, we also don’t think it worth running against a real file since that introduces dependencies (and even worse, asynchrony) not really relevant to the functionality we are developing. We also believe logging API is unlikely to change.

So, just this once, we mock the Logger class. We would not do this for a class that is internal to our code, because then we would be able to write an interface to describe the role it’s playing.

Inverse Salami Development

We hope that by now you’re getting a sense of the rhythm of incrementally growing software, adding functionality in thin but coherent slices. For each new feature, write some tests that show what it should do, work through each of those tests changing just enough code to make it pass, restructure the code as needed either to open up space for new functionality or to reveal new concepts - then ship it.

The skill is in learning how to divide requirements up into incremental slices, always having something working, always adding just one more feature. The process should feel relentless - it just keeps moving. To make this work, we have to understand how to change the code incrementally and, critically, keep the code well structured so that we can take it wherever we need to go. This is why the refactoring part of a TDD cycle is so critical - we always get into trouble when we don’t keep up that side of the bargain.

Small Methods to Express Intent

Our aim is to do what we can to make each level of code as readable and self-explanatory as possible. 

</Text>
        </Document>
        <Document ID="93">
            <Title>Red Bar Patterns</Title>
            <Text>These patterns are about when you write tests, where you write tests and when you stop writing tests.

One Step Test

Which test should you pick next from the list? Pick a test that will teach you something and that you are confident you can implement.

Each test should represent one step towards your overall goal. If we are looking at the following test list, which test should we pick next?

Plus
Minus
Times
Divide
Plus like
Equals
Equals null
Null exchange
Exchange one currency
Exchange two currencies
Cross rate

There is no right answer. What is one step for less domain expertise will be one tenth for someone with more domain knowledge.

When I look at a test list, I think, “That’s obvious, that’s obvious, I have no idea, obvious, what was I thinking about with that one, ah, this one I can do. That last test is the test I implement next. It didn’t strike me as obvious, but I’m also confident I can make it work.

If you don’t find any test on the list that represents one step, add some new tests that would represent progress towards the items there.

A program grown from tests like this can appear to be written top-down, because you can begin with a test that represents a simple case of the entire computation. A program grown from tests can also appear to be written bottom-up, because you start with small pieces and aggregate them larger and larger.

Programs change over time. Growth implies a kind of self-similar feedback loop where the environment affects the program and the program affects the environment. Second, known-to-known is a helpful description. It implies that we have some knowledge and experience on which to draw, and that we expect to learn in the course of development. Put these two together and we have programs growing from known to unknown.

Starter Test

Which test should you start with? Start by testing a variant of an operation that doesn’t do anything.

The first question you have to ask with a new operation is “Where does it belong?”. Until you’ve answered this question, you don’t know what to type for the test. In the spirit of solving one problem at a time, how can we answer just this question and no other?

If you write a realistic test first, you find yourself solving a bunch of problems at once:
Where does the operation belong?
What are the correct inputs?
What is the correct output given those inputs?

Beginning with a realistic test will leave you too long without feedback. Red/green/refactor, red/green/refactor. You want that loop to be minutes.

You can shorten the loop by choosing inputs and outputs that are trivially easy to discover. For example, let’s consider how to write a polygon reducer test-first. The input is a mesh of polygons and the output is a mesh of polygons that describes precisely the same surface, but with the fewest possible polygons. “How can I test-drive this problem since getting a test to work requires reading Ph.D. Theses?

Starter Test provides an answer:
The output should be the same as the input. Some configurations of polygons are already normalized, incapable of further reduction.
The input should be as small as possible, like a single polygon, or even an empty list of polygons.

My starter test looked like this:

Reducer = Reducer.new(Polygon.new())
Assert(0, reducer.result.npoints)

Boom! First test is running. Now for all the rest of the tests on the list…

One Step Test applies. Pick a starter test that will teach you something but that you are certain you can get working quickly. If you are implementing something for the nth time, pick a test that will require an operation or two. You will be justifiably confident you can get it working.

If find that my starter test is often at a higher level, more like an application test than the following tests. 

Explanation Test

How do you spread the use of automated testing? Ask for and give explanation in terms of tests.

You can do this at higher levels of abstraction. If someone is explaining a sequence diagram to you, you can ask for permission to convert it to a more familiar notation. Then you type in a test case that contains all the externally visible objects and messages in the system.

Learning Test

When do you write tests for externally produced software? Before the first time you are going to use it.

Write a little test that verifies that the API works as expected. If our understanding of the API is correct, the test will pass first time. When new releases arrive, the tests were first run (and fixed if necessary). If the tests didn’t run, there was no sense running the application because it certainly wouldn’t run. Once the tests run, the application will run.

Another Test

How do you keep a technical discussion from straying off topic? When a tangential idea arises, add a test to the list and go back to the topic.

New ideas are greeted with respect but not allowed to diver attention. I write them down on the list, and get back to what I was working on.

Regression Test

What’s the first thing you do when a defect is reported? Write a smallest possible test that fails, and that once it runs, the defect will be repaired.

Regression tests are test that with perfect foreknowledge, you would have written when coding originally. Every time you have to write a regression test, think about how you could have known to write the test in the first place.

You will gain value by testing at the level of the whole application. Regression tests for the application give your users a chance to speak concretely to you about what is wrong and what they expect. Regression tests at smaller scale are a way for you to improve your testing. Add to your test list the lesson learned.

You may have to refactor the system before you can easily isolate the defect. The defect in this case is your system’s way of telling you, “You aren’t quite done designing me yet.”

Break

What do you do when you feel tired or stuck? Take a break.

Do Over

What do you do when you are feeling lost? Throw away the code and start over.

Cheap Desk, Nice Chair

What physical setup should you use for TDD? Get a really nice chair, skimping on the rest.</Text>
        </Document>
        <Document ID="87">
            <Title>Cleaning Up After</Title>
            <Text> Tests will sometimes need to allocate external resources in setUp(). If we want the
tests to remain independent, a test that allocates external resources should release
them before it is done, perhaps in a tearDown() method.

The simple minded way to write the test for deallocation is to introduce yet another
flag. All those flags are starting to bug me, and they are missing an important aspect
of the methods—setUp() is called before the test method is run, and tearDown() is
called afterwards. I’m going to change the testing strategy to keep a little log of
what methods are called. By always appending to the log, we will preserve the
order in which the methods are called.

WasRun
def setUp(self):
self.wasRun= None
self.wasSetUp= 1
self.log= "setUp "

 Now we can change testSetUp() to look at the log instead of the flag:
TestCaseTest

def testSetUp(self):
self.test.run()
assert("setUp " == self.test.log)

 Now we can delete the wasSetUp flag. We can record the running of the test
method, too:

WasRun
def testMethod(self):
self.wasRun= 1
self.log= self.log + "testMethod "
 This breaks testSetUp, because the actual log contains “setUp testMethod ”. We
change the expected value:

TestCaseTest
def testSetUp(self):
self.test.run()
assert("setUp testMethod " == self.test.log)
 Now this test is doing the work of both tests, so we can delete testRunning and
rename testSetUp:

TestCaseTest
def setUp(self):
self.test= WasRun("testMethod")
def testTemplateMethod(self):
self.test.run()
assert("setUp testMethod " == self.test.log)
 Unfortunately, we are only using the instance if WasRun in one place, so we have
to undo our clever setUp hack:

TestCaseTest
def testTemplateMethod(self):
test= WasRun("testMethod")
test.run()
assert("setUp testMethod " == test.log)
 (Doing a refactoring based on a couple of early uses, then having to undo it soon
after is fairly common. Some folks wait until they have three or four uses before
refactoring because they don’t like undoing work. I prefer to spend my thinking
cycles on design, so I just reflexively do the refactorings without worrying about
whether I will have to undo them immediately afterward.)

 Now we are ready to implement tearDown(). Got you! We are ready to test for tear-
Down:
TestCaseTest
def testTemplateMethod(self):
test= WasRun("testMethod")
test.run()
assert("setUp testMethod tearDown " == test.log)
 This fails. Making it work is simple:
TestCase
def run(self, result):
result.testStarted()
self.setUp()
exec "self." + self.name + "()"
self.tearDown()
WasRun
def setUp(self):
self.log= "setUp "
def testMethod(self):
self.log= self.log + "testMethod "
def tearDown(self):
self.log= self.log + "tearDown "
 Surprisingly, we get an error, not in WasRun, but in the TestCaseTest. We don’t
have a no-op implementation of tearDown() in TestCase:
TestCase
def tearDown(self):
pass
 This time we got value out of using the same testing framework we are developing.
Yippee…
No refactoring necessary. The Obvious Implementation, after that one glitch,
worked and was clean.
Next we’ll go on to report the results of running a test explicitly, instead of letting
Python’s native error handling and reporting system tell us when there is a problem
with an assertion. Reviewing, in this chapter we:
 Restructured the testing strategy from flags to a log

•  Tested and implemented tearDown() using the new log
•  Found a problem and, daringly, fixed it instead of backing up</Text>
        </Document>
        <Document ID="94">
            <Title>Testing Patterns</Title>
            <Text>Child Test

How do you get a test case running that turns out to be too big? Write a smaller test case that represents the broken part of the bigger test case. Get the smaller test case running. Reintroduce the larger test case.

The red/green/refactor rhythm is so important for continuous success that when you are at risk of losing it, it is work extra effort to maintain it. This happens when the test accidentally requires several changes to make it work. Minimize the time in red bar.

When I write a test that is too big, I first try to learn the lesson. Why was it too big? What could I have done differently that would have made it smaller? How am I feeling right now?

Delete or x the offending test and start over. 

Mock Object

How do you test an object that relies on on expensive or complicated resource? Create a fake version of the resource that answers constants. Mocks improve performance, reliability and readability. 

If you want to use Mock Objects, you can’t easily store expensive resources in global variables. If you do, you will have to set the global to a Mock Object, run the test, and be sure to reset the global when you are done.

Mocks will encourage you down the path of carefully considering the visibility of every object, reducing the coupling in your designs.

Mock Objects add a risk to the project - what if the mock doesn’t behave like the real object? You can reduce this risk by having a set of tests for the mock that can also be applied to the real object when it becomes available.

Self Shunt

How do you test that one object communicates correctly with another? Have the object under test communicate with the test case instead of with the object it expects.

Tests written with Self Shunt read better.

Log String

How do you test that the sequence in which messages are called is correct?

Crash Test Dummy

How do you test error code that is unlikely to be invoked? Invoke it anyway with a special object that throws an exception instead of doing real work. Examples: File system is full, Out of memory error, network connectivity problem etc.

Simulate the real world problems by using crash test dummy. A crash test dummy is like a mock object, except you don’t need to mock up the whole object. You can override just the one method you want.

Broken Test

How do you leave a programming session when you’re programming alone? Leave the last test broken. When you come back to the code, you have an obvious place to start you have a concrete bookmark to help you remember what you were thinking and making that test work should be quick work, so you’ll quickly get your feet back on that victory road.

Clean Check-In

How do you leave a programming session when you’re programming in a team? Leave all the tests running.

</Text>
        </Document>
        <Document ID="127">
            <Title>Sustainable TDD</Title>
            <Text>This part discusses the qualities we look for in test code that keep the development habitable. We want to make sure the tests pull their weigh by making them expressive, so that we can tell what’s important when we read them and when they fail, and by making sure they don’t become a maintenance drag themselves. We need to apply as much care and attention to the tests as we do to the production code, although the coding styles may differ. Difficulty in testing might imply that we need to change our test code, but often it’s a hint that our design ideas are wrong and that we ought to change the production code.

TDD combines testing, specification and design into one holistic activity.

Listening to the Tests

Sometimes we find it difficult to write a test for some functionality we want to add to our code. In our experience, this usually means that our design can be improved - perhaps the class is too tightly coupled to its environment or does not have clear responsibilities. When this happens, we first check whether it’s an opportunity to improve our code, before working around the design by making the test more complicated or using more sophisticated tools. The qualities that make an object easy to test also make our code responsive to change. 

The trick is to let our tests drive our design. TDD is about testing code, verifying its externally visible qualities such as functionality and performance. TDD is also about feedback on the code’s internal qualities. The coupling and cohesion of its classes, dependencies that are explicit or hidden, and effective information hiding - the qualities that keep the code maintainable.

With practice, we’ve become more sensitive to the rough edges in our tests, so we can use them for rapid feedback about the design. Now when we find a feature that’s difficult to test, we don’t just ask ourselves how to test it, but also why is it difficult to test.

I Need to Mock an Object I Can’t Replace (without Magic)

Many systems are impossible to test because the developers did not isolate the concept of time. We want to know about this dependency.

With two objects, we can make sure that each behavior (date checking and request processing) is unit-tested cleanly.

Implicit Dependencies are Still Dependencies

One goal of object orientation as a technique for structuring code is to make the boundaries of an object clearly visible. An object should only deal with values and instances that are either local - created and managed within its scope - or passed in explicitly.

Use the Same Techniques to Break Dependencies in Unit Tests as in Production Code

Notification Rather Than Logging

The noise in the test reminds us that our code is working at two levels: our domain and the logging infrastructure. 

Here’s a common example of code with logging:

Notice the shift in vocabulary and style between the functional part of the loop and the emphasized logging part. The code is doing two things at once - something to do with locations and rendering support information - which breaks the single responsibility principle. Maybe we could do this instead:

Where the support object might be implemented by a logger, a message bus, pop-up windows, or whatever’s appropriate; this detail is not relevant to the code at this level.

This code is also easier to test. We, not the logging framework, own the support object, so we can pass in a mock implementation at our convenience and keep it local to the test case. The other simplification is that now we’re testing for objects, rather than formatted contents of a string. Of course, we will still need to write an implementation of support and some focused integration tests to go with it.

The idea of encapsulating support reporting sounds like over-design. It means we’re writing code in terms of our intent (helping the support people) rather than implementation (logging), so it’s more expressive. All the support reporting is handled in a few known places, so it’s easier to be consistent about how things are reported and to encourage reuse. It can also help us structure and control our reporting in terms of the application domain, rather than in terms of specific programming language terms. 

Mocking Concrete Classes

One approach to interaction testing is to mock concrete classes rather than interfaces. The technique is to inherit from the class you want to mock and override the methods that will be called within the test, either manually or with any mocking framework. This technique should be used only when you have no other options.

Our intention in TDD is to use mock objects to bring out the relationships between objects. If we subclass, there’s nothing in the domain code to make such a relationship visible - just methods on an object. This makes it harder to see if the service that supports this relationship might be relevant elsewhere, and we’ll have to do the analysis again next time we work with the class. 

There’s a more subtle but powerful reason for not mocking concrete classes. When we extract an interface as part of our TDD process, we have to think up a name to describe the relationship we’ve just discovered. We find that this makes us think harder about the domain and teases out concepts that we might otherwise miss. Once something has a name, we can talk about it.

Break Glass in Case of Emergency

There are a few occasions when we can ignore this smell. The least unacceptable situation is where we’re working with legacy code that we control but can’t change all at once. Alternatively, we might be working with third-party code that we can’t change at all. It’s almost always better to write a veneer over an external library rather than mock it directly - but occasionally, it’s just not worth it.

Above all, do not override a class internal features - this locks down your test to the quirks of the current implementation. Only override visible methods. This rule also prohibits exposing internal methods just to override them in a test. If you can’t get to the structure you need, then the tests are telling you that it’s time to break up the class into smaller, composable features.

Don’t Mock Values

There’s no point in writing mocks for values (which should be immutable). Just create an instance and use it. There are a couple of heuristics for when a class is likely to be a value and so not worth mocking. First, its values are immutable - although that might also mean that it’s an adjustment object. Second we cannot think of a meaningful name for a class that would implement an interface for the type. If you’re tempted to mock a value because it’s too complicated to set up an instance, consider writing a builder.

Bloated Constructor

By adding the object’s dependencies one at time, we end up with unwieldy list of arguments in the constructor. The process helped us sort out the design of the class and its neighbors, but now it’s time to clean up. We will still need the functionality that depends on all the current constructor arguments, so we should see if there’s any implicit structure there that we can tease out. Some of the arguments together define a concept that should be packaged up and replaced with a new object to represent it.

Confused Object

Another diagnosis for a bloated constructor might be that the object is too large because it has too many responsibilities. An associated smell for this kind of class is that its test suite will look confused too. The tests for its various features will have no relationship with each other, so we’ll be able to make major changes in one area without touching others. If we can break up the test class into slices that don’t share anything, it might be best to go ahead and slice up the object too.

Too Many Dependencies

When a constructor is too large, and we don’t believe there’s an implicit new type amongst the arguments, we can use more default values and only overwrite them for particular test cases.

Too Many Expectations

When a test has too many expectations, it’s hard to see what’s important and what’s really under test. What makes the test hard to read is that everything is an expectation, so everything looks equally important. We can’t tell what’s significant and what’s just there to get through the test.

We can make our intentions clearer by distinguishing between stubs, simulations of real behavior that help us get the test to pass, and expectations, assertions we want to make about how an object interacts with its neighbors. Be explicit about how we expect the object to change the world around it.

Write Few Expectations

Avoid too many assertions in a test. Avoid too many expectations. If we have more than a few, then either we’re trying to test too large a unit, or we’re locking down too many of the object’s interactions. Pull out a chain of objects to get to the case object, exposing dependencies that aren’t relevant here. Instead, we should have told the nearest object to do the work for us.

What the Tests Will Tell Us

Keep the knowledge local. Needing magic to create mocks, are to do with knowledge leaking between components. If we can keep knowledge local to an object (either internal or passed in), then its implementation is independent of its context; we can safely move it wherever we like. Do this consistently and your application, built out of pluggable components, will be easy to change.

If it’s explicit, we can name it. One reason why we don’t like mocking concrete classes is that we like to have names for the relationships between objects as well as the objects themselves. If we have something’s true name, we can control it. If we can see it, we have a better chance of finding its other uses and so reducing duplication.

More names mean more domain information. When we emphasize how objects communicate, rather than what they are, we end up with types and roles defined more in terms of the domain than of the implementation. This might be because we have a greater number of smaller abstractions, which gets us further away from the underlying language. Somehow we seem to get more domain vocabulary into the code.

Pass behavior rather than data. By applying “Tell, Don’t Ask” consistently, we end up with a coding style where we tend to pass behavior (in the form of callbacks) into the system instead of pulling values up through the stack. 

More precise interfaces give us better information-hiding and clearer abstractions. We care about keeping the tests and code clean as we go, because it helps to ensure that we understand our domain and reduces the risk of being unable to cope when a new requirement triggers changes to the design. It’s much easier to keep a codebase clean than to recover from a mess.

Refactoring should not lead to massive changes in test code. A unit test shouldn’t be 1000 lines long. It should focus on at most a few classes and should not need to create a large fixture or perform lots of preparation just to get the objects into a state where the target feature can be exercised. Such tests are hard to understand - there’s just so much to remember when reading them. And, of course, they’re brittle, all the objects in play are too tightly coupled and too difficult to set to the state the test requires.

Poor quality tests can slow development to a crawl, and poor internal quality of the system being tested will result in poor quality tests. By being alert to the internal quality feedback we get from writing tests, we can nip this problem in the bud, long before our unit tests approach 1000 lines of code, and end up with tests we can live with. 

Write tests that are readable and flexible. It gives us more feedback about the internal quality of the code we are testing. We end up with tests that help, rather than hinder continued development.</Text>
        </Document>
        <Document ID="95">
            <Title>Green Bar Patterns</Title>
            <Text>Once you have a broken test, you need to fix it. If you treat a red bar as a condition to fixed as quickly as possible,  you will discover that you can get to green quickly. These patterns are how to make the code pass (even if the result isn’t something you want to live with for even an hour)

Fake It (Till You Make It)

What is your first implementation once you have a broken test? Return a constant. Once you have the test running, gradually transform the constant into an expression using variables.

You haven’t really gotten there yet (the test is there but the code structure is wrong). However when you do get there, you know you will be safe (the test will still run).

Why would you do something that you know you have to rip out? Because having something running is better than not having something running, especially if you have the tests to prove it. Sometimes fake implementation can teach you that the written test is wrong and you don’t have to invest in the real solution to find out.

There are couple of effects that make Fake It powerful:

Psychological - Having a green bar feels completely different than having a red bar. When the bar is green, you know where you stand. You can refactor from there with confidence.

Scope control - Programmers are good at imagining all sorts of future problems. Starting with one concrete example and generalizing from there prevents you from prematurely confusing yourself with extraneous concerns. You can do a better job of solving the immediate problem because you are focused. When you go to implement the next test case, you can focus on that one, too, knowing that the previous test is guaranteed to work.

Does Fake It violate the rule that says you don’t write any code that isn’t needed? No, because in the refactoring step you are eliminating duplication of data between the test case and the code.

Triangulate

How do you most conservatively drive abstraction with tests? Only abstract when you have two or more examples.

Here’s an example. Suppose you want to write a function that will return the sum of two integers. We write:

Assert(4, plus(3,1))

Plus 
Return 4

If we are triangulating to the right design, we have to write:

Assert(4, plus(3,1))
Assert(7, plus(3,4))

When we have the second example, we can abstract the implementation of plus:

Plus(augend, addend)
Augend + addend

Triangulation is attractive because the rules for it seem so clear. The rules for Fake It, where we are relying on our sense of duplication between the test case and the fake implementation to drive abstraction, seem a bit vague and subject to interpretation. While they seem simple, the rules for triangulation create an infinite loop. Once we have the two assertions and we have abstracted the correct implementation for plus, we can delete on of the assertions on the grounds that it is completely redundant with the other. If we do that, however, we can simplify the implementation of plus to just return a constant, which requires us to add an assertion.

I only use triangulation when I’m unsure about the correct abstraction for the calculation. Otherwise I rely on either Obvious Implementation or Fake It.

Obvious Implementation

How do you implement simple operations? Just implement them.

Fake It and Triangulation are teensy-weensy tiny steps. Sometimes you are sure you know how to implement an operation. Go ahead. For example, would I really use Fake It to implement something as simple as plus? Not usually. I would just type in the obvious implementation. If I noticed I was getting surprised by red bars, I would go to smaller steps.

There’s no particular virtue in the halfway nature of Fake It and Triangulate. If you know what to type, and you can do it quickly, do it. However, by using only Obvious Implementation, you are demanding perfection of yourself. This is not a good move. What if the code you write isn’t really the simplest change that could get the test to pass? What if your partner shows you an even simpler one? You’re a failure!

Solving “clean code” at the same time you solve “that works” can be too much to do at once. As soon as it is, go back to solving “that works”, then “clean code” at leisure.

Keep track of how often you get surprised by red bars using Obvious Implementation. I’ll get stuck in these cycles where I’ll type in an Obvious Implementation. It won’t work. But now I’m sure I know what I should type, so I type that. It doesn’t work. So now…This especially happens with off by one errors and positive/negative errors.

You want to maintain that red/green/refactor rhythm. Obvious Implementation is second gear. Be prepared to downshift if your brain starts writing checks your fingers can’t cash.

One to Many

How do you implement an operation that works with collections of objects? Implement it without the collections first, then make it work with collections.

</Text>
        </Document>
        <Document ID="88">
            <Title>The Art of Unit Testing</Title>
            <Text>A unit test should have the following properties:

❂ It should be automated and repeatable.
❂ It should be easy to implement.
❂ Once it’s written, it should remain for future use.
❂ Anyone should be able to run it.
❂ It should run at the push of a button.
❂ It should run quickly.

Unit test usually exercises and tests only a single unit in isolation.

The technique of test-driven development is quite simple:

1 Write a failing test to prove code or functionality is missing from the end product. The test is written as if the production code were already working, so the test failing means there’s a bug in the production code. For example, if I wanted to add a new feature to a calculator class that remembers the LastSum value, I would write a test that verifies that LastSum is indeed a number. The test will fail because we haven’t implemented that functionality yet.

2 Make the test pass by writing production code that meets the expectations of your test. It should be written as simply as possible.

3 Refactor your code. When the test passes, you’re free to move on to the next unit test or to refactor your code to make it more readable, to remove code duplication, and so on.

Refactoring can be done after writing several tests or after writing each test. It’s an important practice, because it ensures your code gets easier to read and maintain, while still passing all of the previously written tests.

Writing our first test

How do we test our code? A unit test usually comprises three main actions:

❂ Arrange objects, creating and setting them up as necessary.
❂ Act on an object.
❂ Assert that something is as expected.

“Red-Green-Refactor,” meaning that you start with a failing test, then pass it, and then make your code readable and more maintainable.

State-based testing (also called state verification) determines whether the exercised method worked correctly by examining the state of the system under test and its collaborators (dependencies) after the method is exercised.

Indirect Testing of State

The Calculator class works a lot like the pocket calculator you know and love. You can click a number, then click Add, then click another number, then click Add again, and so on. When you’re done, you can click Equals and you’ll get the total so far.

Where do you start testing the Sum() function? You should always consider the simplest test to begin with, such as testing that Sum() returns 0 by default.

what happens when we aren’t facing a simple method with a return value, and we need to test the end state of
an object.

Name your tests clearly using the following model: [MethodUnderTest]_[Scenario]_[ExpectedBehavior].

Use the [SetUp] and [TearDown] attributes to reuse code in your tests, such as code for creating and initializing objects all your tests use.

❂ Don’t use [SetUp] and [TearDown] to initialize or destroy objects that aren’t shared throughout the test class in all the tests, because it makes the tests less understandable. Someone reading your code won’t
know which tests use the logic inside the setup method and which don’t.

we’ll take a look at more realistic examples where the object under test relies on another object over which we have no control (or which doesn’t work yet). That object could be a web service, the time of day, threading, or many other things. The important point is that our test can’t control what that dependency returns to our code under test or how it behaves (if we wanted to simulate an exception, for example).
That’s when we use stubs.

An external dependency is an object in your system that your code under test interacts with, and over which you have no control. (Common examples are filesystems, threads, memory, time, and so on.)

In programming, we use stubs to get around the problem of external dependencies.

A stub is a controllable replacement for an existing dependency (or collaborator) in the system. By using a stub, you can test your code without dealing with the dependency directly.

integration tests are slower to run, they need configuration, they test multiple things, and so on.

You can’t test something? Add a layer that wraps up the calls to that something, and then mimic that layer in your tests. Or make that something replaceable (so that it is itself a layer of indirection). The art also involves figuring out when a layer of indirection already exists instead of having to invent it, or knowing when not to use it because it complicates things too much. But let’s take it one step at a time. There’s a definite pattern for breaking the dependency:

Find the interface or API that the object under test works against.
Replace the underlying implementation of that interface with something that you have control over.

1 Find the interface that the method under test works against. (In this case, “interface” isn’t used in the pure object-oriented sense; it refers to the defined method or class being collaborated with.)

If the interface is directly connected to our method under test (as in this case—we’re calling directly into the filesystem), make the code testable by adding a level of indirection to the interface. In our example, moving the direct call to the filesystem to a separate class (such as FileExtensionManager) would be one way to add a level of indirection. We’ll also look at others. (Figure 3.3 shows how the design might look after this step.)

3 Replace the underlying implementation of that interactive interface with something that you have control over. In our case, we’ll replace the instance of the class that our method calls (FileExtensionManager) with a stub class that we can control (StubExtensionManager), giving our test code control over external dependencies.

Our replacement instance will not talk to the filesystem at all, which breaks the dependency on the filesystem. Because we aren’t testing the class that talks to the filesystem, but the code that calls this class, it’s OK if that stub class doesn’t do anything

 Refactoring is the act of changing the code’s design without breaking existing functionality.

 Seams are places in your code where you can plug in different functionality, such as stub classes.

If we want to break the dependency between our code under test and the filesystem, we can use common design patterns, refactorings, and techniques, and introduce one or more seams into the code. We just
need to make sure that the resulting code does exactly the same thing.

Here are some techniques for breaking dependencies:

❂ Extract an interface to allow replacing underlying implementation.
❂ Inject stub implementation into a class under test.
❂ Receive an interface at the constructor level.
❂ Receive an interface as a property get or set.
❂ Get a stub just before a method call.

We can use it in our tests to make sure that no test will ever have a dependency on the filesystem, but also so we can create new and bizarre scenarios where we can simulate serious system errors like making the stub manager throw an OutOfMemoryException and seeing how the system deals with it. Later in this chapter, we’ll add configurability to the stub class so it can emulate many things and be used by multiple tests.

Now we have an interface and two classes implementing it, but our method under test still calls the real implementation directly:

We somehow have to tell our method to talk to our implementation rather than the original implementation of IExtensionManager. We need to introduce a seam into the code, where we can plug in our stub.

 Inject stub implementation into a class under test. There are several proven ways to create interface-based seams in our code—places where we can inject an implementation of an interface into a class to be used in its methods. Here are some of the most notable ways:

❂ Receive an interface at the constructor level and save it in a field for later use.
❂ Receive an interface as a property get or set and save it in a field for later use.
❂ Receive an interface just before the call in the method under test

Using

• a parameter to the method (parameter injection).
• a factory class.
• a local factory method.
• variations on the preceding techniques.

The parameter injection method is trivial: you send in an instance of a (fake) dependency to the method in question by adding a parameter to the method signature.

Let’s go through the rest of the possible solutions one by one and see why you’d want to use each.

 Receive an interface at the constructor level (constructor injection) In this scenario, we add a new constructor (or a new parameter to an existing constructor) that will accept an object of the interface type we extracted earlier (IExtensionManager). The constructor then sets a local field of the interface type in the class for later use by our method or any other.

The stub analyzer is located in the same file as the test code because currently the stub is used only from within this test class. It’s far easier to locate, read, and maintain a stub in the same file than in a different
one. If, later on, I have an additional class that needs to use this stub, I can move it to another file easily.

Problems with constructor injection

Problems can arise from using constructors to inject implementations. If your code under test requires more than one stub to work correctly without dependencies, adding more and more constructors (or more and more constructor parameters) becomes a hassle, and it can even make the code less readable and less maintainable.

 Receive an interface as a property get or set

In this scenario, we add a property get and set for each dependency we’d like to inject. We then use this dependency when we need it in our code under test.

When you should use property injection

Use this technique when you want to signify that a dependency of the class under test is optional, or if the dependency has a default instance created that doesn’t create any problems during the test.

Interaction Testing using Mock Objects

How to test whether an object calls other objects correctly? The object being called may not return any result or save any state, but it has complex logic that needs to result in correct calls to other objects. Using stubs is not appropriate here, because there’s no externalized API that we can use to check if something has changed in the object under test. How do you test that your object interacts with other objects correctly? That’s where mock objects come in.

What is interaction testing? How is it different from state-based testing that we have done so far?

Interaction testing is testing how an object sends input to or receives input from other objects - how that object interacts with other objects. You can also think of interaction testing as being “action-driven testing” and state-based testing as being “result-driven testing”. Action driven means that you test a particular action on object takes (such as sending a message to another object). Result-driven means you test that some end result is now true (that a property value has changed for example). It’s usually preferable to check the end results of objects, not their particular actions. But sometimes interactions between objects are the end result. That’s when we need to test the interaction itself (where the end result of calling a method on the object under test is that the object then calls another object such as a web service).

Sometimes state-based testing is the best way to go because interaction testing is too difficult.

A mock object is a fake object in the system that decides whether the unit test has passed or failed. It does so by verifying whether the object under test interacted as expected with the fake object. There’s usually no more than one mock per test.

The Difference Between Mocks and Stubs

When using a stub, the assert is performed on the class under test. The stub aids in making sure the test runs smoothly. Stubs cannot fail tests and mocks can.

Stubs replace an object so that we can test another object without problems. The easiest way to tell we’re dealing with a stub is to notice that the stub can never fail the test. The asserts the test uses are always agains the class under test.

On the other hand, the test will use a mock object to verify whether the test failed or not. In this case the assert is performed on the mock. Again, the mock object is the object we use to see if the test failed or not.

The class under test communicates with the mock object and all communication is recorded in the mock. The test uses the mock object to verify that the test passes.

Creating and using a mock object is much like using a stub, except that a mock will do a little more than a stub: it will save the history of communication, which will later be verified.

Example 4.3 Mock is used for a Webservice. Is it not true that Mock should be used only for things that we can control?

In your tests you might find that you need to replace more than one object. We’ll look at combining mocks and stubs next.

Using a Mock and a Stub Together

LogAnalyzer not only needs to talk to a web service but if the web service throws an error, LogAnalyzer has to log the error to a different external dependency, sending it by email to an administrator.

Notice that there’s logic here that only applies to interacting with external objects; there’s no end result returned to the caller. How do you test that LogAnalyzer calls the email service correctly when the web service throws an exception?

We need to answer:

How can we replace the web service?
How can we simulate an exception from the web service so that we can test the call to the email service?
How will we know that the email service was called correctly or at all?

We can deal with the first two questions by using a stub for the web service. To solve the third problem, we can use a mock object for the email service.

A fake is a generic term that can be used to describe a stub or a mock object because they both look like the real object. Whether a fake is a stub or a mock depends on how it’s used in the current test. If it’s used to check an interaction (asserted against), it’s a mock object. Otherwise, it’s a stub.

In our test we’ll have two fakes. One will be the email service mock which we’ll use to verify that the correct parameters were sent to the email service. The other will be a stub that we’ll use to simulate an exception thrown from the web service. It’s a stub because we won’t be using the web service fake to verify the test result, only to make sure the test runs correctly. The email service is a mock because we’ll assert against it that it was called correctly. 

The whole test will be about how LogAnalyzer interacts with other objects.

One Mock Per Test

In a test where you test only one thing (which is a good practice) there should be no more than one mock object. All other fake objects will act as stubs. Having more than one mock per test usually means you’re testing more than one thing, and this can lead to complicated or brittle tests.

If you follow this guideline, when you get to more complicated tests, you can always ask yourself, “Which one is my mock object?” Once you’ve identified it, you can leave the others as stubs and not worry about assertions against them.

Next, we’ll deal with a more complex scenario: using a stub to return a stub or a mock that will be used by the application.

Stub Chains : Stubs that Produce Mocks or Other Stubs

Sometimes we want to have a fake component return another fake component, producing our own little chain of stubs in our tests, so that we can end up collecting some data during our test. A stub leads to a mock object that records data.

The design of many systems under test allows for complex object chains to be created. 

Summary

A mock object is like a stub, but it also helps you to assert something in your test. A stub, on the other hand, can never fail your test and is strictly there to simulate various situations. 

Combining stubs and mocks in the same test is a powerful technique, but you must take care to have no more than one mock in each test. The rest of the fake objects should be stubs that can’t break your test. Following this practice can lead to more maintainable tests that break less often when internal code changes.

Stubs that produce other stubs or mocks can be a powerful way to inject fake dependencies into code that uses other objects to get its data. It’s a great technique to use with factory classes and methods. You can even have stubs that return other stubs that return other stubs and so on but at some point you’ll wonder if it’s all worth it. 

One of the most common problems encountered by people who write tests is using mocks too much in their tests. You should rarely verify calls to fake objects that are used both as mocks and as stubs in the same test. (This is quite a narrow corner case. You verify a function was called. Because it’s still a function, it must return some value, and because you’re faking that method, you’ll need to tell the test what that value will be. This value is the part in the test that’s a stub, because it has nothing to do with asserting whether the test passes or fails.) If you see “verify” and “stub” on the same variable in the same test, you most likely are over-specifying your test, which will make it more brittle.

You can have multiple stubs in a test, because a class may have multiple dependencies. Just make sure your test remains readable. Structure your code nicely for readability.

Isolation Frameworks

An isolation framework is a set of programmable APIs that make creating mock and stub objects much easier. Isolation frameworks save the developer from the need to write repetitive code to test or simulate object interactions.

Expectations on Mocks and Stubs

An expectation on a fake object is an ad hoc rule that’s set on the object.

Expectations on Mocks - The rule will usually tell the object that a specific method call on that object is expected to happen later. It may also define how many times it should be called, whether or not the object should throw an exception when that call arrives, or perhaps that the call to this method should never be expected. Expectations are usually set on mock objects during the recording phase of the object and they’re verified at the end of the test where the mock object lives.

Expectations on stubs - This wording may feel unintuitive at first but the rule can tell the stub what the value to return based on expected method calls, whether to throw exceptions and so on. These expectations aren’t to be verified at the end of the test. They’re there so that you can run the test and simulate some alternative reality for your code under test.

Strict vs Non-strict Mocks

A strict mock object can only be called by methods that were explicitly set via expectations. Any call that differs either by parameter values defined or by the method name will usually be handled by throwing an exception. The test will fail on the first unexpected method call to a strict mock object. I say usually because whether or not the mock throws an exception depends on the implementation of the isolation framework. 

This means that a strict mock can fail in two ways: when an unexpected method is called on it, or when expected methods aren’t called on it.

Non strict mocks make for less brittle tests. A non strict mock object will allow any call to be made to it, even if it was not expected. As long as the call doesn’t require a return value, it will do what’s necessary for everything in the test to work out.

A non strict mock can only fail a test if an expected method was not called. 

Arrange Act Assert Syntax for Isolation

The record and replay model for setting expectations on stubs and mocks has been cumbersome for some people. It also makes tests less readable if you have lots of stubs and expectations in a single test.

A simplified and concise syntax for setting expectations is based on the way we structure our unit tests. We arrange objects, act on them and assert that something is true or false (arrange-act-assert or AAA). It would be nice if we could use isolation frameworks similarly - to arrange mocks and stubs, set their default behavior and only at the end of the test, verify whether a call to the mock object took place. 

Traps to Avoid when using Isolation Frameworks

Some examples are over using an isolation framework when a manual mock object would suffice, making tests unreadable because of overusing mocks in a test, or not separating tests well enough.

Here’s a simple list of things to watch out for:

Unreadable test code
Verifying the wrong things
Having more than one mock per test
Over specifying the tests

Unreadable Test Code

Having many mocks or many expectations in a single test can ruin the readability of the test so it’s hard to maintain or even to understand what’s being tested.

If you find that your test becomes unreadable or hard to follow, consider removing some mocks or some mock expectations or separating the test into several smaller tests that are more readable.

Verifying the wrong things

Mock objects allow us to verify that methods were called on our interfaces, but that doesn’t necessarily mean that we’re testing the right thing. Testing that an object subscribed to an event doesn’t tell us anything about the functionality of that object. Testing that when the event is raised something meaningful happens is a better way to test that object.

Having more than one mock per test

It’s considered good practice to only test one thing per test. Testing more than one thing can lead to confusion and problems maintaining the test. Having two mocks in a test is the same as testing several things. If you can’t name your test because it does too many things, it’s time to separate it into more than one test.

Over specifying the tests

If your test has too many expectations, you may create a test that breaks down with even the smallest of code changes, even though the overall functionality still works. Consider this a more technical way of not verifying the right things. Testing interactions is a double-edged sword: test it too much and you start to lose sight of the big picture - the overall functionality; test it too little, and you’ll miss the important interactions between objects.

Here are some ways to balance this effect:

Use non strict mocks when you can
The test will break less often because of unexpected method calls. This helps when the private methods in the production code keep changing.

Use stubs instead of mocks when you can

You only need to test one scenario at a time. The more mocks you have, the more verifications will take place at the end of the test, but only one of them will usually be the important one. The rest will be noise against the current test scenario.

Avoid using stubs as mocks

Use a stub only for faking return values into the program under test, or to throw exceptions. Don’t verify that methods were called on stubs. Use a mock only for verifying that some method was called on it, but don’t use it to return values into your program under test. If you can’t avoid this situation, you should probably be using a stub and testing something other than what the mock object receives.

Don’t repeat logic in your tests

If you’re asserting that some calculation is correct in your code, make sure your test doesn’t repeat the calculation in the test code, or the bug might be duplicated and the test will magically pass.

Don’t use magic values

Use hardcoded, known return values to assert against production code and don’t create expected values dynamically. That would significantly increase the chances for an unreadable test or a bug in the test.

Over specification is a common form of test abuse. Make sure you keep an eye on this by doing frequent test reviews with your peers.

Summary

Lean toward state-based testing as opposed to interaction testing whenever you can. Your tests assume as little as possible about internal implementation details. Mocks should be used only when there’s no other way to test the implementation, because they eventually lead to tests that are harder to maintain if you’re not careful.

Over specified tests lead to brittle tests. Change your test to test a different result that proves your point but is easier to test.

The Pillars of Good Tests

The good tests should have three properties: Trustworthiness, maintainability, readability. 

Developers will want to run trustworthy tests and they’ll accept the test results with confidence. Trustworthy tests don’t have bugs and they test the right things.
Developers will simply stop maintaining and fixing tests that take too long to change.
Readability means not just being able to read a test but also figuring out the problem if the test seems to be wrong. Without readability the other two pillars fall pretty quickly. Maintaining tests becomes harder, and you can’t trust them anymore.

Writing trustworthy tests

There are several indications that a test is trustworthy. If it passes, you don’t say, “I’ll step through the code in the debugger to make sure”. You trust that it passes and that the code it tests works for that specific scenario. If the test fails, you don’t say, “Oh, it’s supposed to fail”, or “That doesn’t mean the code isn’t working”. You believe that there’s a problem in your code and not in your test. In short, a trustworthy test is one that makes you fee you know what’s going on and that you can do something about it.

We will now see guidelines and techniques to help you do the following:

Decide when to remove or change tests
Avoid test logic
Test only one thing
Make tests easy to run
Assure code coverage

Tests that follow these guidelines can be trusted and will continue to find errors in code.

Deciding when to remove or change tests

Once you have tests in place, you should generally not change or remove them. They are there as your safety net, to let you know if anything breaks when you change your code. 

The main reason for removing a test is when it fails. A test can “suddenly” fail for several reasons:
Production bugs - There’s a bug in the production code under test
Test bugs - There’s a bug in the test.
Semantics or API changes - The semantics of the code under test changed, but not the functionality.
Conflicting or invalid tests - The production code was changed to reflect a conflicting requirement.

There are also reasons for changing or removing tests when nothing is wrong with the tests or code:

To rename or refactor the test
To eliminate duplicate tests.

Production bugs

A production bug occurs when you change the production code and an existing test breaks. If indeed this is a bug in the code under test your test is fine and you shouldn’t need to touch the test. This is the best and most desired outcome of having tests.
Because the occurrence of production bugs is one of the main reasons we have unit tests in the first place, the only thing left to do is to fix the bug in the production code. Don’t touch the test.

Test bugs

If there’s a bug in the test, you need to change the test. Bugs in tests are hard to detect because the test is assumed to be correct. There are several stages developers go through when a test bug is encountered:
Denial - the developer will keep looking for a problem in the code itself, changing it, causing all the other tests to start failing. The developer introduces new bugs into production code while hunting for the bug that’s actually in the test.

Amusement - The developer will call another developer and they will hunt for the non-existent bug together

Debugging - The developer will patiently debug the test and discover that there’s a problem in the test. This can take from an hour to a few days.

Acceptance - The developer will eventually realize where the bug is and will slap himself on the forehead.

When you finally find the bug, it’s important to make sure that the bug gets fixed, and that the test doesn’t magically pass by testing the wrong thing. You need to do the following:

1. Fix the bug in your test
2. Make sure the test fails when it should
3. Make sure the test passes when it should

The first step, fixing the test is straightforward. The next two steps make sure you’re still testing the correct thing and that you test can still be trusted.

Once you have fixed your test, change the production code under test so that that it manifests the bug that the test is supposed to catch. Then run the test. If the test fails, that means it’s half working. The other half will be completed in step 3. If the test doesn’t fail you’re most likely testing the wrong thing. 

Once you see the test fail, change your production code so that the bug no longer exists. The test should now pass. If it doesn’t, you either still have a bug in your test or you’re testing the wrong thing. You want to see the test fail and then pass again after you fix it so that you can be sure that it fails and passes when it should.

Semantics or API changes

A test can fail when the production code under test changes so that an object being tested now needs to be used differently, even though it may still have the same functionality.

Conflicting or invalid tests

A conflict problem arises when the production code introduces a new feature that’s in direct conflict with a test. This means that instead of the test discovering a bug, it discovers conflicting requirements.

This either-or scenario where only one of two tests can pass serves as a warning that these may be conflicting tests. In this case, you first need to make sure that the tests are in conflict. Once that’s confirmed you need to decide which requirement to keep. You should then remove the invalid requirement and its tests.

Renaming or refactoring tests
An unreadable test hinders understanding of any problem it finds. Change the test code to make it more maintainable. 

Eliminating duplicate tests

 In big projects it’s common to come across multiple tests written by different developers for the same functionality. 
The more good tests you have the more certain you are to catch bugs.
You can read the tests and see different ways or semantics of testing the same thing.
Some tests may be more expressive that others so more tests may improve the chances of test readability.
Tests may have little differences and could be testing the same things slightly differently. They may make for a larger and better picture of the object being tested.

It may be harder to maintain.
You need to review all of them for correctness.
Multiple tests may break when a single thing doesn’t work.
Similar tests may be named differently or the tests can be spread across different classes.

Avoiding logic in tests

The chances of having bugs in your tests increase exponentially as you include more and more logic in them. If you have any of the following inside a test method you test contains logic that should not be there:
Switch, if, else and other statements. For, each, or while loops

A test that contains logic is usually testing more than one thing at a time which isn’t recommended because the test is less readable and fragile. But test logic also adds complexity that may contain a hidden bug.
Tests should as a general rule, be a series of method calls with no control flows, not even a rescue clause and with assert calls. Anything more complex causes the following problems:

The test is harder to read and understand.
The test is hard to re-create.
The test is more likely to have a bug or to test the wrong thing.
Naming the test may be harder because it does multiple things.

Generally monster tests replace original simpler tests and that makes it harder to find bug in the production code.

Testing only one thing

If your test contains more than a single assert, it may be testing more than one thing. That doesn’t sound so bad until you name your test or consider what happens if the first assert fails.

Naming a test may seem like a simple task but if you’re testing more than one thing, giving the test a good name that indicates what is being tested becomes almost impossible. When you test just one thing naming the test is easy.

If the first assert fails it will throw an exception and the subsequent asserts will never run. Failures must clearly indicate the reason for failure.

Making tests easy to run

Refactor tests so that they’re easy to run and provide consistent results. 

Assuring code coverage

Use tools to make sure you have good coverage. During test reviews you can also do a manual check, which is a great for ad hoc testing of a test: try commenting out a line or a constraint check. If all tests still pass, you might be missing some tests or the current tests may not be testing the right thing.

When you add a new test that was missing, check whether you’ve added the correct test with these steps:

1. Comment out the production code you think isn’t being covered.
2. Run all the tests.
3. If all the tests pass, you’re missing a test or are testing the wrong thing. Otherwise there would have been a test somewhere that was expecting that line to be called or some resulting consequence of that line of code to be true and that missing test would now fail.
4. Once you’ve found a missing test, you’ll need to add it. Keep the code commented out and write a new test that fails, proving that the code you’ve commented is missing.
5. Uncomment the code you commented before.
6. The test you wrote should now pass. You’ve detected and added a missing test.
7. If the test still fails, it means the test may have a bug or is testing the wrong thing. Modify the test until it passes. Now you’ll want to see that the test is OK, making sure it fails when it should and doesn’t just pass when it should. To make sure the test fails when it should reintroduce the bug into your code (commenting out the line of production code) and see if the test fails.

As an added confidence booster you might also replace various parameters or internal variables in your method under test with constants (making a boolean always true to see what happens, for example).

Writing Maintainable Tests

Maintainability is one of the core issues most developers face when writing unit tests. Eventually the tests seem to become harder and harder to maintain and understand and every little change to the system seems to break one test or another even if bugs don’t exist. With all pieces of code time adds a layer of indirection between what you think the code does and what it really does.

Some techniques in writing maintainable tests are testing only against public contracts, removing duplication in tests and enforcing test isolation.

Testing private or protected methods

Encapsulation hides implementation details so that the implementation can change without changing functionality. It could also be for security related or IP-related reasons (obfuscation, for example).

When you test a private method you’re testing the internal implementation of the system. This test will fail when the internals change even though the overall functionality of the system remains the same. 

For testing purposes, the public contract (the overall functionality) is all that you need to care about. With test-driven development we write tests against methods that are public and those public methods are later refactored into calling smaller private methods. All the while the tests against the public methods continue to pass.

Making methods public

If the private method has a known behavior or contract against the calling code you can make it public. 

Extracting methods to new classes

If your method contains a lot of logic that can stand on its own or it uses state in the class that’s only relevant to the method in question it may be a good idea to extract the method into a new class with a specific role in the system. You can then test that class separately.

Making methods class method

If your method doesn’t use any of its class’s variables, you could refactor the method to make it a class method. It means that it is now a utility method that has a known public contract specified by its name.

Removing Duplication

Duplication means more code to change when one aspect we test against changes. 

You can remove duplication using a Factory helper method. You can also remove duplication using setup method.

Using setup Methods in a Maintainable Manner

The setup method used for thing it was not meant for can make tests less readable and maintainable.

Setup method must be used only to initialize things.
Setup methods cannot have parameters or return values.
Setup methods should only contain code that applies to all the tests in the current test class or the method will be harder to read and understand.

Developers abuse setup methods in several ways:
Initializing objects that are only used in some of the tests in the class.
Having setup code that’s long and hard to understand.
Setting up mocks and face objects

Initializing objects that are only used by some of the tests

It becomes difficult to read and maintain the tests because the setup method quickly becomes loaded with objects that are specific only to some of the tests. 

To read the tests for the first time and understand why they break you need to do the following:

1. Go through the setup method to understand what is being initialized.
2. Assume that objects in the setup method are used in all tests.
3. Find out later you were wrong and read the tests again more carefully to see which test uses the objects that may be causing the problems.
4. Dive deeper into the test code for no good reason, taking more time and effort to understand what the code does.

Always consider the readers of your test when writing the tests. Imagine this is the first time they read them. Make sure it is easy to read.

Having setup code that’s long and hard to understand

Refactor calls to initialize specific things into helper methods that are called from the setup method.

Setting up mocks and fakes in the setup method

It’s ok to create mocks and fake objects if they are used in all the tests in the class. I prefer to have each test create its own mocks and stubs by calling helper methods within the test, so that the reader of the test knows exactly what’s going on without needing to jump from test to setup to understand the full picture.

Enforcing test isolation

The lack of test isolation is the biggest single cause of test blockage. The basic concept is that a test should always run in its own little world, isolated from even the knowledge that other tests out there may do similar or different things. 

When tests aren’t isolated well, they can step on each other’s toes to make life miserable. We don’t bother looking for problems in the tests, so when there’s a problem with the tests, it can take a lot of time to find it.

There are several test “smells” that can hint at broken test isolation:

Constrained test order - Tests expecting to be run in a specific order or expecting information from other test results
Hidden test call - Tests calling other tests
Shared state corruption - Tests sharing in-memory state without rolling back
External state corruption - Integration tests with shared resources and no rollback

Constrained test order

This problem arises when tests are coded to expect a specific state in memory, in an external resource, or in the current test class - a state that was created by running other tests in the same class before the current test. 

A myriad of problems can occur when tests don’t enforce isolation.

A test may suddenly start breaking when a new version of the test framework is introduced that runs the tests in a different order.
Running a subset of the tests may produce different results than running all the tests or a different subset of the tests.
Maintaining the tests is more cumbersome, because you need to worry about how other tests relate to particular tests and how each one affects state.
Your tests may fail or pass for the wrong reasons; for example, a different test may have failed or passed before it, leaving the resources in an unknown state.
Removing or changing some tests may affect the outcomes of other tests.
It’s difficult to name your tests appropriately because they test more than a single thing.

There a couple of common patterns that lead to poor test isolation:
Flow testing - A developer writes tests that must run in a specific order so that they can test flow execution, a big use case composed of many actions or full integration test where each test is one step in that full test.
Laziness in cleanup - A developer is lazy and doesn’t return any state her test may have changed back to its original form and other developers write tests that depend on this symptom knowingly or unknowingly.

These problems can be solved:

Flow testing - Remove flow-related tests in unit test and use integration testing.
Laziness in cleanup - Cleanup your database, filesystem, memory-based objects etc after testing.

Hidden test call

In this anti-pattern, tests contain one or more direct calls to other tests in the same class or other test classes which causes tests to depend on one another. 

This type of dependency can cause several problems:

Running a subset of the tests may produce different results than running all the tests or a different subset of the tests.
Maintaining the tests is cumbersome because you need to worry about how other tests relate to particular tests and how and when they call each other.
Tests may fail or pass for the wrong reasons. For example, a different test may have failed thus failing your test or not calling it at all. Or a different test may have left some shared variables in an unknown state.
Changing some tests may affect the outcome of other tests.
It’s difficult to clearly name test that call other tests.

Here are a few causes for this problem:
Flow testing
Trying to remove duplication - A developer tries to remove duplication in the tests by calling other tests.
Laziness in separating the tests - Doesn’t take the time to create a separate test and refactor the code appropriately, instead taking a shortcut and calling a different test.

Avoiding multiple asserts

Once an assert clause throws an exception, no other line executes in the test method. There are several ways to achieve the same goal:
Create a separate test for each assert
Use parametrized tests
Wrap the assert call with rescue clause

Refactoring into multiple tests

Multiple asserts are really multiple tests without the benefit of test isolation; a failing test causes the other asserts not to execute. Instead, we can create separate test methods with meaningful names that represent each test case. 

Using parametrized tests

Parametrize the method and pass different data sets to test them individually. This gives us a declarative way of creating a single test with different inputs. 
The best thing about this is that if one of the test fails the other tests are still executed by the test runner so we see the full picture of pass/fail states in all tests.

Wrapping with rescue clause
 Parametrized tests are better than this approach.

Avoiding testing multiple aspects of the same object

A test can have multiple asserts that is not acting as multiple tests in one test but to check multiple aspects of the same state. If even one aspect fails we need to know about it. 

Overriding to_s

Override to_s to get meaningful error messages. The test output will be clear and we can understand that the test fails and makes it easy to maintain. 

Another way tests can become hard to maintain is when we make them too fragile by over specification.

Avoiding over specification in tests

An over specified test is one that contains assumptions about how a specific unit under test should implement its behavior, instead of only checking that the end behavior is correct.

Here are some ways unit tests are often over specified:

A test specifies purely internal behavior for an object under test.
A test uses mocks when using stubs would be enough.
A test assumes specific order or exact string matches when it isn’t required.

Specifying purely internal behavior

A test that tests internal state and no outside functionality. This test is over specified because it only test the internal state of the object. Because this state is internal, it could change later on.

Unit tests should be testing the public contract and public functionality of an object.

Using mocks instead of stubs

This is a common mistake. The test should let the method under test run its own internal algorithms and test the results. By doing that we make the test less brittle. The important thing is the test must assert the end result and it doesn’t care about the internal algorithm. 

We also use a stub that doesn’t care how many times it gets called and it always returns the same result. The test becomes less fragile and tests the right thing.

Assuming an order or exact match when it’s not needed

Ask yourself “Can I use string contains rather than string equals?”. The same goes for collections. It’s much better to make sure it contains an expected item than to assert that the item is in a specific place in a collection (unless that’s specifically what is expected).

Writing readable tests

Naming unit tests
The test name has three parts:

The name of the method being tested - This is essential so that you can easily see where the tested logic resides.

The scenario under which it’s being tested - This gives us the “with” part of the name: “When I call method X with a null value, then it should do Y”. 

The expected behavior when the scenario is invoked - This part specifies in plain English what the method should do or return or how it should behave, based on the current scenario (then it should do Y).

Method-under-test_scenario_behavior

Naming variables

Tests also serve as a form of API documentation. Good naming of variables helps people reading our tests understand what we’re trying to prove as quickly as possible (as opposed to understanding what we’re trying to accomplish when writing production code).

Asserting yourself with meaning

Writing a good assert message is like writing a good exception message. There are several things to remember when writing a message for an assert clause:

Don’t repeat what the test framework outputs to the console.
Don’t repeat what the test name explains.
If you don’t have anything good to say, don’t say anything.
Write what should have happened or what failed to happen, and possibly mention when it should have happened.

Separating asserts from actions

Avoid writing the assert line and the method call in the same statement.

Setting up and tearing down

If you have mocks and stubs being set up in a setup method, that means they don’t get set up in the actual test. That means anyone reading your test may not even realize that there are mock objects in use or what the expectations are from them in the test.

It’s much more readable to initialize mock objects directly in the test itself with all their expectations. If you’re worried about readability you can refactor the creation of the mocks into a helper method, which each test calls. That way anyone reading the test will know exactly what is being setup instead of having to look in multiple places.

Summary

Tests grow and change with the system under tests.


</Text>
        </Document>
        <Document ID="96">
            <Title>xUnit Patterns</Title>
            <Text>Assertion

How do you check that tests worked correctly? Write boolean expressions that automate your judgment about whether the code worked.

The test should not be dependent on the implementation. It should use only public protocol. Wishing for white box testing is not a testing problem, it is a design problem. Any time I want to use a variable as a way of checking to see whether code ran correctly or not, I have an opportunity to improve the design. It is a good practice to adopt the convention that all assertions must be accompanied by an informative error message.

Fixture 

How do you create common objects needed by several tests? Covert the local variables in the tests into instance variables. Override setup and initialize those variables.

If we want to remove duplication from our model code, do we want to remove it also from our test code? Maybe.

Here’s the problem - often you write more code setting up objects in an interesting state than you write manipulating them and checking results. The code for setting up the objects is the same for several tests (these objects are test’s fixture, also known as scaffolding). This duplication is bad:

It takes a while to write, even to copy-paste, and we’d like test writing to be fast.
If we need to change an interface by hand, we have to change it in several tests

The same duplication, however is good. Tests written with the set-up code right there with the assertions are readable top to bottom. If we factored the setup code into a separate method we would have to remember that the method was called, and remember what the objects looked like, before we could write the rest of the test.

All the tests sharing a single fixture will be methods in the same class. Tests requiring a different fixture will be in a different class.

External Fixture

How do you release external resources in the fixture? Override tear_down and release the resources.

Remember that the goal of each test is to leave the world in exactly the same state as before it ran. 

Test Method

How do you represent a single test case? As a method whose name begins with test by convention.

Test methods should be easy to read, pretty much straightline code. If a test method is getting long and complicated, you need to play “Baby Steps”. The goal of the game is to write the smallest test method that represents real progress towards end goal. Three lines appears to be about the minimum, without deliberate obfuscation.

Outlines

When I write tests, I first create a short outline of the tests I want to write, for example:

/*  Adding to tuple spaces */
/*  Taking from tuple spaces */
/*  Reading from tuple space */

These are place holders until I add specific tests under each category. When I add tests, I add another level of comments to the outline.
/*  Adding to tuple spaces */
/*  Taking from tuple spaces */
/* *  Taking a non-existent  tuple * */
/* *  Taking an existing tuple **/
/* *  Taking multiple tuples **/
/*  Reading from tuple space */

I usually only have two or three levels to the outline. But the outline essentially becomes documentation of the contract for the class being tested. The examples here are abbreviated, but they would be more specific in a contract-like language. Immediately under the lowest level of the outline is the test case code.

Exception Test

How do you test for expected exceptions? Catch expected exceptions and ignore them, failing only if the exception isn’t thrown. Catch only the particular exception we expect so if the wrong kind of exception is thrown we will be notified.




</Text>
        </Document>
        <Document ID="89">
            <Title>Course Outline</Title>
            <Text>Rhythm of TDD
3 A’s of Testing
State Based Testing
Interaction Testing
Mocks and Stubs
When to use Stubs
When to use Mocks
When to combine Mocks and Stubs
The Three Basic Pillars of Good Unit Tests
How to Avoid Brittle Tests
How to overcome the hurdles to writing the test first
Learn key techniques for making tests easy to understand and for avoiding and removing test code duplication


Presentation Structure

Outline
Check with Audience - Font size, volume etc
Promise
Know your Audience
Cycle 3 times [Introduction to Concept, Demo (near miss), Exercise]
Near Miss
Verbal Punctuation
Rhetorical Questions with 5 to 10 seconds pause
Remind on Delivered Promise
Salute the audience
Feedback : [Anonymous, Incentive for every participant]
50% Lecture and 50% Lab

TDD Boot Camp attendees will learn about the following topics:
	•	The TDD mindset
	◦	Why is TDD important?
	◦	Red Green Refactor
	◦	Behavior driven development
	◦	How TDD helps you design software
	◦	Why testing first is better than testing after
	•	Writing testable code
	◦	How to write tests that run fast and are easy to maintain
	◦	What makes code difficult to test
	◦	The dependency inversion principle
	•	Advanced TDD
	◦	Single responsibility principle, and how TDD helps you design small classes
	◦	How to use mocking 
	◦	How to effectively translate technical specs into tests
</Text>
        </Document>
        <Document ID="128">
            <Title>Test Readability</Title>
            <Text>To design is to communicate clearly by whatever means you can control or master - Milton Glaser

For TDD to be sustainable, the tests must do more than verify the behavior of the code; they must also express that behavior clearly - they must be readable. Otherwise the team velocity drops. 

Test code should describe what the production code does. That means that it tends to be concrete about the values it uses as examples of what results to expect, but abstract about how the code works. Production code, on the other hand, tends to be abstract about the values it operates on but concrete about how it gets the job done. Similarly, when writing production code, we have to consider how we will compose our objects to make up a working system, and manage their dependencies carefully. Test code, on the other hand, is at the end of the dependency chain, so it’s more important for it to express the intention of its target code than to plug into a web of other objects. We want our test code to read like a declarative description of what is being tested. 

 Keep tests readable and expressive. The readability problems we watch out for :

Test names that do not clearly describe the point of each test case and its differences from the other test cases.
Single test cases that seem to be exercising multiple features.
Tests with different structure, so the reader cannot skim-read them to understand their intention.
Tests with lots of code for setting up and handling exceptions, which buries their essential logic.
Tests that use literal values but are not clear about what, if anything, is significant about those values.

Test Names Describe Features

The name of the test should be the first clue for a developer to understand what is being tested and how the target object is supposed to behave. We don’t need to know that TargetObject has a choose method - we need to know what the object does in different situations, what the method is for.

A better alternative is to name tests in terms of the features that the target object provides. We use a TestDox convention where each test name reads like a sentence, with the target class as the implicit subject. For example:
A List holds items in the order they were added.
A List can hold multiple references to the same item.
A List throws an exception when removing an item it doesn’t hold.

We can translate these directly to method names:

ListTests#holds_items_in_the_order_they_were_added
ListTests#can_hold_multiple_references_to_the_same_item
ListTests#throws_an_exception_when_removing_an_item_it_doesnt_hold

The point of the convention is to encourage the developer to think in terms of what the target object does, not what it is. It’s also very compatible with our incremental approach of adding a feature at a time to an existing codebase. It gives us a consistent style of naming all the way from user stories, through tasks and acceptance tests, to unit tests.

As a matter of style, the test name should say something about the expected result, the action on the object, and the motivation for the scenario. For example, if we were testing a ConnectionMonitor class then polls_the_servers_monitoring_port doesn’t tell us enough: why does it poll? What happens when it gets a result? notifies_listeners_that_server_is_unavailable_when_cannot_connect_to_monitoring_port explains both the scenario and the expected behavior.

Test Name First or Last?

Test name must clarify intentions, make sure that the test is, in the end, consistent and expressive.
 The TestDox format fulfills the early promise of TDD - that the tests should act as documentation for the code.

Regularly Read Documentation Generated from Tests

You find that such generated documentation gives us afresh perspective on the test names, highlighting the problems we’re too close to the code to see. We make an effort to at least skim-read the documentation regularly during development.

Canonical Test Structure

If we write tests in a standard form, they’re easier to understand. We can skim-read to find expectations and assertions quickly and see how they related to the code under test. If we’re finding it difficult to write a test in a standard form, that’s often a hint that the code is too complicated or that we haven’t quite clarified our ideas.

The most common form for a test is:

1. Setup: Prepare the context of the test, the environment in which the target code will run.
2. Execute: Call the target code, triggering the tested behavior.
3. Verify: Check for a visible effect that we expect from the behavior
4. Teardown: Clean up any leftover state that might corrupt other tests.

There are other versions of this form, such as Arrange, Act, Assert, which collapse some of the stages.
For example, no teardown. Tests that set expectations on mock objects use a variant of this structure where some of the assertions are declared before the execute stage and are implicitly checked afterwards (setup, expect, assert, teardown)

Write Tests Backwards

Write the test name, which helps us decide what we want to achieve; write the call to the target code, which is the entry point for the feature; write the expectations and assertions, so we know what effects the feature should have; and, write the setup and teardown to define the context for the test. Of course, there may be some blurring of these steps to help the compiler, but this sequence reflects how we tend to think through a new unit test. Then we run it and watch it fail.

How Many Assertions in a Test Method

Some TDD practitioners suggest that each test should only contain one expectation or assertion. This is useful as a training rule when learning TDD, to avoid asserting everything the developer can think of, but we don’t find it practical. A better rule is to think of one coherent feature per test, which might be represented by up to a handful of assertions. If a single test seems to be making assertions about different features of a target object, it might be worth splitting up. Once again, expressiveness is the key: as a reader of this test, can I figure out what’s significant?

Streamline the Test Code

All code should emphasize what it does over how, including test code; the more implementation detail is included in a test method, the harder it is for the reader to understand what’ important. Move everything out of the test method that doesn’t contribute to the description, in domain terms, of the feature being exercised. Sometimes that involves restructuring the code, sometimes just ignoring the syntax noise.

Use Structure to Explain

Use small methods to express intent. The assertion line must express our intent. This may create more program text in the end but we’re prioritizing expressiveness over minimizing the source lines.

Use Structure to Share

Extract common features into methods that can be shared between tests for setting up values, tearing down state, making assertions, and occasionally triggering the event. Write method that wraps up repeated behavior behind a descriptive name.

The only caution with factoring out test structure is that, we have to be careful not to make a test so abstract that we cannot see what it does any more. Our highest concern is making the test describe what the target code does, so we refactor enough to be able to see its flow, but we don’t always refactor as hard as we would for production code.

Accentuate the Positive

We only catch exceptions in a test if we want to assert something about them. Test tells us just what is supposed to happen and ignore everything else.

Delegate to Subordinate Objects

Sometimes helper methods aren’t enough and we need helper objects to support the tests. So we can write tests in domain terminology and not in technical terms. Write test data builders to build up complex data structures with just the appropriate values for a test. Again, the point is to include in the test just the values that are relevant, so that the reader can understand the intent; everything else can be defaulted.

There are two approaches to writing subordinate objects. We start by writing a test we want to see and then filling in the supporting objects: start from a statement of the problem and see where it goes. The alternative is to write the code directly in the tests, and then refactor out any clusters of behavior.

Assertions and Expectations

The assertions and expectations of a test should communicate precisely what matters in the behavior of the target code. Tests that assert too much detail makes them difficult to read and brittle when things change.

For the expectations and assertions we write, we try to keep them as narrowly defined as possible. We check only the strike price and ignore the rest of the values as irrelevant in that test. In other cases, we’re not interested in all of the arguments to a method, so we ignore them in the expectation. We define an expectation that says that we care about the Sniper identifier and message, but that any RunTimeException object will do for the third argument:

Literals and Variables

Test code tends to be more concrete than production code, which means it has more literal values. Literal values without explanation can be difficult to understand because the programmer has to interpret whether a particular value is significant (eg., just outside allowed range) or just an arbitrary place holder to trace behavior (e.g., should be doubled and passed on to a peer). A literal value does not describe its role, although there are some techniques for doing so.

One solution is to allocate literal values to variables and constants with names that describe their function. For example: UNUSED_CHAT = nil to show that we are using null to represent an argument that was unused in the target code. We weren’t expecting the code to receive null in production, but it turns out that we don’t care and it makes testing easier. Similarly, a team might develop conventions for naming common values, such as INVALID_ID = 007 

We name variables to show the roles these values or objects play in the test and their relationships to the target object.

Test Data Builders

Use builder pattern to build instances in tests, most often for values. For a class that requires complex setup, we create a test data builder that has a field for each constructor parameter, initialized to a safe value. Tests that just need an Order object and are not concerned with its contents can create one in a single line. Tests that need particular values within an object can specify just those values that are relevant and use defaults for the rest. This makes the test more expressive because it includes only the values that are relevant to the expected results. Test data builders help keep tests expressive and resilient to change. They make the default case simple and special cases easy. They protect the test against changes in the structure of its objects. If we add an argument to a constructor, then all we have to change is the relevant builder and those tests that drove the need for the new argument.

A final benefit is that we can write test code that’s easier to read and spot errors, because each builder method identifies the purpose of its parameter.

Creating Similar Objects

Initialize each builder with the common state and then, for each object to be built, define the differing values and call its build method. This produces a more focused test with less code. We can name the builder after the features that are common, and the domain objects after their differences.

Combining Builders

Where a test data builder for an object uses other built objects, we can pass in those builders as arguments rather than their objects. This will simplify the test code by removing the build methods. The result is easier to read because it emphasizes the important information - what is being built, rather than the mechanics of building it.

Emphasizing the Domain Model with Factory Methods

We can further reduce the noise in the test code by wrapping up the construction of the builders in factory methods.

Removing Duplication at the Point of Use

We’ve made the process of assembling complex objects for tests simpler and more expressive by using test data builders. How can we structure our tests to make the best use of these builders in context? We often find ourselves writing tests with similar code to create supporting objects and pass them to the code under test, so we want to clean up this duplication.

First Remove Duplication

Think a bit harder about what varies between tests and what is common, and realize that a better alternative is to pass the builder through, not its arguments; it’s similar to when we started combining builders. The helper method can use the builder to add any supporting detail to the order before feeding it into the system.

Then Raise the Game

The test code is looking better, but it still reads like a script. We can change its emphasis to what behavior is expected, rather than how the test is implemented, by rewording some of the names. We started with a test that looked procedural, extracted some behavior into builder objects, and ended up with a declarative description of what the feature does. We’re nudging the test code towards the sort of language we could use when discussing the feature with someone else, even someone non-technical; we push everything else into supporting code.

Communication First

We use test data builders to reduce duplication and make the test code more expressive. It’s another technique that reflects our obsession with the language of code, driven by the principle that code is there to be read. Combined with factory methods and test scaffolding, test data builders help us write more literate, declarative test that describe the intention of a feature, not just a sequence of steps to drive it.

Using these techniques, we can even use higher-level tests to communicate directly with non-technical stakeholders. We can use the tests to help us narrow down exactly what a feature should do and why.

</Text>
        </Document>
        <Document ID="100">
            <Title>Fibonacci</Title>
            <Text>The first test shows that fib(0) = 0. The implementation returns a constant.
The second test shows that fib(1) = 1

I just put the second assert in the same method because there didn’t seem to be any substantial communication value to writing test_fibonacci_of_one_is_one

There are several ways I could go to making this run. I’ll choose to treat 0 as a special case: if n == 0 return 0; return 1;

The duplication in the test case is starting to bug me. We can factor out the common structure of assertions by driving the test from a table of input and expected values. BP: I would extract this into custom assertion and remove login in test code.

Now adding the next case requires 6 keystrokes and no additional lines:

Disconcertingly, the test works. It just so happens that our constant 1 is right for this case as well. On to the next test: 3,2

It fails. Applying the same strategy as before (treating smaller inputs as special cases), we write: if (n&lt;=2) return 1;

Now we are ready to generalize. We wrote 2, but we don’t really mean 2, we mean 1 + 1 return 1 + 1;

That first 1 is an example of fib(n-1) : return fib(n-1) + 1;

The second 1 is an example of fib(n-2): return fib(n-1) + fib(n-2)

Cleaning up now, the same structure should work for fib(2), so we can tighten up the second condition:


And there we have fibonacci, derived totally from the tests.

 The progression of a Lisp programmer - the newbie realizes that the difference between code and data is trivial. The expert realizes that all code is data. And the true master realizes that all data is code.</Text>
        </Document>
        <Document ID="97">
            <Title>xUnit Test Patterns</Title>
            <Text>Fundamental shift for many programmers.
Hiring someone who has the knowledge is the most time-efficient way of learning any new practice or technology.
I hope that by writing down a lot of these mistakes and suggesting ways to avoid them, I can save you lot of grief on your project. 

Some developers test the whole system as a single entity. Most developers prefer to test their software unit by unit. The units may be larger-grained components or they may be individual classes, methods or functions. The key thing that distinguishes these tests from the ones that the testers write is that the units being tested are a consequence of the design of the software, rather than being a direct translation of the requirements.

A small percentage of the unit tests may correspond directly to the business logic described in the requirements and the customer tests, but a large majority tests the code that surrounds the business logic.

The pitfalls associated with record-and-playback automated testing are: behavior sensitivity, interface sensitivity, data sensitivity and context sensitivity.

Behavior Sensitivity

If the behavior of the system is changed (due to requirements change), any tests that exercise the modified functionality will most likely fail when replayed.

Interface Sensitivity

Testing the business logic inside the SUT via the user interface is a bad idea. Even minor changes to the interface can cause tests to fail.

Data Sensitivity

All tests assume some starting point, called the test fixture; this test context is sometimes called the “pre-conditions” or “before picture” of the test. Mostly this test fixture is defined in terms of data that is already in the system. If the data changes, the tests may fail.

Context Sensitivity

The behavior of the system may be affected by the state of things outside the system. These external factors could include the states of the devices, other applications, system clock etc. Any tests that are affected by this context will be difficult to repeat deterministically without getting control over the context.

Overcoming the Four Sensitivities

The xUnit family of test automation frameworks gives us a large degree of control; we just have to learn how to use it effectively.

Uses of Automated Tests

Executable Specification

TDD is more about specification of the behavior of the software yet to be written than it is about regression testing. The effectiveness of TDD comes from the way it lets us separate our thinking about software into two separate phases: what it should do, and how it should do it. We do “continuous design”. 

Taking this to the extreme results in “emergent design”, where very little design is done upfront. But development does not have to be done that way. We can combine high-level design (or architecture) upfront with detailed design on a feature-by-feature basis. Either way, it can be useful to delay thinking about how to achieve the behavior of a specific class or method for a few minutes while we capture what that behavior should be in the form of an executable specification. We focus on one thing at a time.

Once we have finished writing the tests and verifying that they fail as expected, we can switch our perspective and focus on making them pass. The tests are now acting as a progress measurement. If we implement the functionality incrementally, we can see each test pass one by one as we write more code. As we work, we keep running all of the previously written tests as regression tests to make sure our changes have not had any unexpected side effects. This is where the true value of automated unit testing lies: in its ability to ‘pin down’ the functionality of the SUT so that the functionality is not changed accidentally. 

Refactoring

Refactoring is a highly disciplined approach to changing the design without changing the behavior of the code. It goes hand-in-hand with automated testing because it is very difficult to do refactoring without having the safety net of automated tests to prove that you have not broken anything during redesign.

Refactoring a Test

Why Refactor Tests?

Tests can quickly become a bottleneck in an agile development process. There is a difference between simple, easily understood tests and complex, obtuse, hard-to-maintain tests. 

Minimal Test Strategy

This consists of five parts:

Development Process: How the process we use to develop the code affects our tests.
Customer Tests: The first tests written to define “what done looks like”
Unit Tests: The tests that help our design emerge incrementally and ensure that all our code is tested.
Design for Testability: The patterns that make our design easier to test, thereby reducing the cost of test automation.
Test Organization: How we can organize our Test Methods and Testcase classes.

Development Process

Writing test first gives us an agreed-upon definition of what success looks like. We do story test-driven development by first automating a suite of customer tests that verify the functionality provided by the application. To ensure that all of our software is tested, we augment these tests with a suite of unit tests that verify all code paths or at a minimum all the code paths that are not covered by the customer tests. We can use code coverage tools to discover which code is not being exercised and then retrofit unit tests to accommodate the untested code.

We will find fewer missing unit tests when we practice TDD. There is still value in running the code coverage tools with TDD. By organizing the unit tests and customer tests into separate test suites we can run them separately.

The unit tests should pass before we check-in. This is what we mean by “keep the bar green”.  Although many of the customer tests will fail until the corresponding functionality is built, it is nevertheless useful to run all the passing customer tests as part of the integration build phase - but only if this step does not slow the build down too much. In that case, we can leave them out of the check-in build and simply run them every night.

Keep the dependency on the database to a minimum in the unit tests for the business logic.

Customer Tests

The customer tests should capture the essence of what the customer wants the system to do. We can also use well-written tests as documentation to identify how the system is supposed to work. If our application interacts with other applications, we can isolate it from any applications that we do not have in our development environment by using Test Double for the objects that act as interfaces to the other applications.

If the tests run slowly because of dependencies we can replace them with functionally equivalent fake objects to speed up our tests. 

Unit Tests

Each unit test should be fully automated test that does a round-trip test against a class through its public interface. We can strive for defect localization by ensuring that each test is a single condition test that exercises a single method or object in a single scenario. We should also write our test so that each part of the four phases of a test is easily recognizable which enables us to use the tests as documentation. If several tests are expected to result in the same outcome, we can factor out the verification logic into an outcome-describing verification method (custom assertion) that the reader easily recognize.

If we have untested code because we cannot find a way to execute the path through the code, we can use a test stub to gain control of the indirect inputs of the SUT.

If there are untested requirements because not all the system’s behavior is observable via its public interface, we can use a mock object to intercept and verify the indirect outputs of the SUT.

Design for Testability

Automated testing is simpler if we adopt a Layered Architecture. At a minimum we should separate our business logic from the database and the user interface, thereby enabling us to test it easily using either Subcutaneous Test or Service Layer Tests (see Layer Test). We can minimize any dependence on a database by doing most of our testing using in-memory objects. This also helps us to avoid slow tests by reducing disk I/O. 

Keep the UI logic out of the visual classes. It allows us to write unit tests for the UI logic without having to instantiate UI objects or framework they depend on.

If we build components that will be reused by other projects, we can augment the unit tests with component tests that verify the behavior of each component in isolation. We may need to use Test Doubles to replace any components on which our component depends. We can use dependency injection to install the Test Doubles at run time.

Test Organization

If we end up with too many test methods on our test case class, we can split the class based on either the methods (or features) verified by the tests or their fixture needs. 




</Text>
        </Document>
        <Document ID="98">
            <Title>Refactoring</Title>
            <Text>Reconcile Differences

How do you unify two similar looking pieces of code? Gradually bring them closer. Unify them only when they are absolutely identical.

Isolate Change

How do you change one part of a multi-part method or object? First, isolate the part that has to change.

Migrate Data

How do you move from one representation? Temporarily duplicate the data.</Text>
        </Document>
        <Document ID="129">
            <Title>Test Diagnostics</Title>
            <Text>Design to Fail

The point of a test is not to pass but to fail. We want the production code to pass its tests, but we also want the tests to detect and report any errors that do exist. A failing test has actually succeeded at the job it was designed to do. Even unexpected test failures, in an area unrelated to where we are working, can be valuable because they reveal implicit relationships in the code that we hadn’t noticed.

One situation we want to avoid, however, is when we can’t diagnose a test failure that has happened. The last thing we should have to do is open a debugger and step through the tested code to find the point of disagreement. At a minimum, it suggests that our tests don’t yet express our requirements clearly enough. In the worst case, we can find ourselves in debug hell, with deadlines to meet but no idea of how long a fix will take. At this point, the temptation will be high to just delete the test - and lose our safety net.

Stay Close to Home

Synchronize frequently with the source code repository - up to every few minutes - so that if a test fails unexpectedly it won’t cost much to revert your recent changes and try another approach.

The other implication of this tip is not to be too inhibited about dropping code and trying again. Sometimes it’s quicker to roll back and restart with a clear head than to keep digging.

Make tests fail informatively. If a failing test clearly explains what has failed and why, we can quickly diagnose and correct the code. Here are some practices that give us the information we need at runtime.

Small, Focused, Well-Named Tests

The easiest way to improve diagnostics is to keep each test small and focused and give tests readable names. If a test is small, its name should tell us most of what we need to know about what has gone wrong.

Explanatory Assertion Messages

The message should describe the cause instead of the symptom: comparison failure : expected: 10 but was 15.
Add a message to identify the value being asserted: comparison failure: outstanding balance expected: 10 but was: 15

Highlight Detail with Matchers

Failure report should show exactly which values are relevant.

Self Describing Value

An alternative to adding detail to the assertion is to build the detail into values in the assertion. We can take this in the same spirit has the idea that comments are a hint that the code needs to be improved: if we have to add detail to an assertion, maybe that’s a hint that we could make the failure more obvious.

In the customer example, we could improve the failure message by setting the account identifier in the test Customer to the self-describing value “a customer account id”: comparison failure: expected: [a customer account id] but was [id not set]

Now we don’t need to add an explanatory message, because the value itself explains its role.

We might be able to do more when we’re working with reference types. For example, in a test that has this setup: start_date = Date.new(1000); end_date = Date.new(2000) the failure message reports that a payment date is wrong but doesn’t describe where the wrong value might have come from: payment date Expected: xyz got: abc

What we really want to know is the meaning of these dates. If we force the display string: start_date = NamedDate.new(1000, “start date”); end_date = NamedDate.new(2000, “end date”); we get a message that describes the role that each date plays: payment date: Expected &lt;start date> got: &lt;end date> which makes it clear that we’ve assigned the wrong filed to the payment date. This is yet another motivation for defining more domain types to hide the basic types in the language. It gives us somewhere to hang useful behavior like this.

Obviously Canned Value

Sometimes, the values being checked can’t easily explain themselves. There’s not enough information in a char or int, for example. One option is to use improbable values that will be obviously different from the values we would expect in production. For an int, we might use a negative value (if that doesn’t break the code) or MAX_VALUE (if it’s wildly out of range). Similarly, for dates you could use values that is not expected in your production code. Team can develop conventions for common values, it can ensure that they stand out. INVALID_ID = 123 would be obviously wrong if the real system ids were 5 digits and up.

Tracer Object

Sometimes we just want to check that an object is passed around by the code under test and routed to the appropriate collaborator. We can create a tracer object, a type of Obviously Canned Value, to represent this value. A tracer object is a dummy object that has no supported behavior of its own, except to describe its role when something fails. 

Tracer objects can be a useful design tool when TDD’ing a class. We sometimes use an empty interface to mark (and name) a domain concept and show how it’s used in a collaboration. Later, as we grow the code, we fill in the interface with methods to describe its behavior. 

Explicitly Assert That Expectations Were Satisfied

A test that has both expectations and assertions can produce a confusing failure. In some mock object frameworks, the expectations are checked after the body of the test. If, for example, a collaboration doesn’t work properly and returns a wrong value, an assertion might fail before any expectations are checked. This would produce a failure report that shows, say, an incorrect calculation result rather than the missing collaboration that actually caused it.

In a few cases, then, its worth calling the assert_is_satisfied method on the Mockery before any of the test assertions to get the right failure report. This demonstrates why it is important to watch the test fail. If you expect the test to fail because an expectation is not satisfied but a post condition assertion fails instead, you will see that you should add an explicit call to assert that all expectations have been satisfied.

Diagnostics Are a First Class Feature

Follow the four-step TDD cycle (fail, report, pass, refactor) because that’s how we know we’ve understood the feature - and whoever has to change it later will also understand it. 

Fig 23.1 shows that we need to maintain the quality of tests, as well as the production code. Improve the diagnostics as part of the TDD cycle.

</Text>
        </Document>
        <Document ID="101">
            <Title>xUnit Test Patterns Glossary</Title>
            <Text>Direct Input

A test may interact with the SUT directly via its public API or indirectly via its back door. The stimuli injected by the test into the SUT via its public API are direct inputs of the SUT. Direct inputs may consist of method calls to another component or messages sent on a message channel and the arguments or contents.

Indirect Input

When the behavior of the SUT is affected by the values returned by another component whose services it uses, we call those values indirect inputs of the SUT. Indirect inputs may consist of return values of functions and any errors or exceptions raised by the DoC. Testing of the SUT behavior with indirect inputs requires the appropriate control point on the back side of the SUT. We often use a test stub to inject the indirect inputs into the SUT.

Direct Output

A test may interact with the SUT directly via its public API or indirectly via its back door. The responses received by the test from the SUT via its public API are direct outputs of the SUT. Direct outputs may consist of the return values of method calls, updated arguments passed by reference, exceptions raised by the SUT or messages received on a message channel from the SUT.

Indirect Output

When the behavior of the SUT includes actions that cannot be observed through the public API of the SUT but that are seen or experienced by other systems or application components, we call those actions the indirect outputs of the SUT. Indirect outputs may consist of calls to another component, messages sent on a message channel and records inserted into a database or written to a file. Verification of the indirect output behaviors of the SUT requires the use of appropriate observation points on the back side of SUT. Mock objects are often used to implement the observation point by intercepting the indirect outputs of the SUT and comparing them to the expected values.

Back Door

An alternative interface to a SUT that test software can use to inject indirect inputs into the SUT. A database is a common example of a back door, but it could also be any component that can be either manipulated to return test-specific values or replaced by a test double. 

Front Door

Public API.

User Acceptance Test

A customer test that the customer of the software plans to run to help the customer decide whether he or she will accept the software system. Acceptance tests are usually run manually after all automated customer tests have passed. They exercise all layers of the system - from the user interface back to the database. And should include any integration with other systems on which the application depends.

Customer Test

A test that verifies the behavior of a slice of the visible functionality of the overall system. The SUT may consist of the entire system or a fully functional top-to-bottom slice of the system. A customer test should be independent of the design decisions made while building the SUT. That is, we should require the same set of customer tests regardless of how we choose to build the SUT. Of course, how the customer tests interact with the SUT may be affected by high-level software architecture decisions.

Need-Driven Development

A variation of TDD process where code is written from the outside in and all depended-on code is replaced by mock objects that verify the expected indirect outputs of the code being written. This approach ensures that the responsibilities of each software unit are well understood before they are coded, by virtue of having unit tests inspired by examples of real usage. The outermost layer of software is written using storytest-driven development. It should have examples of usage by real clients (eg., a user interface driving the service facade) in addition to customer tests.

A refinement of endoscopic testing in which the dependencies of the SUT are defined as the tests are written. This outside-in approach to writing and testing software combines the conceptual elegance of the traditional top-down approach to writing code with modern TDD techniques supported by mock objects. It allows us to build and test the software layer by layer, starting at the outermost layer before we have implemented the lower layers.

Need driven development combines the benefits of TDD (specifying all software with tests before we build them) with a highly incremental approach to design that removes the need for any speculation about how a depended-on class might be used.

Behavior Driven Development

A variation of the TDD process wherein the focus of the tests is to clearly describe the expected behavior of the SUT. The emphasis is on tests as documentation rather than merely using tests for verification.

Example Driven Development

A reframing of the TDD process to focus on the executable specification aspect of the tests. The act of providing examples is more intuitive to many people. It doesn’t carry the baggage of testing software that doesn’t yet exist.

Component

A larger part of the overall system that is often separately deployable. Component based development involves decomposing the overall functionality into a series of individual components that can be built and deployed separately. This allows sharing of the components between applications that need the same functionality. A service-oriented architecture uses Web Services as its large-grained components. A component is verified using component tests before the overall application is tested using customer tests.

Depended on Component

And individual class or a large-grained component on which the SUT depends. The dependency is usually one of delegation via method calls. In test automation, the DoC is primarily of interest in that we need to be able to observe and control the interactions with the SUT to get complete test coverage.

Control Point

How the test asks the SUT to do something to it. A control point could be created for the purpose of setting up or tearing down the fixture or it could be used during the exercise SUT phase of the test. It is a kind of interaction point. Some control points are provided strictly for testing purposes; they should not be used by the production code because they bypass input validation or short-circuit the normal life cycle of the SUT or some object on which it depends.

Interaction Point

A point at which a test interacts with the SUT. An interaction point can either be a control point or an observation point.

Observation Point

The means by which the test observes the behavior of the SUT. This kind of interaction point can be used to inspect the post-exercise state of the SUT or to monitor interactions between the SUT and its depended-on components. Some observation points are provided strictly from the tests; they should not be used by the production code because they may expose private implementation details of the SUT that cannot be depended on not to change.

Outgoing Interface

A component (eg., a class or a collection of classes) often depends on other components to implement its behavior. The interfaces it uses to access these components are known as outgoing interfaces, and the inputs and outputs transmitted via test interfaces are called indirect inputs and indirect outputs. Outgoing interfaces may consist of calls to another component, messages sent on a message channel or records inserted into a database or written to a file. Testing the behavior of the SUT with outgoing interfaces requires special techniques such as mock objects to intercept and verify the usage of outgoing interfaces.

Retrospective

A process whereby a team reviews its processes and performance for the purpose of identifying better ways of working. Retrospectives are often conducted at the end of a project (called a project retrospective) to collect data and make recommendations for future projects. They have more impact if they are done regularly during the project. Agile projects tend to do retrospectives after at least every release (called a release retrospective) and often after every iteration (called an iteration retrospective).

Round Trip Test

A test that interacts only via the public interface of the SUT.

Layer Crossing Test

A test that either sets up the fixture or verifies the outcome using a back door of the SUT such as the database. Contrast this with round-trip test.

Substitutable Dependency

A software component my depend on any number of other components. If we are to test this component by itself, we must be able to replace the other components with test doubles - that is, each component must be a substitutable dependency. We can turn something into a substitutable dependency in several ways including dependency injection, test specific subclass etc.

Design for Testability 

A way of ensuring that code is easily tested by making sure that testing requirements are considered as the code is designed. When doing TDD, design for testability occurs as a natural side effect of development.

System Under Test

Whatever thing we are testing. The SUT is always defined from the perspective of the test. When we are writing unit tests, the SUT is whatever class, object or method we are testing. When we are writing customer tests, the SUT is the entire application or at least a major subsystem of it. The parts of the application we are not verifying in this particular test may still be involved as a depended-on component.

Test Condition

A particular behavior of the SUT that we need to verify. It can be described as the following collection of points:
If the SUT is in some state S1 and
We exercise the SUT in some way X, then
The SUT should respond with R and 
The SUT should be in state S2

Equivalence Class

A test condition identification technique that reduces the number of tests required by grouping together inputs that should result in the same output or that should exercise the same logic of the system. This organization allows us to focus our tests on key boundary values at which the expected output changes.

Assertion 

A statement that something should be true. It fails when the actual outcome passed to it does not match the expected outcome.

Test Fixture

All the things we need to have in place to run a test and expect a particular outcome. The test fixture comprises the pre-conditions of the test. It is the before picture of the SUT and its context.

Fixture Setup

Before the desired logic of the SUT can be exercised, the pre-conditions of the test need to be set up. Collectively, all objects (and their states) are called the test fixture (or test context), and the phase of the test that sets up the test fixture is called fixture setup.

Test Context

Everything a SUT needs to have in place so that we can exercise the SUT for the purpose of verifying its behavior. For this reason, RSpec calls the test fixture a context.
Context: A set of fruits with contents = {apple, orange, pear}
Exercise: Remove orange from the fruits set
Verify: Fruits set contents = {apple, pear}

In this test the fixture consists of a single set and is created directly in the test.

Exercise SUT

After the fixture setup phase of testing, the test stimulates the SUT logic that is to be tested. This phase of the testing process is called exercise SUT.

Expectation 

What a test expects the SUT to have done. When we are using mock objects to verify the indirect outputs of the SUT, we load each mock object with the expected method calls (including the expected arguments); these are called the expectations.

Expected Outcome

The outcome that we verify after exercising the SUT. A self checking test verifies the expected outcome using calls to assertion methods.

Result Verification

After the exercise of SUT phase of the 4-phase test, the test verifies that the expected (correct) outcome has actually occurred. This phase of the test is called result verification.

Verify Outcome

After the exercise SUT phase of the test, the test compares the actual outcome - including returned values, indirect outputs and the post-test state of the SUT - with the expected outcome. This phase of the test is called the verify outcome phase.

Fixture Teardown

After a test is run, the test fixture that was built by the test should be destroyed. This phase of the test is called fixture teardown.

Test Driven Bug Fixing

A way of fixing bugs that entails writing and automating unit tests that reproduce each bug before we begin debugging the code and fixing the bug.

Emergent Design

The opposite of big design up front. It involves letting the right design be discovered as the software slowly evolves to pass one test at a time during TDD.

False Negative

A situation in which a test passes even though the SUT is not working properly.

False Positive

A situation in which a test fails even though the SUT is working properly.

Fault Insertion Test

A kind of test in which a deliberate fault is introduced in one part of the system to verify that another part reacts to the error appropriately. Replacing a DoC with a Saboteur that throws an exception is an example of a software fault insertion test.</Text>
        </Document>
        <Document ID="99">
            <Title>Mastering TDD</Title>
            <Text>How large should your steps be?

There are really two questions lurking here:
How much ground should each test cover?
How many intermediate stages should you go through as you refactor?

The tendency of TDDers over time has been smaller steps. 

What don’t you have to test?

Write tests until fear is transformed into boredom. You should test: conditionals, loops, operations, polymorphism. But only those that you write. Unless you have a reason to distrust it, don’t test code from others. 

Sometimes I will document the presence of unusual behavior in external code with a test that will fail if the bug is ever fixed.

How do you know if you have good tests?

Here are some attributes of tests suggesting a design in trouble:

Long Setup Code

If you have to spend a hundred lines creating the objects for one simple assertion, something is wrong. Your objects are too big and need to be split.

Setup Duplication

If you can’t easily find a common place for common setup code there are too many objects too tightly intertwingled.

Long Running Tests

TDD tests that run a long time won’t be run often, and often haven’t been run for a while, and probably don’t work. Worse than this, though, they suggest that testing the bits and pieces of the application is hard. This is a design problem and needs to be addressed with design.

Fragile Tests

Tests that break unexpectedly suggest that one part of the application is surprisingly effecting another part. You need to design until the effect at a distance is eliminated, either by breaking the connection or by bringing the two parts together.

How does TDD lead to frameworks?

I’ve been test driving development long enough that I can recover from most of my mistakes faster than you can recognize I’ve made them.

How much feedback do you need?

How many tests should you write? Here’s a simple problem - given three integers representing the length of the sides a triangle, return:

1 - equilateral
2 - isoceles
3 - scalene

And throw an exception if the triangle is not well formed. I wrote 6 tests. Bob Binder wrote 65. You’ll have to decide from experience and reflection, about how many tests you want to write.

TDD’s view of testing is pragmatic. In TDD, the tests are a means to an end, the end being code in which we have great confidence. 

When should you delete tests?

More tests is better, but if two tests are redundant with respect to each other, should you keep them both around?

The first criterion for your tests is confidence. Never delete a test if it reduces your confidence in the behavior of the system.

The second criterion is communication. If you have two tests that exercise the same path through the code, but they speak to different scenarios for a reader leave them alone.

If you have two tests that are redundant with respect to confidence and communication, delete the least useful of the two.


</Text>
        </Document>
        <Document ID="102">
            <Title>Test Smells</Title>
            <Text>What to do about smells?

A very effective technique for identifying the root cause is the “Five Why’s”. First, we ask why something is occurring. Once we have identified the factors that led to it, we next ask why each of those factors occurred. We repeat this process until no new information is forthcoming. In practice, asking why five times is usually enough - hence the name “Five Why’s”. This is also called root cause analysis.

The root causes of fragile tests can be classified into four broad categories:

Interface Sensitivity occurs when tests are broken by changes to the test programming API or the user interface used to automate the tests. Record and Playback Test tools typically interact with the system via the UI. Even minor changes to the interface can cause tests to fail.

Behavior Sensitivity occurs when tests are broken by changes to the behavior of the SUT. Of course, the test should break if we change the SUT. But the issue is that only a few tests should be broken by any one change. If many tests break we have a problem.

Data Sensitivity occurs when tests are broken by changes to the data already in the SUT. This issue is particularly a problem for applications that use databases. 

Context Sensitivity occurs when test are broken by differences in the environment surrounding the SUT. The most common example is when tests depend on the time but this problem can also arise when tests rely on the state of devices such as server, printers, monitors etc.

Tests should be simple, linear sequences of statements. When tests have multiple execution paths, we cannot be sure exactly how the test will execute in a specific case. </Text>
        </Document>
        <Document ID="103">
            <Title>Goals of Test Automation</Title>
            <Text>High level objectives:

Tests should help us improve quality
Tests should help us understand the SUT
Tests should reduce and not introduce risk
Tests should be easy to run
Tests should be easy to write and maintain
Tests should require minimal maintenance as the system evolves around them

The first three objectives demonstrate the value provided by the tests, whereas the last three objectives focus on the characteristics of the tests themselves. 

Tests Should Help Us Improve Quality

Goal : Tests as Specification 

Executable Specification

TDD enable us to specify the behavior in various scenarios captured in a form that we can then execute. To ensure that we are building the right software, we must ensure that our tests reflect how the SUT will actually be used. This effort can be facilitated by developing user interface mockups that capture just enough detail about how the application appears and behaves so that we can write our tests.

The very act of thinking through various scenarios in enough detail to turn them into tests helps us identify those areas where the requirements are ambiguous or self-contradictory. Such analysis improves the quality of the specification, which improves the quality of the software.

Goal : Defect Localization

If our unit tests are small (i.e., we test only a single behavior in each one), we should be able to pinpoint the bug quickly based on which test fails. This specificity is one of the major advantages that unit tests enjoy over customer tests. The customer tests tell us that some behavior expected by the customer isn’t working; the unit tests tell us why. This is defect localization. If a customer test fails but no unit tests fail, it indicates a missing unit test.

Write tests for all possible scenarios that each unit of software needs to cover. If the tests contain bugs we cannot achieve any benefits. It is crucial that we keep the tests as simple as possible so that they can be easily seen to be correct. While writing unit tests for our unit tests is not a practical solution, we can and should write unit tests for any test utility method to which we delegate complex algorithms needed by the test methods.

Tests Should Help Us Understand the SUT

The tests can show the reader how the code is supposed to work. 

Goal: Tests as Documentation

Tests Should Reduce and Not Introduce Risk

One form of risk reduction involve verifying the software’s behavior in the impossible circumstances that cannot be induced when doing traditional customer testing of the entire application as a black box. 

Goal : Tests as Safety Net

The effectiveness of the safety net is determined by how completely our test verify the behavior of the system. Missing tests are like holes in the safety net. 

Goal: Do No Harm

No Test Risk

How might automated tests introduce risk? Avoid putting test-specific hooks into the SUT. Any test specific code should be plugged in by the test and only in the test environment. It should not exist in the SUT when it is in production.

A common mistake is replacing too much of the SUT with a test double. We must be clear about which SUT we are testing and avoid replacing the parts we are testing with test-specific logic.

Tests Should Be Easy to Run

They must be fully automated tests so they can be run without any effort.
They must be self checking tests so they can detect and report any errors without manual inspection
They must be repeatable tests so they can be run multiple times with the same result.
Ideally each test should be an independent test that can be run by itself.

Goal : Repeatable Test

Tests that run only in memory and that use only local variables or fields are usually repeatable. Unrepeatable tests are caused by using a shared fixture. In such a case, we must ensure that our tests are self-cleaning. 

Tests Should Be Easy to Write and Maintain

Coding is a difficult activity because we must keep a lot of information in our heads as we work. When we are writing tests, we should stay focused on testing rather than coding of the tests. This means that tests must be simple. Simple to read and simple to write. They need to be simple to read and understand because testing the automated tests themselves is a complicated endeavor. 

Everyone knows that debugging is twice as hard as writing a program in the first place.  So if you are as clever as you can be when you write it, how will you ever debug it?  ~Brian Kernighan

They can be tested properly only by introducing the very bugs that they are intended to detect in the SUT. This is hard to do in an automated way so it is usually done only once (if at all), when the test is first written. For these reasons, we need to rely on our eyes to catch any problems that creep into the tests, and that means we must keep the tests simple enough to read quickly.

Tests become complicated for two reasons:

We try to verify too much functionality in a single test.
Too large an expressiveness gap separates the test scripting language and the before/after relationships between domain concepts that we are trying to express in the test.

Goal : Simple Tests

Strive to verify one condition per test by creating a test method for each unique combination of pre-test state and input. Each test method should drive the SUT through a single code path. Customer tests are exception to this rule.

Goal : Expressive Tests

The expressiveness gap can be addressed by building up a library of test utility methods that constitute a domain-specific testing language. Such a collection of methods allows us to express the concepts that we wish to test without having to translate our thoughts into much more detailed code. 

Avoid duplication in tests. There is, however, a counterforce at play. Because the tests should communicate intent, it is best to keep the core test logic in each test method so it can be seen in one place. This doesn’t preclude moving a lot of supporting code into test utility methods where it needs to be modified in only one place if it is affected by a change in the SUT.

Goal : Separation of Concerns

1. We want to keep test code separate from our production code
2. We want each test to focus on a single concern

Test concerns separately to void obscure tests.

Tests Should Require Minimal Maintenance as the System Evolves Around Them

We write automated tests mostly to make change easier, so we should strive to ensure that our tests don’t inadvertently make change more difficult.

Goal : Robust Test

We want to write our tests in such a way that the number of tests affected by any one change is small. That means we need to minimize overlap between tests. We also need to ensure that changes to the test environment don’t affect our tests. We do this by isolating the SUT from the environment as much as possible.

Strive to verify one condition per test. Ideally, only one kind of change should cause a test to require maintenance. System changes that affect fixture setup or teardown code can be encapsulated behind test utility methods to reduce the number of tests affected by the change.
</Text>
        </Document>
        <Document ID="110">
            <Title>Organizing Tests</Title>
            <Text>Right-Sizing Test Methods

A test condition is something we need to prove the SUT really does; it can be described in terms of what the starting state of the SUT is, how we exercise the SUT, how we expect the SUT to respond, and what the ending state of the SUT is expected to be. A test method is a sequence of statements in our test DSL that exercises one or more test conditions. What should we include in a single test method?

Fig 12.1 the four phases of a typical test. Each test method implements a four phase test that ideally verifies a single test condition. Not all phases of the four phase test need be in the test method.

Verifying one condition per test gives good defect localization. That is, when a test fails they know exactly what is wrong in the SUT because each test verifies exactly one test condition. A test that verifies a single test condition executes a single code path through the SUT and it should execute exactly the same path each time it runs; that is what makes it a repeatable test. 

That means we need as many test methods as we have paths through the code - but how else an we expect to achieve full code coverage. What makes this pattern manageable is that we isolate the SUT when we write unit tests for each class so we only have to focus on paths through a single object. Also because each test should verify only a single path through the code, each test method should consist of strictly sequential statements that describe what should happen on that one path. 

A test method that contains conditional test logic is a sign of a test trying to accommodate different circumstances because it does not have control of all indirect inputs of the SUT or because it is trying to verify complex expected states on an inline basis within the test method.

Another reason we verify one condition per test is to minimize test overlap so that we have fewer tests to modify if we later modify the behavior of the SUT.
</Text>
        </Document>
        <Document ID="104">
            <Title>Philosophy of Test Automation</Title>
            <Text>Test after vs Test first
Test-by-test vs Test all-at-once
Outside-in vs Inside-out (applies independently to design and coding)
Behavior verification vs State verification
Fixture designed test-by-test vs Big fixture design upfront

Test First or Last?

When tests are written first and we write only enough code to make the tests pass, the production code tends to be more minimalist.

Examples are easier for people to envision writing before code than tests. Examples are executable and reveal whether the requirements have been satisfied. Executable specification mindset has lead to frameworks like RSpec.

State or Behavior Verification?

From writing code outside-in, it is but a small step to verifying behavior rather than just state. The statist view suggest that it is sufficient to put the SUT into a specific state, exercise it, and verify that the SUT is in the expected state at the end of the test. The behaviorist view says that we should specify not only the start and end states of the SUT, but also the calls the SUT makes to its dependencies. That is, we should specify the details of the calls to the outgoing interfaces of the SUT. These indirect outputs of the SUT are outputs just like the values returned by functions, except that we must use special measures to trap them because they do not come directly back to the client or test.

The behaviorist school of thought is sometimes called behavior-driven development. It is evidenced by the copious use of mock objects or test spies throughout the tests. Behavior verification does a better job of testing each unit of software in isolation, albeit at a possible cost of more difficult refactoring. 

Summary

Write the tests first
Tests are examples
Write tests one at a time. It is ok to list all the tests you can think of as skeletons upfront.
Outside-in development helps clarify which tests are needed for the next layer inward
Use primarily state verification. Resort to behavior verification when needed to get good code coverage.
Perform fixture design on a test-by-test basis.
</Text>
        </Document>
        <Document ID="111">
            <Title>A Roadmap to Effective Test Automation</Title>
            <Text>Test Automation Difficulty

Some kinds of tests are harder to write than others. This difficulty arises partly because the techniques are more involved and partly because they are less well known and the tools to do this kind of test automation are less readily available. The following common kinds of tests are listed in approximate order of difficulty, from easiest to most difficult:

1. Simple entity objects (domain model).
     Simple business classes with no dependencies
     Complex business classes with dependencies
2. Stateless service objects
     Individual components via component tests
     The entire business logic layer via layer tests
3. Stateful service objects
     Customer tests via a service facade using subcutaneous tests
     Stateful components via component tests
4. Hard to test code
      UI logic exposed via Humble Dialog
      Database logic
      Multi-threaded software
5. Object-oriented legacy software (software without any tests)
6. Non-object oriented legacy software

Roadmap to Highly Maintainable Automated Tests

Focus on learning to write the easier tests first before you move on to the more difficult kinds of tests. The techniques are introduced in the following sequence:

1. Exercise the happy path code
      Set up a simple pre-test state of the SUT
      Exercise the SUT by calling the method being tested
2. Verify direct outputs of the happy path
      Call assertion methods on the SUT’s responses
      Call assertion methods on the post-test state
3. Verify alternative paths
      Vary the SUT method arguments
      Vary the pre-test state of the SUT
      Control indirect inputs of the SUT via a test stub
4. Verify indirect output behavior
      Use mock objects or test spies to intercept and verify outgoing method calls
5. Optimize test execution and maintainability
      Make the tests run faster
      Make the tests easy to understand and maintain 
      Design the SUT for testability
      Reduce the risk of missed bugs

This is the likely order in which a project team might reasonably expect to learn about the techniques of test automation.

Exercise the Happy Path Code

To run the happy path through the SUT, we must automate one simple success test as a simple round-trip test through the SUT’s API. To get this test to pass, we might simply hard-code some of the logic in the SUT, especially where it might call other components to retrieve information it needs to make decisions that would drive the test down the happy path. Before exercising the SUT, we need to set up the test fixture by initializing the SUT to the pre-test state. As long as the SUT executes without raising any errors, we consider the test as having passed; at this level of maturity we don’t check the actual results against the expected results.

Verity Direct Outputs of the Happy Path

Once the happy path is executing successfully we can add result verification logic to turn our test into a self-checking test. This involves adding calls to assertion methods to compare the expected results with what actually occurred. We can easily make this change for any objects or values returned to the test by the SUT. We can also call other methods on the SUT or use public fields to access the post-test state of the SUT; we can then call assertion methods on these values as well.

Verify Alternative Paths

At this point the happy path through the code is reasonably well tested. The alternative paths through the code are still untested code. So the next step is to write test for these paths. The question to ask here is : What causes the alternative paths to be exercised? The most common causes are as follows:

Different values passed in by the client as arguments.
Different prior state of the SUT itself
Different results of invoking methods on components on which the SUT depends.

The first case can be tested by varying the logic in our tests that calls the SUT methods we are exercising and passing in different values as arguments. The second case involves initializing the SUT with a different starting state. 

Controlling Indirect Inputs

Because the responses from other components are supposed to cause the SUT to exercise the alternative paths through the code, we need to get control over these indirect inputs. We can do so by using a test stub that returns the value that should drive the SUT into the desired code path. As part of the fixture setup, we must force the SUT to use the stub instead of the real component. 

Many of these alternative paths result in successful outputs from the SUT; these tests are considered simple success tests and use a style of test stub called a Responder. Other paths are expected to raise errors or exceptions; they are considered expected exception tests and use a style of stub called a Saboteur.

Making Tests Repeatable and Robust

The act of replacing a real DoC with a test stub has a desirable side effect: It makes our tests robust and repeatable. By using a test stub we replace a possibly nondeterministic component with one that is completely deterministic and under test control. This is a good example of the isolate the SUT principle.

Verify Indirect Output Behavior

Thus far we have focused on getting control of the indirect inputs of the SUT and verifying readily visible direct outputs by inspecting the post-state test of the SUT. This kind of result verification is known as state verification. Sometimes, however, we cannot confirm that the SUT has behaved correctly simply by looking at the post-test state. That is, we may still have some untested requirements that can only be verified by doing behavior verification.

We can build on what we already know how to do by using one of the close relatives of the test stub to intercept the outgoing method calls from SUT. A test spy remembers how it was called so that the test can later retrieve the usage information and use assertion method calls to compare it to the expected usage. A mock object can be loaded with expectations during fixture setup, which it subsequently compares with the actual calls as they occur while the SUT is being exercised.
	Optimize Test Execution and Maintenance

At this point we should have automated tests for all the paths through our code. We may have less than optimal tests:
We may have slow tests
The tests may contain test code duplication that makes them hard to understand
We may have obscure tests that are hard to understand and maintain
We may have buggy tests that are caused by unreliable test utility methods or conditional test logic

Make the Tests Run Faster

Replace a DoC with a fake object that is functionally equivalent but executes much faster. Use of a fake object builds on the techniques we learned for verifying indirect inputs and outputs.	
Make the Tests Easy to Understand and Maintain

We can make obscure tests easier to understand and reduce test code duplication by refactoring out test methods to call test utility methods that contain any frequently used logic instead of doing everything on an in-line basis. 

Reduce the Risk of Missed Bugs

To reduce buggy tests or production bugs we can reduce the risk of false negatives (tests that pass when they shouldn’t) by encapsulating test logic. Use intent-revealing names for test utility methods. Verify the behavior of non trivial test utility methods using test utility tests.
</Text>
        </Document>
        <Document ID="105">
            <Title>Principles of Test Automation</Title>
            <Text>Write the Tests First

The unit tests save us a lot of debugging effort - effort that often fully offsets the cost of automating the tests.
Writing the tests before we write the code forces the code to be designed for testability. 

Design for Testability


Use the Front Door First

Objects have several kinds of interfaces. There is the public interface that clients are expected to use. There may also be a private interface. Many objects also have an outgoing interface consisting of the used part of the interfaces of any objects on which they depend. 

The types of interfaces we use influence the robustness of our tests. Overuse of behavior verification and mock objects can result in over specified software and test that are brittle and may discourage developers from doing  desirable refactorings.

When all choices are equally effective, we should use round-trip tests to test our SUT. To do so, we test an object through its public interface and use state verification to determine whether it behaved correctly. If this is not sufficient to accurately describe the expected behavior, we can make our tests layer-crossing test and use behavior verification to verify the calls the SUT makes to DoCs. If we must replace a slow or unavailable DoC with a faster test double, using a fake object is preferable because it encodes fewer assumptions into the test (the only assumption is that the component that the fake object replaces is actually needed).

Communicate Intent

Higher-Level Language , Single-Glance readable

Fully automated test need to be syntactically correct to compile and semantically correct to run successfully. They need to implement whatever detailed logic is required to put the SUT into the appropriate starting state and to verify that the expected outcome has occurred.

Tests that contain a lot of code or conditional test logic are usually obscure tests. They are much harder to understand because we need to infer the big picture from all the details. This reverse engineering of meaning take extra time whenever we need to revisit the test either to maintain it or to use the tests as documentation. It also increases the cost of ownership of the tests and reduces their return on investment.

Anything more than about 10 lines is getting to be too much. We can communicate intent by calling test utility methods with intent revealing names to set up our test fixture and to verify that the expected outcome has been realized. It should be readily apparent within the test method, how the test fixture influences the expected outcome of each test - that is which inputs result in which outputs. A rich library of test utility methods also makes tests easier to write because we don’t have to code the details into every test. It reduces cognitive load.

Don’t Modify the SUT

Suppose, for example, that we are writing test for object X,Y, and Z, where object X depends on object Y, which in turn depends on object Z. When writing test for X, it is reasonable to replace Y and Z with a test double. When testing Y, we can replace Z with a test double. When testing Z, however we cannot replace it with a test double because Z is what we are testing. This consideration is particularly salient when we have to refactor the code to improve its testability.

Another way of looking at this principle is as follows: the term SUT is relative to the test we are writing. In our “X uses Y uses Z” example, the SUT for some component tests might be the aggregate of X, Y, and Z; for unit testing purposes, it might be just X for some tests, just Y for other tests, and just Z for yet other tests. Just about the only time we consider the entire application to be the SUT is when we are doing user acceptance testing using the user interface and going all the way back to the database. Even here, we might be testing only one module of the entire application (eg., the customer management module). Thus SUT rarely equals application.

Keep Tests Independent

An independent test can be run by itself. It sets up its own fixture to put the SUT into a state that lets it verify the behavior it is testing. Test that build a fresh fixture are much more likely to be independent than test that use a shared fixture. With independent tests, unit test failures give us defect localization to help us pinpoint the source of the failure.

Isolate the SUT

Most software build on other software developed by us or by others. When our software depends on other software that may change over time our tests may start failing because the behavior of the other software has changed. This problem, which is called context sensitivity is a form of fragile test.

When our software depends on other software whose behavior we cannot control, we may find it difficult to verify that our software behaves properly with all possible return values.  This is likely to lead to untested code or untested requirements. To avoid this problem, we need to be able to inject all possible reactions of the DoC into our software under the complete control of our test.

Whatever application, component, class, or method we are testing, we should strive to isolate it as much as possible from all other parts of the software that we choose not to test. This isolation of elements allows us to test concerns separately and allows us to keep tests independent of one another. 

We can satisfy this principle by designing our software such that each piece of depended-on-software can be replaced with a test double using dependency injection. This design for testability makes our test more repeatable and robust.

Minimize test Overlap

Most applications have lots of functionality to verify. Proving that all of the functionality works correctly in all possible combinations and interaction scenarios is nearly impossible. Therefore, picking the tests to write is an exercise in risk management.

We should structure our test so that as few tests as possible depend on a particular piece of functionality. We do want to ensure that all test conditions are covered by the test that we do use. Each test condition should be covered by exactly one test - no more, no less. If it seems to provide value to test the code in several different ways, we may have identified several different test conditions.

Minimize Untestable Code

GUI components, multithreaded code and test methods are difficult to test using fully automated tests. All of these kinds of code share the same problem: they are embedded in a context that makes it hard to instantiate or interact with them from automated tests.

Untestable code wont’ have any fully automated tests to protect it from bugs. That makes it more difficult to refactor this code safely and more dangerous to modify existing functionality or introduce new functionality.

It is highly desirable to minimize the amount of untestable code that we have to maintain. We can refactor the untestable code to improve its testability by moving the logic we want to test out of the class that is causing the lack of testability. Test methods can have much of their untestable code extracted into test utility methods, which can then be tested.

When we minimize untestable code, we improve the overall test coverage of our code. In so doing, we also improve our confidence in the code and extend our ability to refactor at will. The fact that this technique improves the quality of the code is yet another benefit.

Keep Test Logic Out of Production Code

When the production code hasn’t been designed for testability, we may be tempted to put “hooks” into the production code to make it easier to test. These hooks typically take the form of if testing then .. And may either run alternative logic or prevent certain logic from running.

Testing is about verifying the behavior of a system. If the system behaves differently when under test, then how can we be certain that the product code actually works? Also it should not contain any test logic. 

A well-designed system (from a testing perspective) is one that allows for the isolation of functionality. 

Verify One Condition Per Test
Single-Condition Test

Many tests require a starting state other than the default state of the SUT, and many operations of the SUT leave it in a different state from its original state. There is a strong temptation to reuse the end state of one test condition as the starting state of the next test condition by combining the verification of the two test conditions into a single test method because it takes testing more efficient. This approach is not recommended, because when one assertion fails, the rest of the test will not be executed. As a consequence, it becomes more difficult to achieve defect localization.

With automated tests, a single failed assertion will cause the test to stop running and the rest of the test will provide no data on what works and what doesn’t.

We design each test to have four distinct phases that are executed in sequence: fixture setup, exercise SUT, result verification and fixture teardown.

In the first phase, we set up the test fixture (the before picture) that is required for the SUT to exhibit the expected behavior as well as anything we need to put in place to observe the actual outcome (such as using a test double).

In the second phase, we interact with the SUT to exercise whatever behavior we are trying to verify. This should be a single, distinct behavior; if we try to exercise several parts of the SUT, we are not writing a single condition test.

In the third phase, we do whatever is necessary to determine whether the expected outcome has been obtained and fail the test if it has not.

In the fourth phase, we tear down the test fixture and put the world back into the state in which we found it.

Note that there is a single exercise SUT phase and a single result verification phase. We avoid having a series of such alternating calls (exercise, verify, exercise, verify) because that approach would be trying to verify several distinct conditions - something that is better handled via distinct test methods.

What do we mean by one condition? You could have one assertion per test and name the test based on what the one assertion is verifying. Having one assertion per test makes such naming very easy but also leads to many more test methods if we have to assert on many output fields. Of course, we can comply by extracting a custom assertion or verification method that allows us to reduce the multiple assertion method calls to a single call. Sometimes that approach makes the test more readable. When it doesn’t we can violate the single assertion rule.

Test Concerns Separately

Testing our concerns separately allows a failure to tell us that we have a problem in a specific part of our system rather than simply saying that we have a problem somewhere. This approach to testing also makes it easier to understand the behavior now and to separate the concerns in subsequent refactorings. That is, we should just be able to move a subset of the tests to a different test case class that verifies the newly created class; it shouldn’t be necessary to modify the test much more than changing the class name of the SUT.

Ensure Commensurate Effort and Responsibility

The amount of effort it takes to write or modify tests should not exceed the effort it takes to implement the corresponding functionality.

</Text>
        </Document>
        <Document ID="112">
            <Title>Code Smells</Title>
            <Text>Obscure Test

Long Test, Complex Test, Verbose Test

It is difficult to understand the test at a glance

Automated tests should serve at least two purposes. First they should act as documentation of how the SUT should behave (tests as documentation). Second, they should be a self-verifying executable specification. These two goals are often contradictory because the level of detail needed for tests to be executable may make the test so verbose as to be difficult to understand.

The  first is issue is the high test maintenance cost. The second issue is it may allow bugs to slip through because of hidden test coding errors. This can result in buggy tests. Further more, a failure of one assertion in an eager test may hide many more errors that aren’t run, leading to a loss of test debugging data.

Keep test code clean and simple. Avoid “just do it in-line” mentality when writing tests. Putting code in-line results in large, complex test methods.

Eager Test: The test verifies too much functionality in a single test method.
Mystery Guest: The reader is not able to see the cause and effect between fixture and verification logic because part of it is done outside the test method.

The problem of Verbose Tests - tests that use too much code to say what they need to say - can be further broken down into a number of root causes:
General Fixture : The test builds or references a larger fixture than is needed to verify the functionality in question.
Irrelevant Information: The test exposes a lot of irrelevant details about the fixture that distract the reader from what really affects the behavior of the SUT.
Hard-Coded Test Data: Data values in the fixture, assertions, or arguments of the SUT are hard-coded in the test method, obscuring cause - effect relationships between inputs and expected outputs.
Indirect Testing: The test method interacts with the SUT indirectly via another object, thereby making the interactions more complex.

Eager Test

The test goes on and on verifying this, that and ‘everything but the kitchen sink’. It is hard to tell which part is fixture setup and which part is exercising the SUT.

Single condition tests provide good defect localization.

Mystery Guest

Tests require passing data to the SUT. The data used in the fixture setup and exercise SUT phases of the Four-Phase Test define the pre-conditions of the SUT and influence how it should behave. The post-conditions (the expected outcomes) are reflected in the data passed as arguments to the assertion methods in the verify outcome phase of the test.

When either the fixture setup or the result verification part of a test depends on information that is not visible within the test and the reader finds it difficult to understand the behavior that is being verified without first finding and inspecting the external information.

It can take many forms:

A filename of an existing external file is passed to a method of the SUT; the contents of the file should determine the behavior of the SUT.
The contents of a database record identified by a literal key are read into a object that is then used by the test or passed to the SUT.
A setup decorator is used to create a shared fixture, and objects in this fixture are then referenced via variables within the result verification logic.

Use fresh fixture built using in-line setup. When applied to the file example, this would involve creating the contents of the file as a string within our test so that the contents are visible and then writing them out to the file system. To avoid irrelevant information, we may want to hide the details of the construction behind one or more creation methods that append to the file’s contents.

If we must use external resources such as files, we should put them into a special directory and give them names that make it obvious what kind of data they hold.

General Fixture

There seems to be a lot of test fixture being built - much more than would appear to be necessary for any particular test. It is hard to understand the cause-effect relationship between the fixture, the part of the SUT being exercised, and the expected outcome of a test. 

We need to move to a minimal fixture to address this problem. To do so, we can use a fresh fixture for each test.

Irrelevant Information

It is hard to determine which of the values passed to objects actually affect the expected outcome.

Fixture setup logic may seem very long and complicated as it weaves together many interrelated objects. This makes it hard to determine what the test is verifying because the reader doesn’t understand the pre-conditions of the test.

The code that verifies the expected outcome of a test can also be too complicated to understand.

A test contains a lot of data. Irrelevant information often occurs in conjunction with hard-coded test data or a general fixture but can also arise because we make visible all data the test needs to execute rather than focusing on the data the test needs to be understood. When writing tests, the path of least resistance is to use whatever methods are available (on the SUT and other objects) and to fill in all parameters with values, whether or not they are relevant to the test.

Another possible cause is when we include all the code needed to verify the outcome using procedural state verification rather than using a much more compact declarative style  to specify the expected outcome.

The best way to get rid of irrelevant information in fixture setup logic is to replace direct calls to the constructor or factory methods with calls to parameterized creation methods that take only the relevant information as parameters. Fixture values that do not matter to the test (those that do not affect the expected outcome) should be defaulted within creation methods or replaced by dummy objects. In this way we say to the reader, “The values you don’t see don’t affect the expected outcome”. We can replace fixture values that appear in both the fixture setup and outcome verification parts of the test with suitably initialized named constants as long as we are using a fresh fixture approach to fixture setup.

To hide irrelevant information in result verification logic, we can use assertions on entire expected objects, rather than asserting on individual fields, and we can create custom assertions that hid complex procedural verification logic.

Hard-Coded Test Data

It is difficult to determine how various hard-coded (i.e., literal) values in the test are related to one another and which values should affect the behavior of the SUT. 

Tests require passing data to the SUT. The data used in the fixture setup and exercise SUT phases of the Four-Phase Test define the preconditions of the SUT and influence how it should behave. The post-conditions (the expected outcomes) are reflected in the data passed as arguments to the assertion methods in the verify outcome phase of the test. When writing tests, the path of least resistance is to use whatever methods are available (on the SUT and other objects) and to fill in all parameters with values, whether or not they are relevant to the test.

When we use copy-paste reuse of test logic, we find ourselves replicating the literal values to the derivative tests.

The best way to get rid of the obscure test smell is to replace the literal constants with named constants.

Fixture values that do not matter to the test (those that do not affect the expected outcome) should be defaulted within creation methods. Same solution as irrelevant information above.

Values in the result verification logic that are based on values used in the fixture or that are used as arguments of the SUT should be replaced with derived values to make those calculations obvious to the test reader.

Indirect Testing 

A test interacts primarily with objects other than the one whose behavior it purports to verify. The test constructs and interacts with objects that contain references to the SUT rather than with the SUT itself. Testing business logic through the presentation layer is a common example of indirect testing.

It may result in fragile tests because changes in the intermediate objects may require modification of the tests even when the SUT is not modified.

Solution Patterns

A good test strategy helps keep the test code understandable. Nevertheless, just as “no battle plan survives the first contact with the enemy”, no test infrastructure can anticipate all needs of all tests. We should expect the test infrastructure to evolve as the software matures and our test automation skills improve.

We can reuse test logic for several scenarios by having several tests call test utility methods or by asking a common parameterized test to pass in the already built test fixture or expected objects.

Writing tests in an outside-in way can minimize the likelihood of producing an obscure test that might then need to be refactored. This approach starts by outlining the Four-Phase Test using calls to nonexistent test utility methods. Once we are satisfied with these tests, we can write the utility methods needed to run them. By writing the test first, we gain a better understanding of what the utility methods need to do for us to make writing the tests as simple as possible. Write test utility tests before writing the test utility methods.

Conditional Test Logic

Indented Test Code

A test contains code that may or may not be executed

A fully automated test is just code that verifies the behavior of other code. But if this code is complicated, how do we verify that it works properly? We could write tests for our tests - but when would this recursion stop? The simple answer is that test methods must be simple enough to not need tests.

Conditional test logic make tests complicated. This means no looping logic and if statements. What is this test code doing and how do we know that it is doing it correctly? Conditional test logic makes it difficult to know exactly what a test is going to do when it really matters. Code that has multiple execution paths does not give you confidence about its outcome. Code that has only a single execution path always executes in exactly the same way. 

To increase our confidence in production code, we can write self-checking tests that exercise the code. How can we increase our confidence in the test code if it executes differently each time we run it? It is hard to know or prove that the test is verifying the behavior we want it to verify. A test that has branches, loops or that uses different values each time it is run, can be very difficult to debug simply because it isn’t completely deterministic.

Conditional test logic makes writing the test correctly a difficult task. Because the test itself cannot be tested easily, how do we know that it will actually detect the bugs it is intended to catch? Obscure tests are more likely to result in buggy tests than simple code.

Flexible Test

The test code verifies different functionality depending on when or where it is run. It contains conditional logic that does different things depending on the current environment. Most commonly this functionality takes the form of conditional test logic to build different versions of the expected results based on some factor external to the test.

For example, a test which depends on a certain time to determine what the output of the SUT should be.

A flexible test is caused by a lack of control of the environment. The developer was not able to decouple the SUT from its dependencies and decided to adapt the test logic based on the state of the environment.

It makes the test harder understand and maintain. We don’t know which test scenarios are actually being exercised and whether all scenarios are exercised regularly. 

Decouple the SUT from external dependencies by refactoring the SUT to support substitutable dependency. We can then replace the dependency with a test double such as test stub or mock object and write separate tests for different scenarios.

Conditional Verification Logic

Conditional test logic create problems when it is used to verify the expected outcome. This happens when developer tries to prevent the execution of assertions if the SUT fails to return the right objects or uses loops to verify the contents of collections returned by the SUT.

We can replace the if statements that steer execution to a call to fail with a guard assertion that causes the test to fail before we reach the code we don’t want to execute. We can replace conditional test logic for verification of complex objects with an equality assertion on an expected object. We can use a custom assertion to define test-specific equality.

We should move loops in the verification logic to a custom assertion. We can then verify this assertion’s behavior by using custom assertion tests.

Production Logic in Test

This is due to verification of multiple test conditions in a single test method. Given that multiple input values are passed to the SUT, we should also have multiple expected results. The expected SUT logic is replicated in the test to calculate the expected values for assertions.

1. Create a calculator instance
2. Initialize expected = 0
3. Outermost loop iterates from 0 to 9
4. Innermost loop iterates from 0 to 9
5. Exercise SUT : actual = sut.calculate(i, j)
6. Verify result : if conditions to handle special case - expected = 8 else expected = i+j; assert(expected, actual)

Solution:

1. Create a calculator instance
2. Initialize an array of TestValue objects with 4 data sets.
3. Loop through the array to exercise SUT and verify result

Multiple Test Conditions

The same test logic is applied to many sets of input values, each with its own corresponding expected result. The developer is testing many test conditions using the same test logic in a single test method. 

In the previous section’s solution the test iterates over a collection of test values and applies the test logic to each set.

This bad ass test stops executing at the first failure and doesn’t provide defect localization. The solution is to call a parameterized test from a separate test method for each test condition. For large data sets, a data driven test is better.

Hard-to-Test Code
 
Hard-Coded Dependency
Highly Coupled Code

A class cannot be tested without also testing several other classes. Code that is highly coupled to other code is difficult to unit test because it won’t execute in isolation. This is caused by poor design, lack of OOD experience etc.

The key to testing overly coupled code is to break the coupling. A technique that is often used to decouple code for the purpose of testing is a test double (either a test stub or a mock object). 

Asynchronous Code

A class cannot be tested via direct method calls. The test must start an executable (such as a thread, process or application) and wait until its start-up has finished before interaction with the executable. 

The key to testing asynchronous code is to separate the logic from the asynchronous access mechanism. The design for testability pattern humble object is a good example of a way to restructure otherwise asynchronous code so it can be tested in a synchronous manner.

Untestable Test Code

The body of a test method is obscure enough or contains enough conditional test logic that we wonder whether the test is correct.

We can remove the need to test the body of a test method by making it extremely simple and relocating any conditional test logic from it into test utility methods for which we can easily write self checking tests.

Focus on the big picture (the intent) of the test. Write the test methods in an outside-in manner, focusing on their intent. Whenever we need to do something that involves several lines of code, we simply call a nonexistent test utility method to do it. We write all our tests this way and then fill in implementations of the test utility methods to get the tests to compile and run. 

Test Logic in Production

The code that is put into production contains logic that should be exercised only during tests.

Instead of adding test logic into the production code directly, we can move logic into a substitutable dependency. We can put code that should be run in only production into a strategy object that is installed by default and replaced by a Null object when running our tests. In contrast, code that should be run only during tests can be put into a strategy object that is configured as a null object by default. Then when we want the SUT to execute extra code during testing, we can replace this strategy object with test specific version. 

Erratic Test

Sometimes they pass and sometimes they fail. If the cause cannot be easily determined, collect data systematically over a period of time. In which environments did the tests pass and where did they fail? Were all the tests being run or just a subset of them? Did any change in behavior occur when the test suite was run several times in a row? Did any change in behavior occur when it was run from several test runners at the same time.

Once we have some data, match up the observed symptoms with those listed for each of the potential causes and to narrow the list of possibilities to a handful of candidates. Then we can collect some more data focusing on differences in symptoms between the possible causes. 

Unrepeatable Test

A test behaves differently the first time it is run compared with how it behaves on subsequent test runs. In effect, it is interacting with itself across test runs.

Nondeterministic Test

The first step is to make our tests repeatable by ensuring that they execute in a completely linear fashion by removing any conditional test logic. Then we go about replacing any random values with deterministic values. 

Interface Sensitivity

A test written in a dynamically typed language may experience a test error when it invokes an API that has been modified (via a method name change or method signature change). Alternatively the test may fail to find a UI element it needs to interact with the SUT via a UI. 

When the interface is used only internally (within the company or application) and by automated tests, SUT API encapsulation is the best solution for interface sensitivity. It reduces the cost and impact of changes to the API and therefore, does not discourage necessary changes from being made. A common way to implement SUT API encapsulation is through the definition of higher-level language that is used to express the tests. The verbs in the test language are translated into the appropriate method calls by the encapsulation layer, which is then the only software that needs to be modified when the interface is altered in somewhat backward compatible ways. The test language can be implemented in the form of test utility methods such as creation methods and verification methods that hide the API of the SUT from the test.

Behavior Sensitivity 

This occurs when changes to the SUT cause other tests to fail. A test that once passed suddenly starts failing when a new feature is added to the SUT or a bug is fixed.

The functionality the regression tests use to set up the pre-test state of the SUT has been modified.
The functionality the regression tests use to verify the post-test state of the SUT has been modified.
The code the regression tests use to tear down the fixture has been changed.

If the code that changed is not part of the SUT we are verifying, then we may be testing too large a SUT. In such a case, what we really need to do is to separate the SUT into the part we are verifying and the components on which that part depends.

Any newly incorrect assumptions about the behavior of the SUT used during fixture setup may be encapsulated behind creation methods. Similarly assumptions about the details of post-test state of the SUT can be encapsulated in custom assertions or verification methods. These measures reduce the amount of test code that needs to be changed.

Data Sensitivity 

This occurs when a test fails because the data being used to test the SUT has been modified. This sensitivity most commonly arises when the contents of the test database change.

A test once passed suddenly starts failing :

Data is added to the database that holds the pre-test state of the SUT.
Records in the database are modified or deleted.
The code that sets up a standard fixture is modified.
A shared fixture is modified before the first test that uses it.

Use fresh fixture to fix this problem. Another solution is to verify that the right changes have been made to the data. Compare before and after snapshots of the data, thereby ignoring data that hasn’t changed. They eliminate the need to hard-code knowledge about the entire fixture into the result verification phase of the test.

Context Sensitivity

This occurs when a test fails because the state or behavior of the context in which the SUT executes has changed.

A test that once passed suddenly starts failing for mysterious reasons. Unlike the erratic test, the test produces consistent results when run repeatedly over a short period of time. What is different is that it consistently fails regardless of how it is run.
Tests may fail for two reasons:
The functionality they are verifying depends in some way on the time or date.
The behavior of some other code or system on which the SUT depends on has changed.

A major source of context sensitivity is confusion about which SUT we are intending to verify. Recall that the SUT is whatever piece of software we are intending to verify. When unit testing, it should be very small part of the overall system or application. Failure to isolate the specific unit (e.g., class or method) is bound to lead to context sensitivity because we end up testing too much software all at once. Indirect inputs that should be controlled by the test are then left to chance. If someone then modifies a DoC, our tests fail.
To eliminate context sensitivity, we must track down which indirect input to the SUT has changed and why. If the system contains any date or time related logic we should examine this logic to see whether the length of the month or other similar factors could be the cause of the problem.

If the SUT depends on input from any other systems, we should examine these inputs to see if anything has changed recently. Logs of the previous interactions with these other systems are very useful for comparison with logs of the failure scenarios.

We need to control all the inputs of the SUT if our tests are to be deterministic. If we depend on inputs from other systems, we may need to control these inputs by using a test stub that is configured and installed by the test. If the system contains any time related logic we need to be able to control the system clock as part of our testing. This means stubbing out the system clock with a virtual clock that gives the test a way to set the starting time or date and possibly to simulate the passage of time.

Over specified Software

Overcoupled Test

A test says too much about how the software should be structured or behave. This form of behavior sensitivity is associated with the style of testing called behavior verification. It is characterized by extensive use of mock objects to build layer-crossing tests. The main issue is that the tests describe how the software should do something, not what it should achieve. That is, the tests will pass only if the software is implemented in a particular way. This problem can be avoided by applying the principle Use the Front Door First whenever possible to avoid encoding too much knowledge about the implementation of the SUT into the tests.

Frequent Debugging

We may be missing the component tests for a cluster of classes (i.e., a component) that would point out an integration error between the individual classes. This can happen when we use mock objects extensively to replace depended on objects but the unit tests of the depended on objects don’t match the way the mock objects are programmed to behave.

Write unit tests for individual classes as well as component tests for the collections of related classes to ensure we have good defect localization.

Slow Tests 

We can make our tests run faster by replacing the slow components with a test double. Use fake database to make your database unit tests run faster.

Too Many Tests

This might be due to a large system or overlap between tests. The less obvious case is that we are running too many tests too frequently. We don’t have to run all the tests all the time. The key is to ensure that all tests are run regularly. 

If the system is large in size, it is a good idea to break it into a number of fairly independent subsystems or components. This allows teams working on each component to work independently and to run only those tests specific to their own component. Some of those tests should act as proxies for how the other components would use the component; they must be kept up-to-date if the interface contract changes.

Untested Code

If the untested code is caused by an inability to control the indirect inputs of the SUT, the most common solution is to use a test stub to feed the various kinds of indirect inputs into the SUT to cover all the code paths. Otherwise it may be sufficient to configure the DoC to cause it to return the various indirect inputs required to fully test the SUT.

Untested Requirement

We may just know that some piece of functionality is not being tested. Alternatively we may be trying to test a piece of software but cannot see any visible functionality that can be tested via the public interface of the software. All the tests we have written pass, however. 

When doing TDD, we know we need to add some code to handle a requirement. The most common cause of untested requirements is that the SUT includes behavior that is not visible through its public interface. It may have expected “side effects” that cannot be observed directly by the test (such as writing out a file or record or calling a method on another object or component) - in other words, it may have indirect outputs.

When the SUT is an entire application, the untested requirement may be a result of not having a full suite of customer tests that verify all aspects of the visible behavior of the SUT.

If the problem is missing customer tests, we need to write at least enough customer tests to ensure that all components are integrated properly. This may require improving the design-for-testability of the application by separating the presentation layer from the business logic layer. 

When we have indirect outputs that we need to verify, we can do behavior verification through the use of mock objects. Testing of indirect outputs is done using test doubles.



</Text>
        </Document>
    </Documents>
</SearchIndexes>